<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[自动问答系统开源轮子简介]]></title>
    <url>%2F2018%2F06%2F27%2Fqa-os%2F</url>
    <content type="text"><![CDATA[本文对自动问答系统相关的开源轮子做简单的对比，包括全套的问答系统实现和纯粹的NLP框架。对比的指标很简单：能不能跑起来、能不能正常交互。虽然指标很简单，但是实际上很多开源项目并不能达标…… 1 开源的自动问答系统下面这些东西都很难跑起来，基本上都要安装一大堆依赖，每个至少几百兆。 1.1 DrQA Facebook - DrQA 1.1.1 简介Facebook在2017年发表的成果 系统组成： Document Retriever 将数据集训练成模型 Document Reader 抽取答案 DrQA Pipline 连接各组件 Distant Supervision(DS) 远程监督 Tokenizers 分词、命名实体标识 依赖pytorch，装这个东西很耗时间。装的时候不要编译源码，编译既耗时也耗内存，很容易失败，可以采用以下两种方式之一： pip install torch torchversion pip3 install torch torchversion 注意python的版本，官方文档里运行DrQA的demo是用python 命令来运行的，实际上是python3，如果你的系统没有对python3做别名或者链接为python，那么如果用pip3来安装pytorch是没用的（因为输入python命令的时候调用的是python2），然后我也试过用python3来运行DrQA的demo，有部分模块是安装不了的，所以还是要用pip来安装（虽然pytorch的官方安装命令用的是pip3） 1.1.2 配置要求这东西应该是将数据集全部加载到内存里的，很吃内存。wikipedia的数据集下载下来的压缩包有7G+，解压之后有25G，所以硬盘的可用空间起码要有33G，运行的时候大概用了15G内存，因此内存+swap要大于15G。 1.1.3 效果效果貌似也不太好，文档搜索的准确度挺好，但是从目标文档里提取的答案却不太好： 1.2 QuestionAnsweringSystem QuestionAnsweringSystem QuestionAnsweringSystem技术实现分析.pdf Java实现 2015年的工程，没有经受生产验证，也没有论文发表 响应速度太慢了，单机单个访问的响应时间都超过10秒 1.3 OpenEphyra在线： http://www.ephyra.info/https://github.com/TScottJ/OpenEphyra 首个QA系统，很久没维护了，在线网站的域名都被别人买走了。 开源的项目代码还是用eclipse开发的，所有配置文件都放到github了，用到的库全塞到lib目录一并上传了，可见工程经验很不足，估计代码性能也是不行的。用到的库也是很旧的版本，大部分版本在公共的maven仓库上是没有的。 代码跑起来了，但是输入很简单的问题『What is car?』都给不出答案： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283Question: What is car?+++++ Analyzing question (2018-06-27 14:26:40) +++++Normalization: what be carAnswer types:-Interpretations:Property: DEFINITIONTarget: carProperty: NAMETarget: carPredicates:-+++++ Generating queries (2018-06-27 14:26:40) +++++Query strings:"car" car"car" car+++++ Searching (2018-06-27 14:26:40) +++++Search error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed outSearch error:java.net.SocketTimeoutException: connect timed out+++++ Selecting Answers (2018-06-27 14:26:56) +++++Filter "AnswerTypeFilter" started, 0 Results (2018-06-27 14:26:56)Filter "AnswerTypeFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "AnswerPatternFilter" started, 0 Results (2018-06-27 14:26:56)Filter "AnswerPatternFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "WebDocumentFetcherFilter" started, 0 Results (2018-06-27 14:26:56)Filter "WebDocumentFetcherFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "PredicateExtractionFilter" started, 0 Results (2018-06-27 14:26:56)Filter "PredicateExtractionFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "FactoidsFromPredicatesFilter" started, 0 Results (2018-06-27 14:26:56)Filter "FactoidsFromPredicatesFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "TruncationFilter" started, 0 Results (2018-06-27 14:26:56)Filter "TruncationFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "StopwordFilter" started, 0 Results (2018-06-27 14:26:56)Filter "StopwordFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "QuestionKeywordsFilter" started, 0 Results (2018-06-27 14:26:56)Filter "QuestionKeywordsFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "ScoreNormalizationFilter" started, 0 Results (2018-06-27 14:26:56)Filter "ScoreNormalizationFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "ScoreCombinationFilter" started, 0 Results (2018-06-27 14:26:56)Filter "ScoreCombinationFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "FactoidSubsetFilter" started, 0 Results (2018-06-27 14:26:56)Filter "FactoidSubsetFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "DuplicateFilter" started, 0 Results (2018-06-27 14:26:56)Filter "DuplicateFilter" finished, 0 Results (2018-06-27 14:26:56)Filter "ScoreSorterFilter" started, 0 Results (2018-06-27 14:26:56)Filter "ScoreSorterFilter" finished, 0 Results (2018-06-27 14:26:56)Answer: 看了下代码，这些socket timeout发生的地方其实都是去请求搜索引擎拿答案的。。。。。 1.4 Watsonsimhttps://github.com/SeanTater/uncc2014watsonsim/ gradle构建失败：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647FAILURE: Build failed with an exception.* What went wrong:Could not resolve all dependencies for configuration ':compile'.&gt; Could not find :libsvm:. Searched in the following locations: https://repo1.maven.org/maven2//libsvm//libsvm-.pom https://repo1.maven.org/maven2//libsvm//libsvm-.jar https://repo.eclipse.org/content/groups/releases//libsvm//libsvm-.pom https://repo.eclipse.org/content/groups/releases//libsvm//libsvm-.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/lib/libsvm-.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/lib/libsvm.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/data/lib/libsvm-.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/data/lib/libsvm.jar Required by: edu.uncc:uncc2014watsonsim:0.4.0&gt; Could not find lemurproject.indri:indri:5.7. Searched in the following locations: https://repo1.maven.org/maven2/lemurproject/indri/indri/5.7/indri-5.7.pom https://repo1.maven.org/maven2/lemurproject/indri/indri/5.7/indri-5.7.jar https://repo.eclipse.org/content/groups/releases/lemurproject/indri/indri/5.7/indri-5.7.pom https://repo.eclipse.org/content/groups/releases/lemurproject/indri/indri/5.7/indri-5.7.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/lib/indri-5.7.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/lib/indri.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/data/lib/indri-5.7.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/data/lib/indri.jar Required by: edu.uncc:uncc2014watsonsim:0.4.0&gt; Could not find :stanford-srparser-2014-08-28-models:. Searched in the following locations: https://repo1.maven.org/maven2//stanford-srparser-2014-08-28-models//stanford-srparser-2014-08-28-models-.pom https://repo1.maven.org/maven2//stanford-srparser-2014-08-28-models//stanford-srparser-2014-08-28-models-.jar https://repo.eclipse.org/content/groups/releases//stanford-srparser-2014-08-28-models//stanford-srparser-2014-08-28-models-.pom https://repo.eclipse.org/content/groups/releases//stanford-srparser-2014-08-28-models//stanford-srparser-2014-08-28-models-.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/lib/stanford-srparser-2014-08-28-models-.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/lib/stanford-srparser-2014-08-28-models.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/data/lib/stanford-srparser-2014-08-28-models-.jar file:/home/gordon/Documents/repo/github/uncc2014watsonsim/data/lib/stanford-srparser-2014-08-28-models.jar Required by: edu.uncc:uncc2014watsonsim:0.4.0* Try:Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.BUILD FAILEDTotal time: 38.667 secs 1.5 YodaQAhttps://github.com/brmson/yodaqa/ 2015年搞的 在线的示例没响应 1.5.1 原理http://ailao.eu/yodaqa/science.html 1.5.2 运行构建失败（卡在test那一步）： 123456789101112131415➜ yodaqa git:(master) ./gradlew check:generateTypeSystem UP-TO-DATE:compileJava UP-TO-DATE:compileGroovy UP-TO-DATE:processResources UP-TO-DATE:classes UP-TO-DATE:generateTestTypeSystem UP-TO-DATE:compileTestJava UP-TO-DATE:compileTestGroovy UP-TO-DATE:processTestResources UP-TO-DATE:testClasses UP-TO-DATE:testcz.brmlab.yodaqa.provider.rdf.DBpediaTypesTest &gt; testQueryTitleForm STARTED&gt; Building 83% &gt; :test 可以跳过这一步，直接运行./gradlew web -q，这样开启了web服务，访问4567端口，输入问题，提交之后可以从后台的日志里面看到还是需要下载一大堆东西（100MB左右）。 1234567891011121314 rx | tx--------------------------------------+------------------ bytes 108.95 MiB | 9.99 MiB--------------------------------------+------------------ max 6.96 Mbit/s | 574 kbit/s average 361.95 kbit/s | 33.19 kbit/s min 0 kbit/s | 1 kbit/s--------------------------------------+------------------ packets 92679 | 51726--------------------------------------+------------------ max 635 p/s | 451 p/s average 36 p/s | 20 p/s min 0 p/s | 0 p/s--------------------------------------+------------------ 然而即使下完了，还是会出错： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677INFO ResourceObjectProviderBase - Producing resource from jar:file:/home/gordon/.ivy2/cache/de.tudarmstadt.ukp.dkpro.core/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-ner-en-time/jars/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-ner-en-time-20100907.jar!/de/tudarmstadt/ukp/dkpro/core/opennlp/lib/ner-en-time.binINFO ResourceObjectProviderBase - Producing resource took 400msjava.net.ConnectException: Connection timed out (Connection timed out) at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at java.net.Socket.connect(Socket.java:538) at sun.net.NetworkClient.doConnect(NetworkClient.java:180) at sun.net.www.http.HttpClient.openServer(HttpClient.java:463) at sun.net.www.http.HttpClient.openServer(HttpClient.java:558) at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:242) at sun.net.www.http.HttpClient.New(HttpClient.java:339) at sun.net.www.http.HttpClient.New(HttpClient.java:357) at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202) at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138) at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032) at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966) at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1546) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474) at cz.brmlab.yodaqa.provider.rdf.DBpediaTitles.queryFuzzyLookup(DBpediaTitles.java:256) at cz.brmlab.yodaqa.provider.rdf.DBpediaTitles.query(DBpediaTitles.java:102) at cz.brmlab.yodaqa.analysis.question.CluesToConcepts.process(CluesToConcepts.java:104) at org.apache.uima.analysis_component.JCasAnnotator_ImplBase.process(JCasAnnotator_ImplBase.java:48) at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.callAnalysisComponentProcess(PrimitiveAnalysisEngine_impl.java:385) at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.processAndOutputNewCASes(PrimitiveAnalysisEngine_impl.java:309) at cz.brmlab.yodaqa.flow.asb.MultiprocessingAnalysisEngine_MultiplierOk.processAndOutputNewCASes(MultiprocessingAnalysisEngine_MultiplierOk.java:218) at cz.brmlab.yodaqa.flow.asb.MultiThreadASB$AggregateCasIterator$1.call(MultiThreadASB.java:772) at cz.brmlab.yodaqa.flow.asb.MultiThreadASB$AggregateCasIterator$1.call(MultiThreadASB.java:754) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)*** http://yodaqa.felk.cvut.cz/dbp-labels1 or http://yodaqa.felk.cvut.cz/dbp-labels2 label lookup query (temporarily?) failed, retrying in a moment...ALERT: main seems stuck for more than 300s waiting for a job delivery.Thread[main,5,main] - Future java.util.concurrent.FutureTask@42ca6733 done false canc false :: AE cz.brmlab.yodaqa.analysis.question.QuestionAnalysisAE from CAS org.apache.uima.cas.impl.CASImpl@2006673c (CIF cz.brmlab.yodaqa.flow.asb.MultiThreadASB$CasInFlow@749aa36f)ALERT: pool-2-thread-1 seems stuck for more than 300s waiting for a job delivery.Thread[pool-2-thread-1,5,main] - Future java.util.concurrent.FutureTask@4d50917f done false canc false :: AE cz.brmlab.yodaqa.analysis.question.CluesToConcepts from CAS org.apache.uima.cas.impl.CASImpl@2006673c (CIF cz.brmlab.yodaqa.flow.asb.MultiThreadASB$CasInFlow@521a63df)java.net.ConnectException: Connection timed out (Connection timed out) at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at java.net.Socket.connect(Socket.java:538) at sun.net.NetworkClient.doConnect(NetworkClient.java:180) at sun.net.www.http.HttpClient.openServer(HttpClient.java:463) at sun.net.www.http.HttpClient.openServer(HttpClient.java:558) at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:242) at sun.net.www.http.HttpClient.New(HttpClient.java:339) at sun.net.www.http.HttpClient.New(HttpClient.java:357) at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202) at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138) at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032) at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966) at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1546) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474) at cz.brmlab.yodaqa.provider.rdf.DBpediaTitles.queryFuzzyLookup(DBpediaTitles.java:256) at cz.brmlab.yodaqa.provider.rdf.DBpediaTitles.query(DBpediaTitles.java:102) at cz.brmlab.yodaqa.analysis.question.CluesToConcepts.process(CluesToConcepts.java:104) at org.apache.uima.analysis_component.JCasAnnotator_ImplBase.process(JCasAnnotator_ImplBase.java:48) at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.callAnalysisComponentProcess(PrimitiveAnalysisEngine_impl.java:385) at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.processAndOutputNewCASes(PrimitiveAnalysisEngine_impl.java:309) at cz.brmlab.yodaqa.flow.asb.MultiprocessingAnalysisEngine_MultiplierOk.processAndOutputNewCASes(MultiprocessingAnalysisEngine_MultiplierOk.java:218) at cz.brmlab.yodaqa.flow.asb.MultiThreadASB$AggregateCasIterator$1.call(MultiThreadASB.java:772) at cz.brmlab.yodaqa.flow.asb.MultiThreadASB$AggregateCasIterator$1.call(MultiThreadASB.java:754) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)*** http://yodaqa.felk.cvut.cz/dbp-labels1 or http://yodaqa.felk.cvut.cz/dbp-labels2 label lookup query (temporarily?) failed, retrying in a moment... 这里应该是问题所在：https://github.com/brmson/yodaqa/issues/81然而删掉了那两个label的url之后还是出错，再删掉solr那一行和前面的逗号之后，还是同样的错： 12345678910111213141516171819202122232425262728INFO CoreContainer - Loading CoreContainer using Solr Home: 'collection1/'INFO SolrResourceLoader - new SolrResourceLoader for directory: 'collection1/'Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/lucene/index/FilterIndexReader at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.apache.solr.core.CoreContainer.load(CoreContainer.java:265) at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:165) at cz.brmlab.yodaqa.provider.solr.Solr.createEmbeddedSolrServer(Solr.java:70) at cz.brmlab.yodaqa.provider.solr.Solr.&lt;init&gt;(Solr.java:51) at cz.brmlab.yodaqa.provider.solr.SolrNamedSource.register(SolrNamedSource.java:18) at cz.brmlab.yodaqa.pipeline.YodaQA.&lt;clinit&gt;(YodaQA.java:59) at cz.brmlab.yodaqa.YodaQA_Web.main(YodaQA_Web.java:37)Caused by: java.lang.ClassNotFoundException: org.apache.lucene.index.FilterIndexReader at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 19 more 弃了。。。 1.6 QA-基于LSTM的中文问答系统https://github.com/S-H-Y-GitHub/QA 数据集下载地址：https://pan.baidu.com/s/1FzkR0Q9RrKzj4pa2uD1BYw#list/path=%2F%E6%95%B0%E6%8D%AE%2FQA&amp;parentPath=%2F%E6%95%B0%E6%8D%AE 放好了数据之后，直接运行的话运行时会报错，打印一大堆数字，解决方法：https://github.com/S-H-Y-GitHub/QA/issues/27 把wiki 詞向量的第一行兩個數字刪掉，然後不要保留空行 这个东西运行要跑很久，需要训练 训练完也没什么用，不能提问，只生成了一个评分文件，对测试集里的数据进行了评分。 1.7 OpenQAhttp://openqa.aksw.org 源码：https://bitbucket.org/emarx/openqa openQA is a framework and platform for Question Answering development and publication. 文档不太详细，没法直接跑一个直观问答的例子 源码体积很大，1.22G 1.99 对比结果这个阶段只能做这么初步的考量了…… - 可运行 能正常问答 DrQA ✔ ✔ QuestionAnsweringSystem ✔ 慢 OpenEphyra ✔ ✖ Watsonsim ✖ ✖ YodaQA ✔ ✖ QA ✔ ✖ OpenQA ? ? 2 开源NLP工具2.1 LTPhttp://ltp.ai/docs/index.html LTP提供了一系列中文自然语言处理工具，用户可以使用这些工具对于中文文本进行分词、词性标注、句法分析等等工作。从应用角度来看，LTP为用户提供了下列组件： 针对单一自然语言处理任务，生成统计机器学习模型的工具 针对单一自然语言处理任务，调用模型进行分析的编程接口 使用流水线方式将各个分析工具结合起来，形成一套统一的中文自然语言处理系统 系统可调用的，用于中文语言处理的模型文件 针对单一自然语言处理任务，基于云端的编程接口 如果你的公司需要一套高性能的中文语言分析工具以处理海量的文本，或者你的在研究工作建立在一系列底层中文自然语言处理任务之上，或者你想将自己的科研成果与前沿先进工作进行对比，LTP都可能是你的选择。 这个工具的协议需要注意一下，商用需要付费。 2.2 HanLPhttp://www.hankcs.com/nlp/hanlp.htmlGitHub: https://github.com/hankcs/HanLP HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。 HanLP提供下列功能： 中文分词 HMM-Bigram（速度与精度最佳平衡；一百兆内存） 最短路分词、N-最短路分词 由字构词（侧重精度，可识别新词；适合NLP任务） 感知机分词、CRF分词 词典分词（侧重速度，每秒数千万字符；省内存） 极速词典分词 所有分词器都支持： 索引全切分模式 用户自定义词典 兼容繁体中文 训练用户自己的领域模型 词性标注 HMM词性标注（速度快） 感知机词性标注、CRF词性标注（精度高） 命名实体识别 基于HMM角色标注的命名实体识别 （速度快） 中国人名识别、音译人名识别、日本人名识别、地名识别、实体机构名识别 基于线性模型的命名实体识别（精度高） 感知机命名实体识别、CRF命名实体识别 关键词提取 TextRank关键词提取 自动摘要 TextRank自动摘要 短语提取 基于互信息和左右信息熵的短语提取 拼音转换 多音字、声母、韵母、声调 简繁转换 简繁分歧词（简体、繁体、臺灣正體、香港繁體） 文本推荐 语义推荐、拼音推荐、字词推荐 依存句法分析 基于神经网络的高性能依存句法分析器 MaxEnt依存句法分析 文本分类 情感分析 word2vec 词向量训练、加载、词语相似度计算、语义运算、查询、KMeans聚类 文档语义相似度计算 语料库工具 2.3 THUCTCTHUCTC: 一个高效的中文文本分类工具包 THUCTC(THU Chinese Text Classification)是由清华大学自然语言处理实验室推出的中文文本分类工具包，能够自动高效地实现用户自定义的文本分类语料的训练、评测、分类功能。 2.4 Deeplearning4j (DL4J)https://deeplearning4j.org/cn/ DeepLearning4J（DL4J）是一套基于Java语言的神经网络工具包，可以构建、定型和部署神经网络。DL4J与Hadoop和Spark集成，支持分布式CPU和GPU，为商业环境（而非研究工具目的）所设计。Skymind是DL4J的商业支持机构。 Deeplearning4j包括了分布式、多线程的深度学习框架，以及普通的单线程深度学习框架。定型过程以集群进行，也就是说，Deeplearning4j可以快速处理大量数据。神经网络可通过[迭代化简]平行定型，与 Java、 Scala 和 Clojure 均兼容。Deeplearning4j在开放堆栈中作为模块组件的功能，使之成为首个为微服务架构打造的深度学习框架。 GitHub：https://github.com/deeplearning4j/deeplearning4j 其他参考 谷歌开源最精确自然语言解析器SyntaxNet | 机器之心 使用维基百科训练简体中文词向量 - robert_ai - 博客园]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视频技术概念扫盲初步]]></title>
    <url>%2F2018%2F05%2F07%2Fvideo-tech-initial-learning%2F</url>
    <content type="text"><![CDATA[本文就视频技术做一个初步的扫盲（主要是针对我本人，不是针对在座的各位），基本就是资料的搜集和摘要，目的是知道一些常见的概念大概是怎么一回事，包括PPI、分辨率、比特率和视频音频压缩的一些常见术语。（本来题目叫做《视频技术概念扫盲》的，但是一轮下来，发现水深得很，还是加上『初步』比较好，毕竟还有很多盲点没扫到。） 1 一般概念1.1 PPI 每英寸像素（Pixels Per Inch，PPI），又被称为像素密度（Pixel Density），是一个表示打印图像或显示器单位面积上像素数量的指数。一般用来计量电脑显示器，电视机和手持电子设备屏幕的精细程度。通常情况下，每英寸像素值越高，屏幕能显示的图像也越精细。 计算公式如下：$$ PPI=\frac {d_p}{d_i}=\frac {\sqrt{w^2_p+h^2_p}}{d_i}$$其中， $d_p$为屏幕对角线的分辨率 $w_p$为屏幕横向分辨率 $h_p$为屏幕纵向分辨率 $d_i$为屏幕对角线长度（单位为英寸） 参见 https://zh.wikipedia.org/wiki/%E6%AF%8F%E8%8B%B1%E5%AF%B8%E5%83%8F%E7%B4%A0 以下是一个简陋的计算程序 计算PPI function calppi(){ var width = document.getElementById("ppi-width").value; var height = document.getElementById("ppi-height").value; var diagonal = document.getElementById("ppi-diagonal").value; if(!isValidNum(width)){ alert("请填写宽度，必须是正数"); return; } if(!isValidNum(height)){ alert("请填写高度，必须是正数"); return; } if(!isValidNum(diagonal)){ alert("请填写对角长度（英寸），必须是正数"); return; } console.log(width); console.log(height); console.log(diagonal); var result = Math.sqrt(Math.pow(width, 2) + Math.pow(height, 2))/diagonal; console.log(result); document.getElementById("ppi-result").value = result; } function isValidNum(n){ if(n == null || n == undefined || n == "" || isNaN(n)){ return false; } return n > 0; } 1.2 分辨率1.2.1 图形显示分辨率（Graphics display resolution）https://en.wikipedia.org/wiki/Graphics_display_resolution The graphics display resolution is the width and height dimension of an electronic visual display device, such as a computer monitor, in pixels. 高清标准定义如下： Name x (px) y (px) x:y x×y (Mpx) nHD 640 360 16:9 0.230 qHD 960 540 16:9 0.518 HD 1280 720 16:9 0.922 HD+ 1600 900 16:9 1.440 FHD 1920 1080 16:9 2.074 (W)QHD 2560 1440 16:9 3.686 QHD+ 3200 1800 16:9 5.760 4K UHD 3840 2160 16:9 8.294 5K UHD+ 5120 2880 16:9 14.746 8K UHD 7680 4320 16:9 33.178 1.2.2 图像分辨率（Image resolution）https://en.wikipedia.org/wiki/Image_resolutionhttps://zh.wikipedia.org/wiki/%E5%88%86%E8%BE%A8%E7%8E%87 也叫解析度 定义图像分辨率是图像所具有的细节。该术语适用于光栅数字图像，电影图像和其他类型的图像。分辨率越高意味着更多图像细节。 描述图像分辨率的角度有： 像素分辨率 电子图像的像素点数量，一般用长宽描述，例如1000 × 1500 空间分辨率 空间分辨率是指遥感影像上能够识别的两个相邻地物的最小距离 etc 单位 描述分辨率的单位有：dpi（点每英寸）、lpi（线每英寸）和ppi（每英寸像素）。但只有lpi是描述光学分辨率的尺度的。虽然dpi和ppi也属于分辨率范畴内的单位，但是他们的含义与lpi不同。而且lpi与dpi无法换算，只能凭经验估算。 另外，ppi和dpi经常都会出现混用现象。但是他们所用的领域也存在区别。从技术角度说，“像素”只存在于电脑显示领域，而“点”只出现于打印或印刷领域。 数字媒体分辨率： 500×480 : Digital8 720×480 : D-VHS, DVD, miniDV, Digital Betacam (NTSC) 720×480 : Widescreen DVD (anamorphic) (NTSC) 720×576 : D-VHS, DVD, miniDV, Digital8, Digital Betacam (PAL/SECAM) 720×576 : Widescreen DVD (anamorphic) (PAL/SECAM) 1280×720 : D-VHS, HD DVD, Blu-ray, HDV (miniDV) 1440×1080 : HDV (miniDV) 1920×1080 : HDV (miniDV), AVCHD, HD DVD, Blu-ray, HDCAM SR 1998×1080 : 2K Flat (1.85:1) 2048×1080 : 2K Digital Cinema 3840×2160 : 4K UHDTV 4096×2160 : 4K Digital Cinema 7680×4320 : 8K UHDTV 15360×8640 : 16K Digital Cinema 61440×34560 : 64K Digital Cinema Sequences from newer films are scanned at 2,000, 4,000, or even 8,000 columns, called 2K, 4K, and 8K, for quality visual-effects editing on computers. IMAX, including IMAX HD and OMNIMAX: approximately 10,000×7,000 (7,000 lines) resolution. It is about 70 Mpix, which is currently highest-resolution single-sensor digital cinema camera (as of January 2012).[citation needed] 另外还有以下模式： 标准 分辨率 长宽比 1080p HD Widescreen 1920×1080 16:9 1080p SD 1440×1080 4:3 Rec. 601 1620×1080 3:2 FullHD+ 2160×1080 18:9 1.2.3 其他 位分辨率（BitResolution） 又称位深，是用来衡量每个像素储存信息的位数。这种分辨率决定可以标记为多少种色彩等级的可能性。一般常见的有8位、16位、24位或32位色彩。有时我们也将位分辨率称为颜色深度。所谓“位”，实际上是指“2”的平方次数，8位即是2的八次方，也就是8个2相乘，等于256。所以，一幅8位色彩深度的图象，所能表现的色彩等级是256级。 http://vod.sjtu.edu.cn/help/Article_Show.asp?ArticleID=308 比较全面的分辨率列表见 https://en.wikipedia.org/wiki/List_of_common_resolutions 1.3 BtiRatehttps://zh.wikipedia.org/wiki/%E6%AF%94%E7%89%B9%E7%8E%87 在电信和计算领域，比特率（英语：Bit rate，变量R）是单位时间内传输送或处理的比特的数量。比特率经常在电信领域用作连接速度、传输速度、信息传输速率和数字带宽容量的同义词。 在数字多媒体领域，比特率是单位时间播放连续的媒体如压缩后的音频或视频的比特数量。在这个意义上讲，它相当于术语数字带宽消耗量，或吞吐量。 比特率规定使用“比特每秒”（bit/s或bps）为单位，经常和国际单位制词头关联在一起，如“千”（kbit/s或kbps），“兆”(百万)（Mbit/s或Mbps），“吉”（Gbit/s或Gbps）和“太”（Tbit/s或Tbps）。 正式的“比特每秒”的缩写是“bit/s”（不是“bits/s”）。在一些非正式文章，经常使用“b/s”或“bps”缩写。在更不正式的地方，通常省略了“每秒”，简单地应用为“一个128千比特音频流”或“一个100兆比特网络”。 多媒体行业在指音频或者视频在单位时间内的数据传输率时通常使用码流或码率，单位是kbps（千位每秒）。 在数字多媒体领域，比特率代表了信息的数量，更详细地说，存储了一个记录的每单位时间。比特率和以下几个因素相关： 原始物质也许取样在不同的频率里 取样可能使用了不同数量的比特 数据可能按照不同的方式编码 信息可能用不同的算法或不同的程度进行数字压缩 通常，以上因素的选择的目的是在比特率的最小化和播放介质时最优化之间达到理想的平衡。 Understanding Bitrate, Resolution and Quality - Ooyala Community 解释了比特率与视频质量之间的关系： 每秒像素数 = 宽 x 高 x 每秒帧数 每像素比特数（bpp） = 视频比特率 / 每秒像素数 bpp在0.1左右的质量就很高了，再高的bpp不会产生可见的显著提升 bpp在0.03左右的质量算很低，低比特率的通常都没法看 找个例子算一下： 68,692,876字节，也就是549,543,008比特，这个视频时长为7分06秒，也就是426秒，帧率为25fps（上图没有这一项信息），那么$$BitRate=\frac{TotalBtis}{TotalSeconds}=\frac{549543008 bits}{426 s}=1290007 bps$$ $$PiexlsPerSecond=Width \times Height \times FramesPerSecond=1280 \times 720 \times 25=23040000 pps$$ $$BitsPerPixel=\frac{BitRate}{PiexlsPerSecond}=\frac{1290007 bps}{23040000 pps}=0.05599 bpp$$ 可以看到，落在了$[0.03, 0.1]$区间内，根据我本人的肉眼感受，这视频的质量也可以，也就是平时常见的720p的感觉。 如果要实时在线播放这个视频，除去控制需要的其他额外信息，每秒需要传输1290007 bits，也就是1259kb，也就是1.2Mb，因为运营商提供的宽带套餐只有1Mbps、2Mbps和更高的，所以下行带宽至少需要2M。 1.4 容器格式/视频文件格式https://zh.wikipedia.org/wiki/%E8%A7%86%E9%A2%91%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F#%E5%AE%B9%E5%99%A8%E6%A0%BC%E5%BC%8F 视频是现在电脑中多媒体系统中的重要一环。为了适应储存视频的需要，人们设定了不同的视频文件格式来把视频和音频放在一个文件中，以方便同时回放。 视频档 简介 扩展名 Flash Video 由Adobe Flash延伸出来的的一种流行网络视频封装格式。随着视频网站的丰富，这个格式已经非常普及。 flv AVI（Audio Video Interleave） 比较早的AVI是微软开发的。其含义是Audio Video Interactive，就是把视频和音频编码混合在一起存储。AVI也是最长寿的格式，已经存在10余年了，虽然发布过改版（V2.0于1996年发布），但已显老态。AVI格式上限制比较多，只能有一个视频轨道和一个音频轨道（现在有非标准插件可加入最多两个音频轨道），还可以有一些附加轨道，如文字等。AVI格式不提供任何控制功能。 avi WMV（Windows Media Video） 同样是微软开发的一组数字视频编解码格式的通称，ASF（Advanced Systems Format）是其封装格式。ASF封装的WMV档具有“数字版权保护”功能。 wmv/asf wmvhd MPEG（Moving Picture Experts Group） 是一个国际标准化组织（ISO）认可的媒体封装形式，受到大部分机器的支持。其存储方式多样，可以适应不同的应用环境。MPEG-4档的档容器格式在Part 1（mux）、14（asp）、15（avc）等中规定。MPEG的控制功能丰富，可以有多个视频（即角度）、音轨、字幕（位图字幕）等等。MPEG的一个简化版本3GP还广泛的用于准3G手机上。 dat（VCD） vob（DVD） mpg/mpeg mp4 3gp/3g2（手机） Matroska 是一种新的多媒体封装格式，这个封装格式可把多种不同编码的视频及16条或以上不同格式的音频和语言不同的字幕封装到一个Matroska Media档内。它也是其中一种开放源代码的多媒体封装格式。Matroska同时还可以提供非常好的交互功能，而且比MPEG更方便、强大。 mkv Real Video / Real Media（RM） 是由RealNetworks开发的一种档容器。它通常只能容纳Real Video和Real Audio编码的媒体。该档带有一定的交互功能，允许编写脚本以控制播放。RM，尤其是可变比特率的RMVB格式，没有复杂的Profile/Level，制作起来较H.264视频格式简单，非常受到网络上传者的欢迎。此外很多人仍有RMVB体积小高质量的错误认知，这个不太正确的观念也导致很多人倾向使用rmvb，事实上在相同码率下，rmvb编码和H.264这个高度压缩的视频编码相比，体积会较大。 rm/rmvb QuickTime File Format 是由苹果公司开发的容器。1998年2月11日，国际标准组织（ISO）认可QuickTime文件格式作为MPEG-4标准的基础。QT可存储的内容相当丰富，除了视频、音频以外还可支持图片、文字（文本字幕）等。 mov qt Ogg Media是一个完全开放性的多媒体系统项目，OGM（Ogg Media File）是其容器格式。OGM可以支持多视频、音频、字幕（文本字幕）等多种轨道。 ogg/ogv/oga MOD 格式是JVC生产的硬盘摄录机所采用的单元格式名称。 mod 1.5 Ogghttps://zh.wikipedia.org/wiki/Ogg Ogg是一个自由且开放标准的多媒体文件格式，由Xiph.Org基金会所维护。Ogg格式并不受到软件专利的限制，并设计用于有效率地流媒体和处理高品质的数字多媒体。 “Ogg”意指一种文件格式，可以纳入各式各样自由和开放源代码的编解码器，包含音效、视频、文字（像字幕）与元数据的处理。 在Ogg的多媒体框架下，Theora提供有损的视频层面，而通常用音乐导向的Vorbis编解码器作为音效层面。针对语音设计的压缩编解码器Speex和无损的音效压缩编解码器FLAC与OggPCM也可能作为音效层面使用。 “Ogg”这个词汇通常意指Ogg Vorbis此一音频文件格式，也就是将Vorbis编码的音效包含在Ogg的容器中所成的格式。在以往，.ogg此一扩展名曾经被用在任何Ogg支持格式下的内容；但在2007年，Xiph.Org基金会为了向后兼容的考量，提出请求，将.ogg只留给Vorbis格式来使用。Xiph.Org基金会决定创造一些新的扩展名和媒体格式来描述不同类型的内容， 像是只包含音效所用的.oga、 包含或不含声音的视频（涵盖Theora）所用的.ogv， 和可以包含任何比特流的.ogx。 2 声音编码https://en.wikipedia.org/wiki/Audio_coding_format An audio coding format (or sometimes audio compression format) is a content representation format for storage or transmission of digital audio (such as in digital television, digital radio and in audio and video files). Examples of audio coding formats include MP3, AAC, Vorbis, FLAC, and Opus. A specific software or hardware implementation capable of audio compression and decompression to/from a specific audio coding format is called an audio codec(音频编解码器) 2.1 AAChttps://zh.wikipedia.org/wiki/%E9%80%B2%E9%9A%8E%E9%9F%B3%E8%A8%8A%E7%B7%A8%E7%A2%BC 高级音频编码（Advanced Audio Coding，AAC），基于MPEG-2的有损声音编码技术 格式类型 有损数据压缩 扩展名： .aac .mp4 .m4a .m4b .m4p .m4v .3gp .m4r 2.2 Ogg Vorbishttps://zh.wikipedia.org/wiki/Vorbis Vorbis是一种有损音频压缩格式，由Xiph.Org基金会所领导并开放源代码的一个免费的开源软件项目。该项目为有损音频压缩产生音频编码格式和软件引用编码器/解码器（ 编解码器 ）。Vorbis通常以Ogg作为容器格式，所以常合称为Ogg Vorbis。 技术特点 32 kb/秒（-q-2）到500 kb/秒（-q10）的比特率。 采样率从8 kHz（窄带）到192 kHz（超频）。 支持采样精度 16bit\20bit\24bit\32bit。 采用可变比特率（VBR），动态调整比特率达到最佳的编码效果。 支持单声道、立体声、四声道和5.1环绕声道；支持多达255个音轨（多数据流的帧）。 可动态调节比特率，音频带宽和帧大小。 Vorbis使用了一种灵活的格式，能够在文件格式已经固定下来后还能对音质进行明显的调节和新算法调校。 可以封装在多种媒体容器格式中，如Ogg（ .oga）、Matroska（ .mka）、WebM（ .webm）等。 3 视频压缩/编码https://en.wikipedia.org/wiki/Video_coding_format A video coding format (or sometimes video compression format) is a content representation format for storage or transmission of digital video content (such as in a data file or bitstream). Examples of video coding formats include MPEG-2 Part 2, MPEG-4 Part 2, H.264 (MPEG-4 Part 10), HEVC, Theora, RealVideo RV40, VP9, and AV1. A specific software or hardware implementation capable of video compression and/or decompression to/from a specific video coding format is called a video codec(视频编解码器);…Video content encoded using a particular video coding format is normally bundled with an audio stream (encoded using an audio coding format) inside a multimedia container format such as AVI, MP4, FLV, RealMedia, or Matroska. As such, the user normally doesn’t have a H.264 file, but instead has a .mp4 video file, which is an MP4 container containing H.264-encoded video, normally alongside AAC-encoded audio. 3.1 Theorahttps://zh.wikipedia.org/wiki/Theora Theora是一个免权利金、开放格式的有损视频压缩技术，由Xiph.Org基金会开发，该基金会还开发了著名的声音编码技术Vorbis，以及多媒体容器文件格式Ogg。libtheora则是Xiph.Org基金会针对Theora格式的实现。[5][6] Theora是由On2 Technologies公司专属的VP3编码器，经过开放源代码后派生而来，目标是达成比MPEG-4 Part 2更好的编码效率。 浏览器原生支持 Mozilla Firefox在3.5之后的版本包括了Firefox Mobile（Fennec）。 Google Chrome在3.0.182.2之后的版本 including Chromium as of 14 July 2009.。 SeaMonkey在2.0之后的版本。 Konqueror在4.4.2之后的版本。 Opera在10.50之后的版本，在Opera 9.5的实验版本中也有支持。 3.2 H.264/MPEG-4 AVChttps://zh.wikipedia.org/wiki/H.264/MPEG-4_AVC H.264，又称为MPEG-4第10部分，高级视频编码（英语：MPEG-4 Part 10, Advanced Video Coding，缩写为MPEG-4 AVC）是一种面向块，基于运动补偿的视频编码标准 。到2014年，它已经成为高精度视频录制、压缩和发布的最常用格式之一。第一版标准的最终草案于2003年5月完成。…H.264因其是蓝光盘的其中一种编解码标准而著名，所有蓝光盘播放器都必须能解码H.264。它也被广泛用于网络流媒体数据如Vimeo、YouTube、以及iTunes Store，网络软件如Adobe Flash Player和Microsoft Silverlight，以及各种高清晰度电视陆地广播（ATSC，ISDB-T，DVB-T或DVB-T2），线缆（DVB-C）以及卫星（DVB-S和DVB-S2）。 Reference 每英寸像素 - 维基百科，自由的百科全书 Image resolution - Wikipedia 分辨率 - 维基百科，自由的百科全书 比特率 - 维基百科，自由的百科全书 视频文件格式 - 维基百科，自由的百科全书 Ogg - 维基百科，自由的百科全书 Audio coding format - Wikipedia 進階音訊編碼 - 维基百科，自由的百科全书 Vorbis - 维基百科，自由的百科全书 Video coding format - Wikipedia Theora - 维基百科，自由的百科全书 H.264/MPEG-4 AVC - 维基百科，自由的百科全书 基础知识――各种分辨率的知识 1080p - Wikipedia Graphics display resolution - Wikipedia 空间分辨率_百度百科 List of common resolutions - Wikipedia -]]></content>
      <tags>
        <tag>流媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种开源的媒体服务器对比]]></title>
    <url>%2F2018%2F05%2F06%2Fcompare-of-some-open-source-media-server%2F</url>
    <content type="text"><![CDATA[本文对几种开源的流媒体服务器做对比。 1 概念1.1 RTP实时传输协议 - 维基百科，自由的百科全书 实时传输协议（Real-time Transport Protocol或简写RTP）是一个网络传输协议，它是由IETF的多媒体传输工作小组1996年在RFC 1889中公布的。 RTP协议详细说明了在互联网上传递音频和视频的标准数据包格式。它一开始被设计为一个多播协议，但后来被用在很多单播应用中。RTP协议常用于流媒体系统（配合RTSP协议），视频会议和一键通（Push to Talk）系统（配合H.323或SIP），使它成为IP电话产业的技术基础。RTP协议和RTP控制协议RTCP一起使用，而且它是创建在UDP协议上的。 1.2 RTMP实时消息协议 - 维基百科，自由的百科全书 实时消息协议（英语：Real-Time Messaging Protocol，缩写RTMP）也称实时消息传输协议，是最初由Macromedia为通过互联网在Flash播放器与一个服务器之间传输流媒体音频、视频和数据而开发的一个专有协议。Macromedia后被Adobe Systems收购，该协议也已发布了不完整的规范供公众使用。 RTMP协议有许多变种： 默认使用TCP端口1935的纯粹（plain）协议。 RTMPS，通过一个TLS/SSL连接传输RTMP。 RTMPE，使用Adobe自有安全机制加密的RTMP。虽然实现的细节为专有，但该机制使用行业标准的密码学原函数。 RTMPT，用HTTP封装以穿透防火墙。RTMPT通常在TCP通信端口80和443上使用明文请求来绕过大多数的公司流量过滤。封装的会话中可能携带纯粹的RTMP、RTMPS或RTMPE数据包。 RTMFP, 使用UDP而非TCP的RTMP，取代RTMP Chunk Stream。Adobe Systems开发了安全的实时媒体流协议包，可以让最终用户直接地相互连接（P2P）。 1.3 WebRTChttps://webrtc.org/ WebRTC is a free, open project that provides browsers and mobile applications with Real-Time Communications (RTC) capabilities via simple APIs. The WebRTC components have been optimized to best serve this purpose. Our mission: To enable rich, high-quality RTC applications to be developed for the browser, mobile platforms, and IoT devices, and allow them all to communicate via a common set of protocols. The WebRTC initiative is a project supported by Google, Mozilla and Opera, amongst others. 支持的浏览器和平台： Chrome Firefox Opera Android iOS 特点： 基于浏览器，且主流浏览器都支持，跨平台能力强 默认P2P，但是需要TURN服务器作为fallback 自适应码率 webrtc 视频编码之 h264 自动调节分辨率 — newrtc webm - Does WebRTC support Adaptive Bitrate Streaming for video? - Stack Overflow 相关资料： 2013 Google I/O 大会上WebRTC的幻灯片 Getting Started with WebRTC - Sam Dutton WebRTC in the real world: STUN, TURN and signaling - Sam Dutton IETF Real-Time Communication in WEB-browsers (rtcweb) Working Group RFC7742 - WebRTC Video Processing and Codec Requirements 1.4 HLSHTTP Live Streaming - 维基百科，自由的百科全书 HTTP Live Streaming（缩写是HLS）是一个由苹果公司提出的基于HTTP的流媒体网络传输协议。它的工作原理是把整个流分成一个个小的基于HTTP的文件来下载，每次只下载一些。当媒体流正在播放时，客户端可以选择从许多不同的备用源中以不同的速率下载同样的资源，允许流媒体会话适应不同的数据速率。在开始一个流媒体会话时，客户端会下载一个包含元数据的extended M3U (m3u8) playlist文件，用于寻找可用的媒体流。 HLS只请求基本的HTTP报文，与实时传输协议（RTP）不同，HLS可以穿过任何允许HTTP数据通过的防火墙或者代理服务器。它也很容易使用内容分发网络（CDN）来传输媒体流。 2017年8月，RFC 8216发布，描述了HLS协议第7版的定义 RFC 8216 2 开源方案2.0 开源方案选型要考虑的因素：10 Tips for Choosing the Right WebRTC Open Source Media Server Framework 你是否深刻理解其代码？ 代码版本是否足够新？ 有谁在使用它？ 它的文档是否齐全？ 它可以debug吗？ 它可以伸缩吗？ 它使用哪种语言？ 对于媒体服务器而言，这种语言的性能是否足够？ 团队是否足够了解这门语言？ 是否适应你现有的Signaling范式？ 你在看的Media Server是否容易与你决定使用的STUN/TURN服务器集成 许可证是否适合你？ 谁在提供支持？ 很多成功的、被良好维护的开源项目背后都有一个商业模式，尤其是中小型的项目，这意味着有一个团队以此为谋生手段。 具备可选的付费支持意味着： 有人愿意全职来改善这东西，而不是作为爱好来维护。 如果你需要紧急帮助，只要花钱就能得到。 2.1 Red5官网 http://red5.org/Github https://github.com/Red5 非官方资料： red5-development-series 特性： Server 自动伸缩集群 Adaptive Stream Rebuffering 支持多协议： WebRTC RTMP RTSP HLS FLV MP4 实时流录制 离线部署 Browser 定制化HTML5播放器，支持旋转 实时HLS流 支持Flash作为后备方案 移动端SDK（iOS, Android） 自适应比特率发布 硬件编码和解码 定制视频源 直播摄像头切换 双路视频聊天 多方视频会议 Adaptive Stream Rebuffering 低延迟播放器 高级通知API 消息与RPC HD h.264, AAC编码高达4k 粗略的特性收费情况调查结果： - 开源版本 付费版本 自动伸缩集群 不支持 \$279/MO 多协议支持 支持 支持 转码 不支持 \$109/MO 离线部署 支持 \$999+/MO 来自Red5 open source vs Red5pro · rajdeeprath/red5-development-series Wiki 的数据 FEATURES RED5 OS RED5 PRO 实时流 &#10004; &#10004; 视频录制 &#10004; &#10004; WebSockets &#10004; &#10004; 移动端回放(HLS) Community &#10004; Adobe Flash 支持 &#10004; &#10004; WebRTC &#10006; &#10004; 第二屏 &#10006; &#10004; 自动伸缩 &#10006; &#10004; 集群 &#10006; &#10004; 移动端SDK &#10006; &#10004; 扩展文档 &#10006; &#10004; 客户支持 Community &#10004; 管理API (RESTful) &#10006; &#10004; 缺点： 开源版本很难用，官方几乎没有文档，即使有，也是几年前的了，也没有教程。 考察点总结： 考察点 OS PRO 语言 Java Java 最新发布日期 2017-01-12 ? 谁在用 ? Intel, LimeLight,…,etc（最经典的方案） 文档 &#10006; &#10004; 伸缩 &#10006; &#10004; debug ? ? 许可证 Apache License v2 付费 网络协议 HLS, WebSockets, RTSP, RTMP, RTMPT, RTMPS, RTMPE HLS, WebSockets, RTSP, RTMP, RTMPT, RTMPS, RTMPE, WebRTC 自适应码率 ? &#10004; 流控 ? ? 录制 ? &#10004; 回放 ? &#10004; 2.2 kurento主页：http://www.kurento.org/github：https://github.com/kurento 服务端采用C++实现：https://github.com/Kurento/kurento-media-server 特性： 支持HTTP、RTP和WebRTC 集成OpenCV，支持分析图像抽取信息 转码（这个是刚需了） 广播 提供Kurento API，可以在此基础上构建服务器的高层逻辑，并提供了Java和NodeJS的实现，基于此可以实现权限校验等业务逻辑 支持NAT穿越 文档地址：http://readthedocs.org/projects/doc-kurento/downloads/pdf/stable/ 考察点总结： 考察点 情况 语言 C++ 最新发布日期 2018-03-21 谁在用 没有业界权威在用，Github 800+Star 文档 比较全 伸缩 &#10006; debug ？ 许可证 LGPL v2.1 网络协议 HTTP, RTP, WebRTC 自适应码率 &#10004; 流控 ? 录制 &#10004; 回放 &#10004; 2.3 RestComm官网 https://www.restcomm.com/GitHub https://github.com/RestComm文档 https://www.restcomm.com/docs/ 免费版本功能受限 考察点总结： 考察点 情况 语言 Java 最新发布日期 2018-04-20 谁在用 ? 文档 齐全 伸缩 付费 debug ？ 许可证 AGPL-3.0 网络协议 WebRTC 自适应码率 &#10004; 流控 ? 录制 ? 回放 ? 2.4 Ant Media Server官网 https://antmedia.io/文档 https://antmedia.io/documentationGithub https://github.com/ant-media/ant-media-server Star Fork 服务端采用Java实现，是Red5-server的分叉版本 特性： 实时流发布 支持RTMP、RTSP、WebRTC、HLS WebRTC转RTMP适配器 IP摄像头支持 录制实时流（FLV、MP4和HLS容器） 同步重制流到社交媒体（企业版里Facebook和Youtube） 低延迟1:N WebRTC实时流（企业版） 实时流自适应比特率转换（FLV、MP4、HLS）（企业版） 考察点总结： 考察点 情况 语言 Java 最新发布日期 2018-05-01 谁在用 文档 齐全 伸缩 收费 debug ? 许可证 Apache License v2 网络协议 RTMP、RTSP、WebRTC、HLS 自适应码率 &#10004; 流控 ? 录制 &#10004; 回放 &#10004; 2.5 BigBlueButton官网 https://bigbluebutton.org/文档 http://docs.bigbluebutton.org/github https://github.com/bigbluebutton/bigbluebuttonStar Fork 采用Java开发，主要针对场景是授课 特性： 实时添加字幕 屏幕共享 分组讨论室 录制与回放 考察点总结： 考察点 情况 语言 Java 最新发布日期 谁在用 文档 伸缩 debug 许可证 网络协议 自适应码率 流控 录制 回放 2.6 NextRTC主页 https://nextrtc.org/Github https://github.com/mslosarz/nextrtc-signaling-server 考察点总结： 考察点 情况 语言 Java 最新发布日期 2018-04-05 谁在用 （估计没人） 文档 简单 伸缩 &#10004; debug &#10004; 许可证 MIT 网络协议 WebRTC 自适应码率 &#10004; 流控 &#10006; 录制 &#10006; 回放 &#10006; 2.7 OpenBroadcasterhttps://obsproject.com/https://github.com/obsproject TBD Comparison of streaming media systems - Wikipedia 名称 创建者 最新稳定版(发布日期) 最新发布日期 价钱 (USD) 许可证 支持媒体 Cameleon Yatko 1.0.7 (2016-11-11) 2016-11-11 Free 专有 Audio/Video Helix Universal Server RealNetworks 15.2.1 (2014-09-16) 2014-09-16 Free for 12 months (Basic) and \$1,000-\$10,000 专有 Audio/Video IIS Media Services Microsoft 4.1 (2011-11-09) 2011-11-09 Free 专有 Audio/Video/Data Nimble Streamer WMSPanel 2.15.1-3 (2016-03-21) 2016-03-21 Free 专有 Audio/Video Open Broadcaster Software OBS Project 0.657 (2015-11-24) 2015-11-24 Free GPL v2 Audio/Video OpenBroadcaster OpenBroadcaster 5.0.0 (2018-01-27) 2018-01-27 Free AGPLv3 Audio/Video Plex (software) Plex Media Server 1.0.3.2461-35f0caa (2016-07-28) 2016-07-28 Free GPL Audio/Video Red5 (open source) Infrared5/community 1.0.6 (2015-09-08) 2015-09-08 Free Apache License v2 Audio/Video/Data Unified Streaming Platform Unified Streaming 1.7.18 (2016-04-11) 2016-04-11 Free Trial license; Perpetual License; Amazon EC2 with embedded license; Microsoft Azure 专有 Audio/Video Unreal Media Server Unreal Streaming Technologies 12.0 (2017-04-03) 2017-04-03 \$995 perpetual license 专有 Audio/Video VLC media player VideoLAN 2.2.4 (2016-06-05) 2016-06-05 Free GPL v2 Audio/Video Wowza Streaming Engine Wowza Media Systems 4.5.0 build 18676 (2016-06-23)[2] 2016-06-23 Free Developer and Trial licenses; Subscription Licenses at \$65/month; Perpetual Pro License at \$1995; Amazon EC2 with embedded license starting at \$0.15/hour 专有 Audio/Video/Data -]]></content>
      <tags>
        <tag>直播</tag>
        <tag>流媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM GC相关问题]]></title>
    <url>%2F2018%2F04%2F02%2Fjvm-gc-issues%2F</url>
    <content type="text"><![CDATA[1. 概念http://blog.csdn.net/u011080472/article/details/51324422 1.0 并发和并行这两个名词都是并发编程中的概念，在谈论垃圾收集器的上下文语境中，它们可以解释如下。 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。 1.1 Minor GC 和 Full GC 新生代GC（Minor GC / Young GC / YGC）：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。 老年代GC（Major GC / Full GC / FGC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。 2 Java中什么样的对象才能作为gc root，gc roots有哪些呢？ http://blog.csdn.net/fenglibing/article/details/8928927 常说的GC(Garbage Collector) roots，特指的是垃圾收集器（Garbage Collector）的对象，GC会收集那些不是GC roots且没有被GC roots引用的对象。 一个对象可以属于多个root，GC root有几下种： Class - 由系统类加载器(system class loader)加载的对象，这些类是不能够被回收的，他们可以以静态字段的方式保存持有其它对象。我们需要注意的一点就是，通过用户自定义的类加载器加载的类，除非相应的java.lang.Class实例以其它的某种（或多种）方式成为roots，否则它们并不是roots，. Thread - 活着的线程 Stack Local - Java方法的local变量或参数 JNI Local - JNI方法的local变量或参数 JNI Global - 全局JNI引用 Monitor Used - 用于同步的监控对象 Held by JVM - 用于JVM特殊目的由GC保留的对象，但实际上这个与JVM的实现是有关的。可能已知的一些类型是：系统类加载器、一些JVM知道的重要的异常类、一些用于处理异常的预分配对象以及一些自定义的类加载器等。然而，JVM并没有为这些对象提供其它的信息，因此就只有留给分析分员去确定哪些是属于”JVM持有”的了。 Help - Eclipse Platform Garbage Collection RootsA garbage collection root is an object that is accessible from outside the heap. The following reasons make an object a GC root: System ClassClass loaded by bootstrap/system class loader. For example, everything from the rt.jar like java.util.* . JNI LocalLocal variable in native code, such as user defined JNI code or JVM internal code. JNI GlobalGlobal variable in native code, such as user defined JNI code or JVM internal code. Thread BlockObject referred to from a currently active thread block. ThreadA started, but not stopped, thread. Busy MonitorEverything that has called wait() or notify() or that is synchronized. For example, by calling synchronized(Object) or by entering a synchronized method. Static method means class, non-static method means object. Java LocalLocal variable. For example, input parameters or locally created objects of methods that are still in the stack of a thread. Native StackIn or out parameters in native code, such as user defined JNI code or JVM internal code. This is often the case as many methods have native parts and the objects handled as method parameters become GC roots. For example, parameters used for file/network I/O methods or reflection. FinalizableAn object which is in a queue awaiting its finalizer to be run. UnfinalizedAn object which has a finalize method, but has not been finalized and is not yet on the finalizer queue. UnreachableAn object which is unreachable from any other root, but has been marked as a root by MAT to retain objects which otherwise would not be included in the analysis. Java Stack FrameA Java stack frame, holding local variables. Only generated when the dump is parsed with the preference set to treat Java stack frames as objects. UnknownAn object of unknown root type. Some dumps, such as IBM Portable Heap Dump files, do not have root information. For these dumps the MAT parser marks objects which are have no inbound references or are unreachable from any other root as roots of this type. This ensures that MAT retains all the objects in the dump. java的gc为什么要分代？ - 知乎 作者：RednaxelaFX链接：https://www.zhihu.com/question/53613423/answer/135743258来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 所谓“GC roots”，或者说tracing GC的“根集合”，就是一组必须活跃的引用。例如说，这些引用可能包括： 所有Java线程当前活跃的栈帧里指向GC堆里的对象的引用；换句话说，当前所有正在被调用的方法的引用类型的参数/局部变量/临时值。 VM的一些静态数据结构里指向GC堆里的对象的引用，例如说HotSpot VM里的Universe里有很多这样的引用。 JNI handles，包括global handles和local handles（看情况） 所有当前被加载的Java类（看情况） Java类的引用类型静态变量（看情况） Java类的运行时常量池里的引用类型常量（String或Class类型）（看情况） String常量池（StringTable）里的引用 注意，是一组必须活跃的引用，不是对象。Tracing GC的根本思路就是：给定一个集合的引用作为根出发，通过引用关系遍历对象图，能被遍历到的（可到达的）对象就被判定为存活，其余对象（也就是没有被遍历到的）就自然被判定为死亡。注意再注意：tracing GC的本质是通过找出所有活对象来把其余空间认定为“无用”，而不是找出所有死掉的对象并回收它们占用的空间。GC roots这组引用是tracing GC的起点。要实现语义正确的tracing GC，就必须要能完整枚举出所有的GC roots，否则就可能会漏扫描应该存活的对象，导致GC错误回收了这些被漏扫的活对象。这就像任何递归定义的关系一样，如果只定义了递推项而不定义初始项的话，关系就无法成立——无从开始；而如果初始项定义漏了内容的话，递推出去也会漏内容。 那么分代式GC对GC roots的定义有什么影响呢？答案是：分代式GC是一种部分收集（partial collection）的做法。在执行部分收集时，从GC堆的非收集部分指向收集部分的引用，也必须作为GC roots的一部分。具体到分两代的分代式GC来说，如果第0代叫做young gen，第1代叫做old gen，那么如果有minor GC / young GC只收集young gen里的垃圾，则young gen属于“收集部分”，而old gen属于“非收集部分”，那么从old gen指向young gen的引用就必须作为minor GC / young GC的GC roots的一部分。 继续具体到HotSpot VM里的分两代式GC来说，除了old gen到young gen的引用之外，有些带有弱引用语义的结构，例如说记录所有当前被加载的类的SystemDictionary、记录字符串常量引用的StringTable等，在young GC时必须要作为strong GC roots，而在收集整堆的full GC时则不会被看作strong GC roots。 换句话说，young GC比full GC的GC roots还要更大一些。 那么分代有什么好处？对传统的、基本的GC实现来说，由于它们在GC的整个工作过程中都要“stop-the-world”，如果能想办法缩短GC一次工作的时间长度就是件重要的事情。如果说收集整个GC堆耗时太长，那不如只收集其中的一部分？于是就有好几种不同的划分（partition）GC堆的方式来实现部分收集，而分代式GC就是这其中的一个思路。这个思路所基于的基本假设大家都很熟悉了：weak generational hypothesis——大部分对象的生命期很短（die young），而没有die young的对象则很可能会存活很长时间（live long）。这是对过往的很多应用行为分析之后得出的一个假设。基于这个假设，如果让新创建的对象都在young gen里创建，然后频繁收集young gen，则大部分垃圾都能在young GC中被收集掉。由于young gen的大小配置通常只占整个GC堆的较小部分，而且较高的对象死亡率（或者说较低的对象存活率）让它非常适合使用copying算法来收集，这样就不但能降低单次GC的时间长度，还可以提高GC的工作效率。（参见三种基本算法的对比） 放几个传送门： JVM GC遍历一次新生代所有对象是否可达需要多久？- RednaxelaFX 的回答 - 知乎 有关 Copying GC 的疑问？- RednaxelaFX 的回答 - 知乎 但是！有些比较先进的GC算法是增量式（incremental）的，或者部分并发（mostly-concurrent），或者干脆完全并发（fully-concurrent）的。例如鄙司Azul Systems的Zing JVM里的C4 GC，就是一个完全并发的GC算法。它不存在“GC整个工作流程中都要把应用stop-the-world”的问题——从算法的设计上就不存在。然而C4却也是一个分两代的分代式GC。为什么呢？C4 GC的前身是Azul System的上一代JVM里的“Pauseless GC”算法，而Pauseless是一个完全并发但是不分代的GC。Oracle的HotSpot VM里的G1 GC，在最初设计的时候是不分代的部分并发+增量式GC，而后来在实际投入生产的时候使用的却也是分两代的分代式GC设计。现在Red Hat正在开发中的Shenandoah GC是一个并发GC，它目前的设计还是不分代的，但根据过往经验看，它后期渐渐发展为分代式的可能性极其高——如果这个项目能活足够久的话。对于这些GC来说，解决stop-the-world时间太长的问题并不是选择分代的主要原因。就Azul的Pauless到C4的发展历程来看，选择实现分代的最大好处是，GC能够应付的应用内存分配速率（allocation rate）可以得到巨大的提升。并发GC根本上要跟应用玩追赶游戏：应用一边在分配，GC一边在收集，如果GC收集的速度能跟得上应用分配的速度，那就一切都很完美；一旦GC开始跟不上了，垃圾就会渐渐堆积起来，最终到可用空间彻底耗尽的时候，应用的分配请求就只能暂时等一等了，等GC追赶上来。所以，对于一个并发GC来说，能够尽快回收出越多空间，就能够应付越高的应用内存分配速率，从而更好地保持GC以完美的并发模式工作。虽然并不是所有应用中的对象生命周期都完美吻合weak generational hypothesis的假设，但这个假设在很大范围内还是适用的，因而也可以帮助并发GC改善性能。 3 GC算法3.1 判断是否回收3.1.1 标记算法3.1.1.1 引用计数（Reference Counting）算法给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。 优点 实现简单，判断效率高 缺点 不能解决循环引用问题例如a引用了b，同时b又引用了a。 主流的Java虚拟机里面没有选用引用计数算法来管理内存，其中最主要的原因是它很难解决对象之间相互循环引用的问题。 3.1.1.2 可达性分析（Reachability Analysis）算法周志明. 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版）: 这个算法的基本思路就是通过一系列的称为”GC Roots“的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连（用图论的话来说，就是从GC Roots到这个对象不可达）时，则证明此对象是不可用的。 在Java语言中，可作为GCRoots的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中类静态属性引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI（即一般说的Native方法）引用的对象。 3.1.2 引用在JDK 1.2之后，Java对引用的概念进行了扩充，将引用分为了4种： 强引用（Strong Reference） 类似 Object obj = new Object()的引用 垃圾回收器永远不会回收被引用的对象 软引用（Soft Reference） 描述一些还有用但是非必需的对象 在系统将要发生内存溢出异常之前，会将这些被引用对象列入回收范围内 JDK 1.2之后提供了SoftReference类来实现 弱引用（Weak Reference） 也用于描述非必需对象，但强度比软引用更弱 被引用对象只能生存到下一次GC发生之前 JDK 1.2之后提供了WeakReference类来实现 虚引用（Phantom Reference） 也称为幽灵引用或者幻影引用，引用关系最弱 完全不影响生存时间 为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 在JDK 1.2之后，提供了PhantomReference类来实现虚引用。 这4种引用强度依次逐渐减弱。 3.1.3 二次标记周志明. 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版）: 即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。 如果这个对象被判定为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束，这样做的原因是，如果一个对象在finalize()方法中执行缓慢，或者发生了死循环（更极端的情况），将很可能会导致F-Queue队列中其他对象永久处于等待，甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移除出“即将回收”的集合；如果对象这时候还没有逃脱，那基本上它就真的被回收了。 3.1.4 回收方法区周志明. 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版）: 很多人认为方法区（或者HotSpot虚拟机中的永久代）是没有垃圾收集的，Java虚拟机规范中确实说过可以不要求虚拟机在方法区实现垃圾收集，而且在方法区中进行垃圾收集的“性价比”一般比较低：在堆中，尤其是在新生代中，常规应用进行一次垃圾收集一般可以回收70%～95%的空间，而永久代的垃圾收集效率远低于此。 永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。回收废弃常量与回收Java堆中的对象非常类似。以常量池中字面量的回收为例，假如一个字符串”abc”已经进入了常量池中，但是当前系统没有任何一个String对象是叫做”abc”的，换句话说，就是没有任何String对象引用常量池中的”abc”常量，也没有其他地方引用了这个字面量，如果这时发生内存回收，而且必要的话，这个”abc”常量就会被系统清理出常量池。常量池中的其他类（接口）、方法、字段的符号引用也与此类似。 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是“无用的类”： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样，不使用了就必然会回收。是否对类进行回收，HotSpot虚拟机提供了-Xnoclassgc参数进行控制，还可以使用-verbose:class以及-XX:+TraceClassLoading、-XX:+TraceClassUnLoading查看类加载和卸载信息，其中-verbose:class和-XX:+TraceClassLoading可以在Product版的虚拟机中使用，-XX:+TraceClassUnLoading参数需要FastDebug版的虚拟机支持。 在大量使用反射、动态代理、CGLib等ByteCode框架、动态生成JSP以及OSGi这类频繁自定义ClassLoader的场景都需要虚拟机具备类卸载的功能，以保证永久代不会溢出。 3.2 一般的算法 3.2.1 标记-清除（Mark-Sweep）算法周志明. 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版）: 最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程其实在前一节讲述对象标记判定时已经介绍过了。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其不足进行改进而得到的。它的主要不足有两个： 效率问题，标记和清除两个过程的效率都不高； 空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 3.2.2 复制（Copying）算法周志明. 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版）: 为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生夕死”的，所以并不需要按照1:1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 内存的分配担保就好比我们去银行借款，如果我们信誉很好，在98%的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。 JVM GC遍历一次新生代所有对象是否可达需要多久？ - 知乎：Copying GC算法的特征之一就是它的开销只跟活对象的多少（live data set）有关系，而跟它所管理的堆空间的大小没关系。 3.2.3 标记-整理（Mark-Compact）算法周志明. 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版）: 复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 3.2.4 分代收集（Generational Collection）算法周志明. 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版）: 当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记—清理”或者“标记—整理”算法来进行回收。 分代收集的好处参见此处。 3.2.5 三种基本算法对比 - mark-sweep mark-compact copying 速度 中等 最慢 最快 空间开销 少（但会堆积碎片） 少（不堆积碎片） 通常需要活对象的2倍大小（不堆积碎片） 移动对象？ 否 是 是 关于时间开销：mark-sweep：mark阶段与活对象的数量成正比，sweep阶段与整堆大小成正比mark-compact：mark阶段与活对象的数量成正比，compact阶段与活对象的大小成正比copying：与活对象大小成正比 如果把mark、sweep、compact、copying这几种动作的耗时放在一起看，大致有这样的关系：compaction &gt;= copying &gt; marking &gt; sweeping 还有 marking + sweeping &gt; copying（虽然compactiont与copying都涉及移动对象，但取决于具体算法，compact可能要先计算一次对象的目标地址，然后修正指针，然后再移动对象；copying则可以把这几件事情合为一体来做，所以可以快一些。另外还需要留意GC带来的开销不能只看collector的耗时，还得看allocator一侧的。如果能保证内存没碎片，分配就可以用pointer bumping方式，只有挪一个指针就完成了分配，非常快；而如果内存有碎片就得用freelist之类的方式管理，分配速度通常会慢一些。） 在分代式假设中，年轻代中的对象在minor GC时的存活率应该很低，这样用copying算法就是最合算的，因为其时间开销与活对象的大小成正比，如果没多少活对象，它就非常快；而且young gen本身应该比较小，就算需要2倍空间也只会浪费不太多的空间。而年老代被GC时对象存活率可能会很高，而且假定可用剩余空间不太多，这样copying算法就不太合适，于是更可能选用另两种算法，特别是不用移动对象的mark-sweep算法。 不过HotSpot VM中除了CMS之外的其它收集器都是会移动对象的，也就是要么是copying、要么是mark-compact的变种。 3.3 HotSpot的算法实现3.3.1 枚举根节点 从可达性分析中从GC Roots节点找引用链这个操作为例，可作为GC Roots的节点主要在全局性的引用（例如常量或类静态属性）与执行上下文（例如栈帧中的本地变量表）中，现在很多应用仅仅方法区就有数百兆，如果要逐个检查这里面的引用，那么必然会消耗很多时间。 另外，可达性分析对执行时间的敏感还体现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行——这里“一致性”的意思是指在整个分析期间整个执行系统看起来就像被冻结在某个时间点上，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果准确性就无法得到保证。这点是导致GC进行时必须停顿所有Java执行线程（Sun将这件事情称为”Stop The World“）的其中一个重要原因，即使是在号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。 由于目前的主流Java虚拟机使用的都是准确式GC（这个概念在第1章介绍Exact VM对Classic VM的改进时讲过），所以当执行系统停顿下来后，并不需要一个不漏地检查完所有执行上下文和全局的引用位置，虚拟机应当是有办法直接得知哪些地方存放着对象引用。在HotSpot的实现中，是使用一组称为OopMap的数据结构来达到这个目的的： 在类加载完成的时候，HotSpot就把对象内什么偏移量上是什么类型的数据计算出来， 在JIT编译过程中，也会在特定的位置记录下栈和寄存器中哪些位置是引用。 这样，GC在扫描时就可以直接得知这些信息了。 3.3.2 安全点（Safepoint） 在OopMap的协助下，HotSpot可以快速且准确地完成GC Roots枚举，但一个很现实的问题随之而来：可能导致引用关系变化，或者说OopMap内容变化的指令非常多，如果为每一条指令都生成对应的OopMap，那将会需要大量的额外空间，这样GC的空间成本将会变得很高。 实际上，HotSpot也的确没有为每条指令都生成OopMap，前面已经提到，只是在“特定的位置”记录了这些信息，这些位置称为安全点（Safepoint），即程序执行时并非在所有地方都能停顿下来开始GC，只有在到达安全点时才能暂停。Safepoint的选定既不能太少以致于让GC等待时间太长，也不能过于频繁以致于过分增大运行时的负荷。所以，安全点的选定基本上是以程序“是否具有让程序长时间执行的特征”为标准进行选定的——因为每条指令执行的时间都非常短暂，程序不太可能因为指令流长度太长这个原因而过长时间运行，“长时间执行”的最明显特征就是指令序列复用，例如方法调用、循环跳转、异常跳转等，所以具有这些功能的指令才会产生Safepoint。 对于Sefepoint，另一个需要考虑的问题是如何在GC发生时让所有线程（这里不包括执行JNI调用的线程）都“跑”到最近的安全点上再停顿下来（Mutator suspension）。这里有两种方案可供选择： 抢先式中断（Preemptive Suspension）不需要线程的执行代码主动去配合，在GC发生时，首先把所有线程全部中断，如果发现有线程中断的地方不在安全点上，就恢复线程，让它“跑”到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程从而响应GC事件。 主动式中断（Voluntary Suspension）思想是当GC需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志（Polling Point），各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。轮询标志的地方和安全点是重合的，另外再加上创建对象需要分配内存的地方。 简单的解释： In order to support precise enumeration, JIT compiler should do additional work, because only JIT knows exactly stack frame info and register contents. When JIT compiles a method, for every instruction, it can book-keep the root reference information in case the execution is suspended at that instruction. But to remember the info for every instruction is too expensive. It requires substantial space to store the information. This is also unnecessary, because only a few instructions will have the chances to be the suspension points in real execution. JIT only needs to book-keep information for those instruction points – they are called safe-points. Safe-point means it is a safe suspension point for root set enumeration. 进一步阐述：现代JVM中的Safe Region和Safe Point到底是如何定义和划分的? 作者：RednaxelaFX链接：https://www.zhihu.com/question/29268019/answer/43762165来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 其实在高度优化的现代JVM里，safepoint有几种不同的用法。GC safepoint是最常见、大家听说得最多的，但还有deoptimization safepoint也很重要。在HotSpot VM里，这两种safepoint目前实现在一起，但其实概念上它们俩没有直接联系，需要的数据不一样。无论是哪种safepoint，最简洁的定义是“A point in program where the state of execution is known by the VM”。这里“state of execution”特意说得模糊，是因为不同种类的safepoint需要的数据不一样。GC safepoint需要知道在那个程序位置上，调用栈、寄存器等一些重要的数据区域里什么地方包含了GC管理的指针；Deoptimization safepoint需要知道在那个程序位置上，原本抽象概念上的JVM的执行状态（所有局部变量、临时变量、锁，等等）到底分配到了什么地方，是在栈帧￼的具体某个slot还是在某个寄存器里，之类的。如果要触发一次GC，那么JVM里的所有Java线程都必须到达GC safepoint；如果要执行一次deoptimization，那么需要执行deoptimization的线程要在到达deoptimization safepoint之后才可以开始deoptimize。不同JVM实现会选用不同的位置放置safepoint。以HotSpot VM为例，在解释器里每条字节码的边界都可以是一个safepoint，因为HotSpot的解释器总是能很容易的找出完整的“state of execution”。而在JIT编译的代码里，HotSpot会在所有方法的临返回之前，以及所有非counted loop的循环的回跳之前放置safepoint。HotSpot的JIT编译器不但会生成机器码，还会额外在每个safepoint生成一些“调试符号信息”，以便VM能找到所需的“state of execution”。为GC生成的符号信息是OopMap，指出栈上和寄存器里哪里有GC管理的指针；为deoptimization生成的符号信息是debugInfo，指出如果要把当前栈帧从compiled frame转换为interpreted frame的话，要从哪里把相应的局部变量、临时变量、锁等信息找出来。之所以只在选定的位置放置safepoint是因为： 挂在safepoint的调试符号信息要占用空间。如果允许每条机器码都可以是safepoint的话，需要存储的数据量会很大（当然这有办法解决，例如用delta存储和用压缩） safepoint会影响优化。特别是deoptimization safepoint，会迫使JVM保留一些只有解释器可能需要的、JIT编译器认定无用的变量的值。本来JIT编译器可能可以发现某些值不需要而消除它们对应的运算，如果在safepoint需要这些值的话那就只好保留了。这才是更重要的地方，所以要尽量少放置safepoint 像HotSpot VM这样，在safepoint会生成polling代码询问VM是否要“进入safepoint”，polling也有开销所以要尽量减少。 还有一种情况是当某个线程在执行native函数的时候。此时该线程在执行JVM管理之外的代码，不能对JVM的执行状态做任何修改，因而JVM要进入safepoint不需要关心它。所以也可以把正在执行native函数的线程看作“已经进入了safepoint”，或者把这种情况叫做“在safe-region里”。JVM外部要对JVM执行状态做修改必须要通过JNI。所有能修改JVM执行状态的JNI函数在入口处都有safepoint检查，一旦JVM已经发出通知说此时应该已经到达safepoint就会在这些检查的地方停下来把控制权交给JVM。换一个JVM说，JRockit选择放置safepoint的地方在方法的入口以及循环末尾回跳之前，跟HotSpot略为不同。 3.3.3 Polling Point GC safe-point (or safepoint) and safe-region | Xiao-Feng Li For voluntary suspension, a more serious problem is the polling overhead. So the basic principles for polling point insertion are: Firstly, polling points should be frequent enough so that GC does not wait too long for a mutator to suspend, because other mutators might be waiting for GC to free the space in order to continue. Secondly, polling points should not be too frequent to introduce big runtime overhead. The best result is to have only adequate polling points that are necessary and sufficient. The mandatory polling points are the allocation sites. Allocation can trigger collection, so allocation site has to be a safe point. Long-time execution are always associated with method call or loop. So call sites and loop back sites are also expected polling points. Those are the sites for polling points in Harmony: allocation sites, call sites and loop back sites. Mostly the runtime overhead is smaller than 1%. 3.3.4 安全区域（Safe Region） 使用Safepoint似乎已经完美地解决了如何进入GC的问题，但实际情况却并不一定。Safepoint机制保证了程序执行时，在不太长的时间内就会遇到可进入GC的Safepoint。但是，程序“不执行”的时候呢？所谓的程序不执行就是没有分配CPU时间，典型的例子就是线程处于Sleep状态或者Blocked状态，这时候线程无法响应JVM的中断请求，“走”到安全的地方去中断挂起，JVM也显然不太可能等待线程重新被分配CPU时间。对于这种情况，就需要安全区域（Safe Region）来解决。 安全区域是指在一段代码片段之中，引用关系不会发生变化。在这个区域中的任意地方开始GC都是安全的。我们也可以把Safe Region看做是被扩展了的safe-point。 在线程执行到SafeRegion中的代码时，首先标识自己已经进入了Safe Region，那样，当在这段时间里JVM要发起GC时，就不用管标识自己为Safe Region状态的线程了。在线程要离开Safe Region时，它要检查系统是否已经完成了根节点枚举（或者是整个GC过程），如果完成了，那线程就继续执行，否则它就必须等待直到收到可以安全离开Safe Region的信号为止。 GC safe-point (or safepoint) and safe-region | Xiao-Feng Li In safe-point design, the mutator polling for GC event will respond if the event is triggered. It responds by setting a ready flag when it’s sure to suspend. Then the GC can proceed with root set enumeration. This is a hand-shaking protocol. Safe-region just follows this protocol. The mutator sets the ready flag when it enters a safe-region. Before it leaves the region, it checks if GC has finished its enumeration (or collection), and no longer needs the mutator under suspension state. If it’s true, it goes ahead and leaves the region; otherwise, it suspends itself as in a safe-point. In Harmony implementation, we insert suspend_enable and suspend_disable to delimit the scope of safe-region. 4 各种垃圾收集器的异同4.1 HotSpot Memory Management in the Java HotSpot™ Virtual Machine 4.1.1 GC自适应的调节策略（GC Ergonomics） Useful JVM Flags - Part 6 (Throughput Collector) - codecentric AG Blog The throughput collectors offer an interesting (but common, at least on modern JVMs) mechanism to improve user-friendliness of GC configuration. This mechanism is part of what is known as “ergonomics”, a concept introduced for HotSpot with Java 5. With ergonomics, the garbage collector may dynamically apply modifications to the different heap areas sizes as well as the GC settings if it has evidence that these modifications would improve GC performance. The precise meaning of “improve GC performance” may be specified by the user via the flags -XX:GCTimeRatio and -XX:MaxGCPauseMillis (see below). It is important to know that ergonomics is activated by default – which is fine, as adaptive behavior is one of the biggest strengths of the JVM. Still, sometimes we may have a pretty clear idea of what settings are best for a particular application, and in these cases we might not want the JVM to mess around with our settings. Whenever we find ourselves in such a situation, we may consider deactivating some of the ergonomics by setting -XX:-UseAdaptiveSizePolicy. Garbage Collection Ergonomics Garbage Collector ErgonomicsThe following changes take effect with J2SE 5.0. Implementation of -XX:+UseAdaptiveSizePolicy Used by Parallel Garbage Collector Changed The implementation of -XX:+UseAdaptiveSizePolicy used by default with the -XX:+UseParallelGC garbage collector has changed to consider three goals: a desired maximum GC pause goal a desired application throughput goal minimum footprint Suggested strategyDo not choose a maximum value for the heap unless you know that the heap is greater than the default maximum heap size. Choose a throughput goal that is sufficient for your application. In an ideal situation the heap will grow to a value (less than the maximum) that will support the chosen throughput goal. If the heap grows to its maximum, the throughput cannot be met within that maximum. Set the maximum heap as large as you can, but no larger than the size of physical memory on the platform, and execute the application again. If the throughput goal can still not be met, then it is too high for the available memory on the platform. If the throughput goal can be met but there are pauses that are too long, select a pause time goal. This will likely mean that your throughput goal will not be met, so choose values that are an acceptable compromise for the application. 4.1.2 Fast Allocation这个概念应该不重要，很多地方都没提到，就Memory Management in the Java HotSpot™ Virtual Machine提到了这个概念。 4.1.3 Serial Collector单线程的收集器，是JVM在client模式下运行的默认收集器。 Serial收集器新生代垃圾收集器，采用复制算法。 优点： 简单而高效对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得较高的收集效率 4.1.4 Serial Old CollectorSerial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。如果在Server模式下，那么它主要还有两大用途：一种用途是在JDK 1.5以及之前的版本中与Parallel Scavenge收集器搭配使用，另一种用途就是作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。 4.1.5 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数（例如：-XX:SurvivorRatio、-XX:PretenureSizeThreshold、-XX:HandlePromotionFailure等）、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一样，在实现上，这两种收集器也共用了相当多的代码。 ParNew收集器除了多线程收集之外，其他与Serial收集器相比并没有太多创新之处，但它却是许多运行在Server模式下的虚拟机中首选的新生代收集器，其中有一个与性能无关但很重要的原因是，除了Serial收集器外，目前只有它能与CMS收集器配合工作。在JDK 1.5时期，HotSpot推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器——CMS收集器（Concurrent Mark Sweep，本节稍后将详细介绍这款收集器），这款收集器是HotSpot虚拟机中第一款真正意义上的并发（Concurrent）收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作， 不幸的是，CMS作为老年代的收集器，却无法与JDK 1.4.0中已经存在的新生代收集器Parallel Scavenge配合工作，所以在JDK 1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Serial收集器中的一个。ParNew收集器也是使用-XX:+UseConcMarkSweepGC选项后的默认新生代收集器，也可以使用-XX:+UseParNewGC选项来强制指定它。ParNew收集器在单CPU的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个CPU的环境中都不能百分之百地保证可以超越Serial收集器。当然，随着可以使用的CPU的数量的增加，它对于GC时系统资源的有效利用还是很有好处的。它默认开启的收集线程数与CPU的数量相同，在CPU非常多（譬如32个，现在CPU动辄就4核加超线程，服务器超过32个逻辑CPU的情况越来越多了）的环境下，可以使用-XX:ParallelGCThreads参数来限制垃圾收集的线程数。 4.1.6 Parallel Scavenge 收集器Parallel Scavenge收集器是新生代垃圾收集器，使用复制算法，也是并行的多线程收集器。与ParNew收集器相比，很多相似之处，但是Parallel Scavenge收集器更关注可控制的吞吐量。吞吐量越大，垃圾收集的时间越短，则用户代码则可以充分利用CPU资源，尽快完成程序的运算任务。 Parallel Scavenge收集器使用两个参数控制吞吐量： XX:MaxGCPauseMillis 控制最大的垃圾收集停顿时间 XX:GCRatio 直接设置吞吐量的大小。 直观上，只要最大的垃圾收集停顿时间越小，吞吐量是越高的，但是GC停顿时间的缩短是以牺牲吞吐量和新生代空间作为代价的。比如原来10秒收集一次，每次停顿100毫秒，现在变成5秒收集一次，每次停顿70毫秒。停顿时间下降的同时，吞吐量也下降了。 除此之外，Parallel Scavenge收集器还可以设置参数-XX:+UseAdaptiveSizePocily来动态调整停顿时间或者最大的吞吐量，这种方式称为GC自适应调节策略（GC Ergonomics），这点是ParNew收集器所没有的。 4.1.7 Parallel Old 收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。这个收集器是在JDK 1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old（PS MarkSweep）收集器外别无选择（还记得上面说过Parallel Scavenge收集器无法与CMS收集器配合工作吗？）。由于老年代Serial Old收集器在服务端应用性能上的“拖累”，使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，由于单线程的老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew加CMS的组合“给力”。直到Paralle lOld收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。 4.1.8 Concurrent Mark-Sweep (CMS) CollectorCMS收集器（Concurrent Mark Sweep）的目标就是获取最短回收停顿时间。在注重服务器的响应速度，希望停顿时间最短，则CMS收集器是比较好的选择。 整个执行过程分为以下4个步骤： 初始标记 并发标记 重新标记 并发清除 初始标记和重新标记这两个步骤仍然需要暂停Java执行线程，初始标记只是标记GC Roots能够直接关联到的对象，并发标记就是执行GC Roots Tracing的过程，而重新标记就是为了修正并发标记期间因用户程序执行而导致标记发生变动使得标记错误的记录。其执行过程如下： 不足： CMS收集器对CPU资源非常敏感。其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。CMS默认启动的回收线程数是（CPU数量+3）/4，也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个（譬如2个）时，CMS对用户程序的影响就可能变得很大，如果本来CPU负载就比较大，还分出一半的运算能力去执行收集器线程，就可能导致用户程序的执行速度忽然降低了50%，其实也让人无法接受。为了应付这种情况，虚拟机提供了一种称为“增量式并发收集器”（Incremental Concurrent MarkSweep/i-CMS）的CMS收集器变种，所做的事情和单CPU年代PC机操作系统使用抢占式来模拟多任务机制的思想一样，就是在并发标记、清理的时候让GC线程、用户线程交替运行，尽量减少GC线程的独占资源的时间，这样整个垃圾收集的过程会更长，但对用户程序的影响就会显得少一些，也就是速度下降没有那么明显。实践证明，增量时的CMS收集器效果很一般，在目前版本中，i-CMS已经被声明为”deprecated”，即不再提倡用户使用。 CMS收集器无法处理浮动垃圾（Floating Garbage），可能出现”Concurrent Mode Failure”失败而导致另一次Full GC的产生。由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。在JDK 1.5的默认设置下，CMS收集器当老年代使用了68%的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高参数-XX:CMSInitiatingOccupancyFraction的值来提高触发百分比，以便降低内存回收次数从而获取更好的性能，在JDK 1.6中，CMS收集器的启动阈值已经提升至92%。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次”Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说参数-XX:CMSInitiatingOccupancyFraction设置得太高很容易导致大量”Concurrent Mode Failure”失败，性能反而降低。 CMS是一款基于“标记—清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。为了解决这个问题，CMS收集器提供了一个-XX:+UseCMSCompactAtFullCollection开关参数（默认就是开启的），用于在CMS收集器顶不住要进行FullGC时开启内存碎片的合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但停顿时间不得不变长。虚拟机设计者还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction，这个参数是用于设置执行多少次不压缩的Full GC后，跟着来一次带压缩的（默认值为0，表示每次进入Full GC时都进行碎片整理）。 4.1.9 Garbage-First（G1） CollectorG1（Garbage-First）收集器是现今收集器技术的最新成果之一，之前一直处于实验阶段，直到jdk 7u4之后，才正式作为商用的收集器。 与前几个收集器相比，G1收集器有以下特点： 并行与并发 分代收集（仍然保留了分代的概念） 空间整合（整体上属于“标记-整理”算法，不会导致空间碎片） 可预测的停顿（比CMS更先进的地方在于能让使用者明确指定一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒） 此外，G1收集器将Java堆划分为多个大小相等的Region（独立区域），新生代与老年代都是一部分Region的集合，G1的收集范围则是这一个个Region（化整为零）。 G1的工作过程如下： 初始标记（Initial Marking） 并发标记（Concurrent Marking） 最终标记（Final Marking） 筛选回收（Live Data Counting and Evacuation） 初始标记阶段仅仅只是标记一下GC Roots能够直接关联的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段的用户程序并发运行的时候，能在正确可用的Region中创建对象，这个阶段需要暂停线程。并发标记阶段从GC Roots进行可达性分析，找出存活的对象，这个阶段是由用户线程并发执行的。最终标记阶段则是修正在并发标记阶段因为用户程序的并发执行而导致标记产生变动的那一部分记录，这部分记录被保存在Remembered Set Logs中，最终标记阶段再把Logs中的记录合并到Remembered Set中，这个阶段是并行执行的，仍然需要暂停用户线程。最后在筛选阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间制定回收计划。整个执行过程如下： 5 GC相关参数 client/serrver端不同的GC方式： Sun JDK HotSpot虚拟机GC组合方式： 6 GC性能指标Memory Management in the Java HotSpot™ Virtual Machine 6.1 Throughput 吞吐量 the percentage of total time not spent in garbage collection, considered over long periods of time （在一段长时间内，在垃圾收集中花费的总时间的百分比） 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即：吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 6.2 Garbage collection overhead GC耗费 the inverse of throughput, that is, the percentage of total time spent in garbage collection （吞吐量的倒数，也就是垃圾收集总时间的百分比） 6.3 Pause time 停顿时间 the length of time during which application execution is stopped while garbage collection is occurring. （垃圾收集发生时停止应用程序执行的时间） 6.4 Frequency of collection（GC的频率）6.5 Footprint a measure of size, such as heap size 6.6 Promptness 速度 the time between when an object becomes garbage and when the memory becomesavailable. （从对象变为垃圾到内存变为可用的时间可用。） Reference Memory Management in the Java HotSpot™ Virtual Machine 【深入理解JVM】：HotSpot垃圾收集器 - CSDN博客 周志明. 深入理解Java虚拟机：JVM高级特性与最佳实践（第2版） GC safe-point (or safepoint) and safe-region | Xiao-Feng Li java的gc为什么要分代？ - 知乎 现代JVM中的Safe Region和Safe Point到底是如何定义和划分的? 并发垃圾收集器（CMS）为什么没有采用标记-整理算法来实现？ - 讨论 - 高级语言虚拟机 - ITeye群组 JVM GC遍历一次新生代所有对象是否可达需要多久？ - 知乎 Useful JVM Flags - Part 6 (Throughput Collector) - codecentric AG Blog Garbage Collection Ergonomics]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebSocket简介]]></title>
    <url>%2F2018%2F03%2F15%2Fwebsocket-abstract%2F</url>
    <content type="text"><![CDATA[1 概念1.1 comet天不生WS，万古如长夜。 在WebSocket出现之前，服务端主动推送消息给浏览器的需求就已经存在了，但是HTTP这个协议本来就不是设计用来进行双向通信的，所以机智的工程师们采取了各种hack的方式来实现这种功能，这些实现方式都统称为comet。 但是无可避免地，这些技术都存在缺陷，既然WebSocket都出现了，那么这些技术大概也能进博物馆了，除了实现Fallback方案时需要去了解之外，这些技术感觉价值不大了，所以不展开说，如果读者有兴趣，可以根据关键词找资料去了解。 Comet is a web application model in which a long-held HTTPS request allows a web server to push data to a browser, without the browser explicitly requesting it https://en.wikipedia.org/wiki/Comet_(programming)) With long-polling we set the bar to cross-browser push. With XHR streaming and ActiveXObject(’htmlfile’) we raised it to cross-browser streaming. With SSE we’ve been trying to raise the bar to native, cross-browser streaming 1.1.1 实现1.1.1.1 Streaming1.1.1.1.1 Hidden iframe1.1.1.1.2 XMLHttpRequest1.1.1.2 Ajax Long-pulling1.1.1.2.1 XMLHttpRequest long polling1.1.1.2.2 Script tag long polling1.2 WebSocket协议WebSocket是一种在单个TCP连接上进行全双工通讯的协议。是有别于HTTP的另一个TCP协议，它们都属于OSI模型中的第七层: RFC 6455 states that WebSocket “is designed to work over HTTP ports 80 and 443 as well as to support HTTP proxies and intermediaries” thus making it compatible with the HTTP protocol. 原文为： The WebSocket Protocol attempts to address thegoals of existing bidirectional HTTP technologies in the context ofthe existing HTTP infrastructure; as such, it is designed to workover HTTP ports 80 and 443 as well as to support HTTP proxies andintermediaries, even if this implies some complexity specific to thecurrent environment. 1.2.1 握手 To establish a WebSocket connection, the client sends a WebSocket handshake request, for which the server returns a WebSocket handshake response, as shown in the example below: 客户端请求 1234567GET / HTTP/1.1Upgrade: websocket Connection: UpgradeHost: example.comOrigin: http://example.comSec-WebSocket-Key: sN9cRrP/n9NdMgdcy2VJFQ==Sec-WebSocket-Version: 13 服务器回应 12345HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: fFBooB7FAkLlXgRSz0BT3v4hq5s=Sec-WebSocket-Location: ws://example.com/ The handshake resembles HTTP in allowing servers to handle HTTP connections as well as WebSocket connections on the same port. Once the connection is established, communication switches to a bidirectional binary protocol which doesn’t conform to the HTTP protocol. 1.2.2 FrameWebSocket传输的数据是基于帧的，一帧数据里包含两部分：数据主体和控制帧 there are types for textual data (which is interpreted as UTF-8 [RFC3629]text), binary data (whose interpretation is left up to theapplication), and control frames (which are not intended to carrydata for the application but instead for protocol-level signaling,such as to signal that the connection should be closed) 为了避免混淆网络中间件（例如拦截代理）以及出于安全原因， 客户端必须屏蔽（mask）所有发送给服务器的帧，如果服务器收到了未屏蔽的帧，必须关闭连接。在这种情况下，服务器可以发送一个Close帧，状态码为1002。 服务器不能屏蔽任何发送给客户端的帧，如果客户端收到了屏蔽过的帧，必须关闭连接。这种情况下，也可以利用状态码1002。 基本的帧协议（The Base Framing Protocol）简单地定义以下部分： Frame： opcode：操作码，用于定义一个帧的类型 Payload data Extension data： payload length：载荷长度 designated locations：指定位置 Application Data 1.2.3 心跳一端发送Ping帧，另一端发送Pong帧作为响应 The Ping frame contains an opcode of 0x9. The Pong frame contains an opcode of 0xA. 1.2.4 优点 较少的控制开销。在连接创建后，服务器和客户端之间交换数据时，用于协议控制的数据包头部相对较小。在不包含扩展的情况下，对于服务器到客户端的内容，此头部大小只有2至10字节（和数据包长度有关）；对于客户端到服务器的内容，此头部还需要加上额外的4字节的掩码。相对于HTTP请求每次都要携带完整的头部，此项开销显著减少了。 更强的实时性。由于协议是全双工的，所以服务器可以随时主动给客户端下发数据。相对于HTTP请求需要等待客户端发起请求服务端才能响应，延迟明显更少；即使是和Comet等类似的长轮询比较，其也能在短时间内更多次地传递数据。 保持连接状态。于HTTP不同的是，Websocket需要先创建连接，这就使得其成为一种有状态的协议，之后通信时可以省略部分状态信息。而HTTP请求可能需要在每个请求都携带状态信息（如身份认证等）。 更好的二进制支持。Websocket定义了二进制帧，相对HTTP，可以更轻松地处理二进制内容。 可以支持扩展。Websocket定义了扩展，用户可以扩展协议、实现部分自定义的子协议。如部分浏览器支持压缩等。 更好的压缩效果。相对于HTTP压缩，Websocket在适当的扩展支持下，可以沿用之前内容的上下文，在传递类似的数据时，可以显著地提高压缩率。 2 API2.1 JavaScript APIThe WebSocket API: The WebSocket Interface123456789101112131415161718192021222324252627interface WebSocket : EventTarget &#123; readonly attribute DOMString url; // ready state const unsigned short CONNECTING = 0; const unsigned short OPEN = 1; const unsigned short CLOSING = 2; const unsigned short CLOSED = 3; readonly attribute unsigned short readyState; readonly attribute unsigned long bufferedAmount; // networking attribute EventHandler onopen; attribute EventHandler onerror; attribute EventHandler onclose; readonly attribute DOMString extensions; readonly attribute DOMString protocol; void close([Clamp] optional unsigned short code, optional DOMString reason); // messaging attribute EventHandler onmessage; attribute DOMString binaryType; void send(DOMString data); void send(Blob data); void send(ArrayBuffer data); void send(ArrayBufferView data);&#125;; 2.2 Java API2.2.1 Java EEJSR 356, Java API for WebSocket JSR 356是Java EE 7标准的一部分。 开源实现有： GlassFish Jetty 9.1.+ 以下是一个典型的例子： 服务端由Java实现，用JSR 356的一个实现来处理WebSocket协议的细节。客户端中，JavaFX客户端可以依赖任何符合JSR 356的客户端来处理WebSocket协议，而其他客户端则可以使用符合RFC 6455的实现来与服务器通信。 兼容JSR 356的好处： 防止供应商锁定（vendor-lock），可以自由地选择库和应用服务器 JSR 356的缺点： Learning Spring Boot 2.0: Simplify the development of lightning fast … - Greg L. Turnquist - Google 图书 基于Servlet 3.1 目前兼容Servlet 3.1的有tomcat，但是如果在使用Netty，则不兼容了 J2EE标准前景不太好 J2EE标准发布比较慢，RFC6455在2011年12月就正式发布了，而相应的JSR 356却在2013年5月才发布，相应地，Jetty在2013年11月才发布对JSR 356支持的稳定版本，从RFC标准发布到JSR标准实现之间相距了2年。 考虑到在一般的场景下，WebSocket客户端都不是Java客户端，所以花费精力去适配JSR 356没必要，别的语言才不管J2EE的标准。 2.2.2 非Java EE实现了RFC6455标准即可。 3 Fallback目前主流的浏览器基本都支持WebSocket，但是，IE 10（不含10）以下的版本不支持，而这些版本的IE用户量还是不少的，因此需要有后备方案。 3.1 sockjs项目主页：https://github.com/sockjs/sockjs-client The API should follow HTML5 Websockets API as closely as possible. All the transports must support cross domain connections out of the box. It’s possible and recommended to host a SockJS server on a different server than your main web site. There is support for at least one streaming protocol for every major browser. Streaming transports should work cross-domain and should support cookies (for cookie-based sticky sessions). Polling transports are used as a fallback for old browsers and hosts behind restrictive proxies. Connection establishment should be fast and lightweight. No Flash inside (no need to open port 843 - which doesn’t work through proxies, no need to host ‘crossdomain.xml’, no need to wait for 3 seconds in order to detect problems) API尽量接近HTML5 WebSocket的API 开箱即用地对所有协议支持跨域连接，可以在主站点以外架设SockJS服务器，并且也推荐这样做。 对每个主流的浏览器至少支持一种streaming协议 流传输（streaming transports）应该能跨域工作，并且支持cookies（基于cookies的会话） Polling传输作为一种后备方案，支持老版本的浏览器和被限制性代理所限制的主机 连接的建立应该是快速和轻量级的 不包含Flash（不必开启843端口） 协议：sockjs-protocol-0.3.3.py 在底层的实现上，SockJS会首先尝试使用原生的WebSocket，如果失败了，再根据浏览器来决定采取相应的传输协议。 Under the hood SockJS tries to use native WebSockets first. If that fails it can use a variety of browser-specific transport protocols and presents them through WebSocket-like abstractions. Supported transports, by browser (html served from http:// or https://) _Browser_ _Websockets_ _Streaming_ _Polling_ IE 6, 7 no no jsonp-polling IE 8, 9 (cookies=no) no xdr-streaming &dagger; xdr-polling &dagger; IE 8, 9 (cookies=yes) no iframe-htmlfile iframe-xhr-polling IE 10 rfc6455 xhr-streaming xhr-polling Chrome 6-13 hixie-76 xhr-streaming xhr-polling Chrome 14+ hybi-10 / rfc6455 xhr-streaming xhr-polling Firefox &lt;10 no &Dagger; xhr-streaming xhr-polling Firefox 10+ hybi-10 / rfc6455 xhr-streaming xhr-polling Safari 5.x hixie-76 xhr-streaming xhr-polling Safari 6+ rfc6455 xhr-streaming xhr-polling Opera 10.70+ no &Dagger; iframe-eventsource iframe-xhr-polling Opera 12.10+ rfc6455 xhr-streaming xhr-polling Konqueror no no jsonp-polling 4 实现4.1 Nettyhttp://netty.io/ 4.2 Undertowhttp://undertow.io/ 4.3 Jetty4.4 Vert.xhttp://http//vertx.io 4.5 Spray-WebSockethttps://github.com/dcaoyuan/spray-websocket 4.6 nodejs-websockethttps://github.com/sitegui/nodejs-websocket 4.7 Grizzlyhttps://javaee.github.io/grizzly/ 4.8 Go5 进一步封装的协议为什么不直接使用WebSocket协议？ 数据的解析是未定义的。 WebSocket支持text和binary两种传输数据格式，无论是哪种格式，客户端和服务器都需要约定如何包装、解析数据。因此在WebSocket协议的基础上，还需要再定义一层协议，客户端和服务端才能完成协作。 兼容Fallback方案。 当WebSocket不能工作的时候（例如在IE 6上），采用后备方案实现消息推送，除了通信的实现方式改变之外，数据解析的协议应当保持不变。 5.1 STOMP Simple (or Streaming) Text Orientated Messaging Protocol 的缩写。 项目主页：http://stomp.github.io STOMP是基于帧的消息协议。 A frame consists of a command, a set of optional headers and an optional body. It is an alternative to other open messaging protocols such as AMQP and implementation specific wire protocols used in JMS brokers such as OpenWire. STOMP is text based but also allows for the transmission of binary messages. 5.1.1 采用STOMP的理由 目前RabbitMQ和Spring都支持STOMP协议，表明STOMP已经是一个主流的消息协议之一，基于主流的协议，可以替换各种实现； STOMP协议的帧格式比较简洁，由command + headers + body构成，各部分使用EOL进行划分，冗余字符较少，不会浪费带宽。（EOL（end-of-line，consists of an OPTIONAL carriage return (octet 13) followed by a REQUIRED line feed (octet 10)）） 支持发布/订阅模型和事务 每个STOMP客户端既可以作为生产者，也可以作为消费者 5.1.2 帧STOMP消息帧由以下部分组成： Command Headers（Optional） Body（Optional） 示例：12345COMMANDheader1:value1header2:value2Body^@ 5.1.3 Heart-beating略。 5.1.4 发布-订阅模型通过以下帧来控制： SUBSCRIBE UNSUBSCRIBE 在RFC 6455里，限制了一个客户端到一个host的连接数： If the client already has a WebSocket connection to the remote host (IP address) identified by /host/ and port /port/ pair, even if the remote host is known by another name, the client MUST wait until that connection has been established or for that connection to have failed. There MUST be no more than one connection in a CONNECTING state. If multiple connections to the same IP address are attempted simultaneously, the client MUST serialize them so that there is no more than one connection at a time running through the following steps. 因此，如果直接利用原始的WebSocket全双工通信连接进行通信，在适配业务逻辑的时候很可能会耗费大量的工作，而通过发布-订阅模型，则可以复用同一个连接实现不同主题消息的分发 5.1.5 事务通过以下帧来控制： ACK NACK BEGIN COMMIT ABORT 5.2 SocketIOsocketio/socket.io-protocol: Socket.IO 1.0 Protocol specification and parser component / node.js module. SocketIO实际上也定义了发布订阅模型。通过namespace来订阅不同的『主题』，namespace下继续划分room，，类似于二级主题。 5.2.1 协议定义5.2.2 实现5.2.2.1 socket.iohttps://github.com/socketio/socket.io/ 5.2.2.2 netty-socketiohttps://github.com/mrniko/netty-socketio 6 测试https://github.com/crossbario/autobahn-testsuite 7 FAQ7.1 Socket和WebSocket有什么关系？就像Java和JavaScript，并没有什么太大的关系，但又不能说完全没关系。 通常所说的Socket API，是指操作系统中（也可能不是操作系统）提供的对于传输层（TCP/UDP）抽象的接口。 WebSocket是一种在单个TCP连接上进行全双工通讯的协议。 WebSocket这个名称的由来： 08年6月18日，一群WHATWG的工程师在讨论一些技术问题，一个工程师提到说「我们之前讨论的那个东西，不要叫TCPConnection 了，还是起个别的名字吧 」，接着几个名字被提及，DuplexConnection，TCPSocket，SocketConnection ，一个叫mcarter（Michael Carter ）的工程师说他马上要写一篇关于Comet的文章，如果可以确定这个名称，想在文章中引用这个名字。 Socket一直以来都被人用来表示网络中一个连接的两端，考虑到怎么让工程师更容易接受，后来Hixie说了一句「我看WebSocket这个名字就很适合嘛（Hixie briefly pops back online to record that “WebSocket” would probably be a good new name for the TCPConnection object）」，大家都没有异议，紧接着mcarter在Comet Daily中发表了文章Independence Day: HTML5 WebSocket Liberates Comet From Hacks，后来随着各大浏览器对WebSocket的支持，它变成了实际的标准，IETF也沿用了这个名字。 Reference 七种WebSocket框架的性能比较 smallnest/C1000K-Servers - GitHub 使用四种框架分别实现百万websocket常连接的服务器 WebSocket - Wikipedia RFC 6455 STOMP STOMP Over WebSocket STOMP Protocol Specification, Version 1.2 sockjs/sockjs-client WebSocket 和 Socket 的区别 - 文章 - 伯乐在线 Comet Daily » Blog Archive » Independence Day: HTML5 WebSocket Liberates Comet From Hacks The WebSocket API]]></content>
      <tags>
        <tag>WebSocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM运行时数据区域]]></title>
    <url>%2F2018%2F03%2F12%2Fjvm-run-time-data-area%2F</url>
    <content type="text"><![CDATA[根据 The Java&reg; Virtual Machine Specification - Java SE 8 Edition，可以整理出其定义的JVM内存结构如下： JVM Specification定义的运行时数据区域模型↑↑↑ 其中，方法区所属的区域并没有强制要求。不同的JVM实现，其内存分布细节会不同。 1 PCR（程序计数寄存器 The Program Counter Register） The Java Virtual Machine can support many threads of execution at once (JLS §17). Each Java Virtual Machine thread has its own pc (program counter) register. At any point, each Java Virtual Machine thread is executing the code of a single method, namely the $current method$ (§2.6) for that thread. If that method is not native, the pc register contains the address of the Java Virtual Machine instruction currently being executed. If the method currently being executed by the thread is native, the value of the Java Virtual Machine’s pc register is undefined. The Java Virtual Machine’s pc register is wide enough to hold a returnAddress or a native pointer on the specific platform. 2 JVM Stack2.1 Frames 栈帧 A $frame$ is used to store data and partial results, as well as to perform dynamic linking, return values for methods, and dispatch exceptions. A new frame is created each time a method is invoked. A frame is destroyed when its method invocation completes, whether that completion is normal or abrupt (it throws an uncaught exception). Frames are allocated from the Java Virtual Machine stack (§2.5.2) of the thread creating the frame. Each frame has its own array of local variables (§2.6.1), its own operand stack (§2.6.2), and a reference to the run- time constant pool (§2.5.5) of the class of the current method.The sizes of the local variable array and the operand stack are determined at compile-time and are supplied along with the code for the method associated with the frame (§4.7.3). Thus the size of the frame data structure depends only on the implementation of the Java Virtual Machine, and the memory for these structures can be allocated simultaneously on method invocation.Only one frame, the frame for the executing method, is active at any point in a given thread of control. This frame is referred to as the $current frame$, and its method is known as the $current method$. The class in which the current method is defined is the $current class$. Operations on local variables and the operand stack are typically with reference to the current frame.A frame ceases to be current if its method invokes another method or if its method completes. When a method is invoked, a new frame is created and becomes current when control transfers to the new method. On method return, the current frame passes back the result of its method invocation, if any, to the previous frame. The current frame is then discarded as the previous frame becomes the current one.Note that a frame created by a thread is local to that thread and cannot be referenced by any other thread. 2.1.1 Local Variables 本地变量表A single local variable can hold a value of type boolean, byte, char, short, int, float, reference returnAddress. A pair of local variables can hold a value of type long or double. 本地变量表通过索引来处理，第一个本地变量的下标（index）为0. The Java Virtual Machine uses local variables to pass parameters on method invocation. On class method invocation, any parameters are passed in consecutive local variables starting from local variable 0. On instance method invocation, local variable 0 is always used to pass a reference to the object on which the instance method is being invoked (this in the Java programming language). Any parameters are subsequently passed in consecutive local variables starting from local variable 1. Java虚拟机使用局部变量在方法调用上传递参数。 在类方法（静态方法）调用中，任何参数都是从局部变量0开始的连续局部变量中传递的。在实例方法调用中，局部变量0用于将引用（this）传递给调用者。 随后从局部变量1开始，连续的局部变量传递任何参数。 2.1.2 Operand Stacks 操作数栈 The maximum depth of the operand stack of a frame is determined at compile-time and is supplied along with the code for the method associated with the frame 用处： The operand stack is empty when the frame that contains it is created. The Java Virtual Machine supplies instructions to load constants or values from local variables or fields onto the operand stack. Other Java Virtual Machine instructions take operands from the operand stack, operate on them, and push the result back onto the operand stack. The operand stack is also used to prepare parameters to be passed to methods and to receive method results. 2.1.3 Return Value2.1.4 Dynamic Linking Each frame (§2.6) contains a reference to the run-time constant pool (§2.5.5) for the type of the current method to support $dynamic linking$ of the method code. The class file code for a method refers to methods to be invoked and variables to be accessed via symbolic references. Dynamic linking translates these symbolic method references into concrete method references, loading classes as necessary to resolve as-yet-undefined symbols, and translates variable accesses into appropriate offsets in storage structures associated with the run-time location of these variables. JVM内部原理: 每个栈帧都包含了运行时常量池的引用。这个引用指向了这个栈帧正在执行的方法所在的类的常量池，它对动态链接提供了支持。 C/C++ 代码通常编译成一个对象文件，然后多个文件被链接起来生成一个可用的文件比如一个可执行文件或者动态链接库。在链接阶段，符号引用在每个对象文件里被替换成一个和最终执行相关的实际的内存地址。在Java里，这个链接过程在运行时是自动发生的。 当Java文件被编译时，所有的变量和方法引用都作为符号引用保存在class文件的常量池里。一个符号引用是一个逻辑引用并不是一个实际的指向一个物理内存地址的引用。不同的JVM实现能选择什么时候去解决符号引用，它通常发生在class文件加载后的验证，加载完成，立即调用或者静态解析等阶段，另外一种发生的时候是当符号引用第一次被使用，也叫做延迟或者延期解析。无论如何当每个引用第一次使用的时候，JVM必须保证解析发生，并抛出任何解析错误。绑定是一个字段、方法或者类在符号引用被替换为直接引用的过程，这仅仅发生一次，因为符号引用是完全替换的。如果符号引用关联到某个类，而这个类却还没准备好，就会引发类加载。每个直接引用被保存为偏移地址而不是和变量或者方法在运行时的位置相关的存储结构。 3 Heap 堆 The Java Virtual Machine has a heap that is shared among all Java Virtual Machine threads. The heap is the run-time data area from which memory for all class instances and arrays is allocated.…Heap storage for objects is reclaimed by an automatic storage management system (known as a garbage collector); objects are never explicitly deallocated. https://www.zhihu.com/question/49044988/answer/113961406 评论里： 如果“堆”是说Java heap，那么这个也对也错。因为JVM规范对抽象的“Java heap”的定义是“存储Java对象的地方”，也就是说Java对象在哪里，哪里就是Java heap。HotSpot的PermGen里是会存储部分Java对象的，例如说一些java.lang.String实例。这些String实例占的部分就得算是Java heap。 如果“堆”是说GC heap，那么这个错误。PermGen是HotSpot的GC heap的一部分。 一般说“native memory”都是跟GC heap相对的，所以一般取上述后者的定义 4 Method AreaThe Java&reg; Virtual Machine Specification - Java SE 8 Edition: The Java Virtual Machine has a method area that is shared among all Java Virtual Machine threads. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors, including the special methods (§2.9) used in class and instance initialization and interface initialization. Although the method area is logically part of the heap, simple implementations may choose not to either garbage collect or compact it. This specification does not mandate the location of the method area or the policies used to manage compiled code. 尽管方法区域在逻辑上是堆的一部分，但是简单的实现可以选择不垃圾收集或压缩它。 本规范不要求方法区域的位置或用于管理编译代码的策略 4.1 Run-Time Const Pool A run-time constant pool is a per-class or per-interface run-time representation of the constant_pool table in a class file.…The run-time constant pool for a class or interface is constructed when the class or interface is created (§5.3) by the Java Virtual Machine. 根据上面这段引用，可知以下几点： 每个类（或者interface，为便于行文，下面都统称为类）对应一个run-time constant pool 是class文件中constant_pool的运行时表达 在类被JVM加载时创建 那么，要理清其结构，需要先了解class文件的结构。可参见我的另一篇博文：Java class文件格式 其余具体的存储结构，JVM规范并没有作进一步阐述。 4.2 Field具体的存储结构，JVM规范并没有作进一步阐述。 4.3 Method Data具体的存储结构，JVM规范并没有作进一步阐述。 4.4 Code具体的存储结构，JVM规范并没有作进一步阐述。 5 Native Method Stacks Java Virtual Machine implementations that cannot load native methods and that do not themselves rely on conventional stacks need not supply native method stacks. If supplied, native method stacks are typically allocated per thread when each thread is created. Reference JVM内部原理 | 并发编程网 – ifeve.com The Java&reg; Virtual Machine Specification - Java SE 8 Edition JEP 122: Remove the Permanent Generation 方法区的Class信息,又称为永久代,是否属于Java堆？ - 知乎]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM加载、启动和初始化]]></title>
    <url>%2F2018%2F03%2F10%2Fjvm-loading-linking-initializing%2F</url>
    <content type="text"><![CDATA[0 概述这实际上是《The Java&reg; Virtual Machine Specification - Java SE 8 Edition》中第五章内容（Loading, Linking, and Initializing）的部分翻译。主要目的是整理阅读笔记，让我自己看得明白，在这个前提下，尽量让别人看得明白，如果读者觉得我写得很混乱，还请自行阅读原文，不便之处敬请见谅。如有错误，还请指正（拉到页面底部点击『联系我』就可以发邮件给我）。 阅读本文内容需要先对java的class文件结构有所了解。如果尚不了解，不妨参考我的另一篇博文 Java class文件格式 加载是这样一个过程：寻找一个特定名称的class或者interface的二进制表达形式（binary representation），然后从这个二进制表达形式中创建出一个class或者interface。 链接是这样一个过程：取得class或者interface，然后将其结合到JVM的运行时状态，使得它可以被执行。 初始化一个class或者interface的过程就是执行这个class或者interface的初始化方法&lt;clinit&gt;的过程。 1 运行时常量池（Run-Time Constant Pool）在class或interface被创建的时候，用二进制形式的class文件中的constant_pool表来构造运行时常量池。运行时常量池中的所有引用最初都是符号引用。运行时常量池中的符号引用来自二进制表达形式中的以下结构： Symbolic Reference to Structure in class file class CONSTANT_Class_info field CONSTANT_Fieldref_info class method CONSTANT_Methodref_info interface method CONSTANT_InterfaceMethodref_info method handle CONSTANT_MethodHandle_info method type CONSTANT_MethodType_info call site specifier CONSTANT_InvokeDynamic_info 另外，某些不是符号引用的运行时值则来自constant_pool表中的以下结构： 字符串字面量 CONSTANT_String_info 运行时常量值 CONSTANT_Integer_info CONSTANT_Float_info CONSTANT_Long_info CONSTANT_Double_info constant_pool里剩下的结构则只会被间接使用： CONSTANT_NameAndType_info CONSTANT_Utf8_info 2 JVM启动JVM通过使用引导类加载器（bootstrap class loader）创建初始类来启动，初始类的指定方式与实现相关。 然后，JVM链接初始类，初始化它，并调用public static void main(String[])方法。此方法的调用将驱动所有进一步的执行。执行构成 main 方法的JVM指令可能会导致附加类和接口的链接（链接之后进而创建），以及调用其他方法。 在JVM的实现中，初始类可以作为命令行参数提供。或者， JVM的实现可以提供一个初始类来设置类加载程序，然后加载一个应用程序。可以选择其他初始类，只要它们与前一段中给出的规范一致即可。 3 创建和加载（Creation and Loading） 符号约定class或者interface $C$ 用其类名 $N$ 来表示，其创建过程由另一个class or interface $D$ 触发，$D$通过运行时常量池引用$C$。class or interface的创建也可以通过$D$调用Java SE平台的特定class libraries中的方法来触发，例如反射。 如果$C$不是数组类，则通过使用class loader加载$C$的二进制表达来创建。 数组类没有外部的二进制表达，由JVM来创建，而不是class loader。 类加载器 $L$可以通过以下两种方式来创建$C$： 直接定义$C$ 委托给另一个class loader 如果$L$委托了另一个class loader来加载$C$，则说$L$启动了$C$的加载（$L$ initiates loading of $C$），或者等价地说，$L$是$C$的initiating loader。$N^L$ 表示由$L$ initiates loading的$C$。 如果$L$直接创建$C$，则我们说$L$定义了$C$（$L$ defines $C$），或者等价地说，$L$是$C$的 defining loader。&lt;$N, L$&gt; 表示由$L$ defines的$C$。同时，$L$也是$C$的initiating loader。 3.1 用Bootstrap Class Loader加载下面的步骤用于加载、从而创建由$N$表示的、非数组类型的类或者接口$C$。 首先，JVM判断bootstrap class loader是否已经被标记为由$N$表示的类或接口的initiating loader。如果是，这个类或者接口就是$C$，后续没有创建过程。 否则，JVM将$N$作为参数来调用bootstrap class loader的方法，用平台依赖的方式来搜索给出的$C$的表达形式。 通常，类或接口会使用分层文件系统中的文件来表达，并且其名称会编码在文件的路径名（pathname）中。 这个阶段需要检查以下错误： 如果找不到给出的$C$的表达形式，抛出ClassNotFoundException 然后，JVM尝试使用加载算法，从给出的表达形式中取得$N$表示的类，将结果作为$C$。 3.2 用User-defined Class Loader加载下面的步骤用于使用用户定义的类加载器（user-defined class loader） $L$加载、从而创建由$N$表示的、非数组类型的类或者接口$C$。 首先，JVM判断$L$是否已经被标记为由$N$表示的类或接口的initiating loader。如果是，这个类或者接口就是$C$，后续没有创建过程。 否则，JVM调用$L$的loadClass(N)方法，这个调用的返回值就是创建后的类或接口$C$。 然后JVM将$L$记录为$C$的initiating loader。 当使用$N$来调用$L$的loadClass方法时，$L$必须执行以下两个操作之一来加载$C$： $L$可以创建一个byte数组，这个数组表达了$C$的ClassFile结构；然后它必须调用ClassLoader类的defineClass方法，执行这个方法使得JVM用$L$使用加载算法从byte数组中取得$C$。 $L$可以将加载过程委托给另一个类加载器$L’$，将参数$N$直接或者间接地传给$L’$的方法调用（一般是loadClass方法）。方法调用的结果是$C$。 在上述两个步骤中，无论处于任何原因无法加载$N$所表示的类或接口时，都必须抛出ClassNotFoundException。 3.3 创建数组类（Array Classes）下面的步骤用于使用类加载器$L$来加载、从而创建由$N$表示的、数组类型的类$C$。 如果$L$已被记录为与$N$相同的组件类型（component type）的数组类的initiating loader，则该类为$C$，并且不需要再进行任何数组类创建。 否则，执行下面的步骤来创建$C$： 如果组件类型是引用类型（reference type），则使用$L$递归地应用本节的算法，以加载、从而创建$C$的组件类型； JVM使用指定的组件类型和维数创建一个新的数组类。 如果组件类型是一个引用类型，$C$会被标记为 『已被组件类型的defining class loader定义了』（having been defined by the defining class loader of the component type）。否则，$C$会被标记为『已被bootstrap class loader定义了』（having been defined by the bootstrap class loader）。 无论如何，JVM都会将$L$记录为$C$的initiating loader。 如果组件类型是一个引用类型，那么这个数组类的可见性（accessibility）和组件类型一致，否则可见性为public 3.4 加载的约束在class loader出现的时候，确保类型安全链接要特别小心。有可能存在这样一种情况：两个不同的class loader触发了由$N$表示的class or interface的加载，而$N$在每个class loader里可能表示了不同的class or interface。 （TBD） 3.5 从class文件表达形式中获得Class将一个非数组类型的class或者interface $C$ 记为 $N$，下面是用loader $L$从class文件格式中加载$C$为Class对象的步骤： JVM判断$L$是否已经被标记为$N$的initiating loader，如果是，创建过程将不可用，并且抛出LinkageError 否则，JVM尝试解析给出的表达。但是，给出的表达不一定是$C$的一个有效的表达。这个加载阶段必须检查以下错误： 如果给出的表达形式不是ClassFile结构，抛出ClassFormatError 否则，如果给出的表达不在支持的版本范围内（major version和minor version），抛出UnsupportedClassVersionError 否则，如果给出的表达不是类名$N$的实际表达，抛出NoClassDefFoundError或者其子类 如果$C$有直接父类，就用Class and Interface Resolution算法来解析出$C$到其直接父类的符号链接。 这个阶段要检查以下错误： 如果其直接父类实际上是一个interface，抛出IncompatibleClassChangeError 否则，如果$C$任意父类是$C$本身，抛出ClassCircularityError 如果$C$有任何直接父接口，则使用 Class and Interface Resolution 算法来解析出$C$到其直接父接口的符号链接。 这个阶段要检查以下错误： 如果其直接父接口实际上不是一个interface，抛出IncompatibleClassChangeError 如果$C$的任何一个父接口是$C$本身，抛出ClassCircularityError JVM将$C$标记为拥有$L$作为其defining class loader，并且标记$L$是$C$的initiating loader 4 链接（Linking）链接一个class或者interface包括验证和准备阶段，涉及到的对象有： 该class或interface本身 其直接父类 其直接父接口 如果这是数组类型，还涉及其元素类型 解析class或interface中的符号链接是链接阶段可选的一部分。 该规范允许实现的灵活性，以便在链接活动（以及由于递归、加载）发生时，只要保持以下所有属性： 一个class或interface在链接前要被完全加载 一个class或interface在初始化前要被完全验证 程序执行的一些操作可能直接或间接地要求链接涉及错误的class或interface，当这些错误被检测到时，必须在发生这些操作的地方抛出这些错误。 例如，一个JVM实现可能选择在使用到一个class或interface的时候才去解析其中的每个符号链接（懒加载或者延迟加载，”lazy” or “late” resolution），或者在验证class的时候一次性解析其中所有的符号链接（”eager” or “static” resolution）。这意味着在某些实现中，在class或interface初始化之后还有可能继续执行解析过程。无论采用哪种策略，在解析期间检测到的任何错误都必须在程序中的使用对class或interface的符号引用的地方抛出，不论是直接还是间接地使用到。 由于链接阶段涉及到新数据结构的分配（allocation），有可能因OutOfMEmoryError而失败。 4.1 验证（Verification）验证阶段确保了class或interface的二进制表达在结构上是正确的。验证阶段有可能引起其他类或接口被加载，但是不需要导致它们被验证或者准备。 如果class或interface的二进制表达不满足The class File Format - Constraints on Java Virtual Machine Code中列出的静态约束或结构型约束，那么在程序中导致class或interface被校验的地方必须要抛出VerifyError。 如果因为抛出了LinkageError（或子类）实例错误导致JVM尝试验证class或interface失败，则随后尝试验证class或interface始终会失败，并抛出相同的错误 作为初步验证尝试的结果。 4.2 准备（Preparation）准备阶段包括创建class or interface的static fields，并初始化默认值。这个过程不需要执行任何JVM代码。静态字段的显式初始值设定是作为初始化的一部分执行，而不是准备阶段。 在class or interface $C$ 的准备阶段，JVM有以下约束： 令$L_1$为$C$的defining loader，$m$为$C$中覆盖自父类或者父接口&lt;$D, L_2$&gt;的方法，对于每个$m$，令其返回值为$T_r$，形参为$T_{f_1},…,T_{f_n}$，那么： 如果$T_r$不是数组类型，令$T_0$为$T_r$；否则令$T_0$为$T_r$的元素类型； 对于$i=1, …, n$，如果$T_{f_i}$不是数组类型，令$T_i$为$T_{f_i}$；否则令$T_i$为$T_{f_i}$的元素类型 则有：$$ {T_i}^{L_1} = {T_i}^{L_2}, i=0, …, n $$ 更进一步的情况，如果$C$实现了父接口&lt;$I, L_3$&gt;中的方法$m$，但是$C$没有声明方法$m$，但是$C$的父类&lt;$D, L_2$&gt;声明了方法$m$的实现，则有以下约束： $m$的返回类型记为$T_r$，$m$的形参类型记为$T_{f1}, …, T_{fn}$，则：如果$T_r$不是数组类型，令$T_0$为$T_r$，否则令$T_0$为$T_r$的元素类型（element type）。对于所有$i=0, …, n$：如果$T_{fi}$不是数组类型，则$T_i$为$T_{fi}$，否则$T_i$为$T_{fi}$的元素类型。那么有$$ {T_i}^{L_2}={T_i}^{L_3}, i=0, …, n $$ 4.3 解析（Resolution）以下JVM指令对运行时常量池做了符号引用，执行任何这些指令都需要解析其符号引用：anewarray, checkcast, getfield, getstatic, instanceof, invokedynamic, invokeinterface, invokespecial, invokestatic, invokevirtual, ldc, ldc_w, multianewarray, new, putfield, and putstatic。 解析是从运行时常量池中的符号引用中动态确定具体值的过程。 解析某次出现的invokedynamic指令中的符号引用并不意味着该符号引用对于其它任何invokedynamic指令来说都被解析了。 对于其他指令来说，解析了某次出现的指令中的符号引用，确实意味着该符号应用对于其他任意的非invokedynamic指令来说都视为被解析了。 上文的意思是，由一个特定的invokedynamic指令确定的具体值是一个绑定到该特定invokedynamic指令的call site object。 （TBD） 下面部分阐述对一个class或interface $D$所引用的、尚在运行时常量池中的符号引用的解析过程。符号引用的类型不同，解析的细节也不同。 4.3.1 Class and Interface Resolution 类和接口解析 执行以下步骤来将$D$所引用的、未解析的符号引用解析为由$N$表示的class或interface $C$： 用$D$的defining class loader来创建由$N$表示的class或interface，细节在第三节（Creation and Loading）给出了。 在创建过程中抛出的任何作为失败结果的exception都可以作为解析过程的失败结果抛出。 如果$C$是数组类并且其元素类型是一个引用类型，则递归地调用上一小节（Class and Interface Resolution）中的算法来解析其元素类型的符号引用。 最后，检查$C$的访问授权： 如果$C$不能被$D$访问，抛出IllegalAccessError 这种情况举例：如果$C$这个类本来是被声明为public的，但是在$D$编译完之后被改为了非public了。 如果第1、2步成功执行但是第3步失败了，$C$仍然是有效和可用的。尽管如此，这个解析过程也是失败了的，并且$D$也禁止访问$C$。 4.3.2 Field Resolution 字段解析为了将$D$中未解析的符号引用解析为一个class或interface $C$中的一个字段（field），由字段引用（field reference）给出的到$C$的符号引用必须首先被解析（4.3.1）。 在解析字段引用的时候，字段解析（field resolution）首先尝试查找在$C$及其父类中引用的字段： 如果$C$使用了由字段引用指定的名称和描述符来声明一个字段，则字段查找（field lookup）成功。所声明的字段就是查找结果。 否则，对$C$的直接父接口递归地进行字段查找。 否则，如果$C$具有父类$S$，对$S$递归地进行字段查找。 否则，字段查找失败。 然后： 如果字段查找失败了，字段解析抛出NoSuchFieldError 否则，如果字段查找成功了，但是$D$不能访问该引用字段，抛出IllegalAccessError 否则，令实际声明该引用字段的class或interface为&lt;$E, L_1$&gt;，令$L_2$为$D$的defining loading 假设引用字段的类型为$T_f$，如果$T_f$不是数组类型，则$T$为$T_f$，如果$T_f$为数组类型，则$T$为$T_f$的元素类型。 JVM必须保证$T^{L_1}=T^{L_2}$的约束。 4.3.3 Method Resolution 方法解析为了将$D$中的符号引用解析为class $C$中的方法，由该方法引用给出的到$C$的符号引用要首先被解析（4.3.1）。 当解析一个方法引用时： 如果$C$是一个interface，抛出IncompatibleClassChangeError 否则，方法解析（method resolution）尝试在$C$及其父类中定位该引用方法： 如果$C$刚好声明了一个由该方法引用指定的名字的方法，并且声明的方法是一个signature polymorphic method，那么方法查找成功。描述符中声明的所有类名都被解析了。 The resolved method is the signature polymorphic method declaration. It is not necessary for C to declare a method with the descriptor specified by the method reference. signature polymorphic method这个概念在《The Java&reg; Virtual Machine Specification - Java SE 8 Edition》的 2.9 Special Methods 中定义： A method is signature polymorphic if all of the following are true: • It is declared in the java.lang.invoke.MethodHandle class. • It has a single formal parameter of type Object[]. • It has a return type of Object. • It has the ACC_VARARGS and ACC_NATIVE flags set. 否则，如果$C$用该方法引用指定的名字和描述符声明了一个方法，方法查找成功。 否则，如果$C$有父类，对$C$的直接父类递归地调用步骤2。 否则，方法解析尝试在$C$的父接口中定位引用方法： If the maximally-specific superinterface methods of C for the name and descriptor specified by the method reference include exactly one method that does not have its ACC_ABSTRACT flag set, then this method is chosen and method lookup succeeds. Otherwise, if any superinterface of C declares a method with the name and descriptor specified by the method reference that has neither its ACC_PRIVATE flag nor its ACC_STATIC flag set, one of these is arbitrarily chosen and method lookup succeeds. Otherwise, method lookup fails. A maximally-specific superinterface method of a class or interface $C$ for a particular method name and descriptor is any method for which all of the following are true: The method is declared in a superinterface (direct or indirect) of $C$. The method is declared with the specified name and descriptor. The method has neither its ACC_PRIVATE flag nor its ACC_STATIC flag set. Where the method is declared in interface I, there exists no other maximally- specific superinterface method of $C$ with the specified name and descriptor that is declared in a subinterface of $I$. The result of method resolution is determined by whether method lookup succeeds or fails: If method lookup fails, method resolution throws a NoSuchMethodError. Otherwise, if method lookup succeeds and the referenced method is notaccessible (§5.4.4) to $D$, method resolution throws an IllegalAccessError. Otherwise,let&lt;$E,L_1$&gt;be the class or interface in which the referenced method $m$ is actually declared, and let $L_2$ be the defining loader of $D$. Given that the return type of $m$ is $T_r$, and that the formal parameter types of $m$are $T_{f_1}, …, T_{f_n}$, then: If $T_r$ is not an array type, let $T_0$ be $T_r$; otherwise, let $T_0$ be the element type (§2.4) of $T_r$. For $i = 1, …, n$: If $T_{f_i}$ is not an array type, let $T_i$ be $T_{f_i}$; otherwise, let $T_i$ be the element type (§2.4) of $T_{f_i}$. The Java Virtual Machine must impose the loading constraints $T^{L_1} = T^{L_2}$ for $i = 0, …, n$ (§5.3.4). When resolution searches for a method in the class’s superinterfaces, the best outcome is to identify a maximally-specific non-abstract method. It is possible that this method will be chosen by method selection, so it is desirable to add class loader constraints for it. Otherwise, the result is nondeterministic. This is not new: The Java® Virtual Machine Specification has never identified exactly which method is chosen, and how “ties” should be broken. Prior to Java SE 8, this was mostly an unobservable distinction. However, beginning with Java SE 8, the set of interface methods is more heterogenous, so care must be taken to avoid problems with nondeterministic behavior. Thus: Superinterface methods that are private and static are ignored by resolution. This is consistent with the Java programming language, where such interface methods are not inherited. Any behavior controlled by the resolved method should not depend on whether the method is abstract or not. Note that if the result of resolution is an abstract method, the referenced class $C$ may be non-abstract. Requiring $C$ to be abstract would conflict with the nondeterministic choice of superinterface methods. Instead, resolution assumes that the run time class of the invoked object has a concrete implementation of the method. 4.3.4 Interface Method Resolution（TBD） 4.3.5 Method Type and Method Handle Resolution（TBD） 4.3.6 Call Site Specifier Resolution（TBD） 4.4 访问控制（Access Control）当且仅当以下都为真时，class或interface $C$对class或interface $D$来说是可访问的： $C$为public； $C$和$D$是同一个运行时包（run-time package）的成员。 当且仅当以下都为真时，一个字段或方法$R$对class或interface $D$来说是可访问的： $R$为public； $R$为protected并且在类$C$中被声明，同时$D$是$C$的子类或者是$C$本身。更进一步，如果$R$不是static的，指向$R$的符号引用必须包含指向类$T$的符号引用，这个$T$是$D$的子类或者是$D$的父类或者是$D$本身； $R$是protected的，或者具有默认的访问级别（即没有显式声明访问修饰符，非public、非protected、非private），并且和$D$是同一个运行时包（run-time package）的成员； $R$是private的并且在$D$里声明。 上述访问控制的讨论省略了调用protected方法或者访问protected字段的目标的相关限制（目标必须是$D$或者是$D$的子类型）。这种约束是验证阶段的一部分，不是链接时的访问控制。 4.5 覆盖（Overriding）有$C$类中声明的实例方法$m_C$和$A$类中声明的另一个实例方法$m_A$，当$m_C$和$m_A$一样或者下列条件都为真时，我们说$m_C$覆盖了$m_A$： $C$是$A$的子类 $m_C$和$m_A$具有同样的名称和描述符 $M_C$没有标记为ACC_PRIVATE 以下其中一个为真： $m_A$被标记为ACC_PUBLIC；或者被标记为ACC_PROTECTED；或者都没有标记为ACC_PUBLIC、ACC_PROTECTED、ACC_PRIVATE并且$A$和$C$属于同一个运行时包。 $m_C$覆盖了方法$m’$（$m’$与$m_C$和$m_A$都不同），而$m’$覆盖了$m_A$ 5 初始化（Initialization）（TBD） 6 绑定本地方法实现（Binding Native Method Implementations）绑定是这样一个过程：一种用Java以外的语言编写的、实现本地方法的函数被集成到JVM中以便执行。 虽然这个过程通常被称为链接，但术语『绑定』在这个规范中用来避免与JVM的类或接口链接混淆。 7 JVM Exit某个线程调用了Runtime或System类的exit方法或者Runtime类的halt方法、并且exit或halt操作得到了安全管理器（security manager）的准许时，JVM退出。 另外，JNI（Java Native Interface）规范描述了当JNI Invocation API被用于加载和卸载JVM时JVM的终止。 Reference The Java&reg; Virtual Machine Specification - Java SE 8 Edition]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java class文件格式]]></title>
    <url>%2F2018%2F03%2F10%2Fjava-class-file-format%2F</url>
    <content type="text"><![CDATA[0 概述这实际上是《The Java&reg; Virtual Machine Specification - Java SE 8 Edition》中第四章内容（The class File Format）的部分翻译。主要目的是整理阅读笔记，让我自己看得明白，在这个前提下，尽量让别人看得明白，如果读者觉得我写得很混乱，还请自行阅读原文，不便之处敬请见谅。如有错误，还请指正（拉到页面底部点击『联系我』就可以发邮件给我）。 每一个class文件都包含了一个单独的class或者interface的定义。尽管一个class或者interface并不是有一个以文件形式存在的外部表达，但是下面还是通俗地将class或interface的任何有效表达称为类文件格式（the class file format）。 一个类文件由一个8位字节流组成。 所有的16位，32位和64位量分别通过读取2、4、8个连续的8位字节来构造。 多字节数据项总是以big-endian顺序存储，其中高字节排在第一位。 在Java SE平台中，此格式由接口java.io.DataInput和java.io.DataOutput以及类如java.io.DataInputStream和java.io.DataOutputStream支持。 本章定义了自己的一组表示类文件数据的数据类型： 类型u1，u2和u4分别表示一个无符号的一个，两个或四个字节数量（即1 byte、2 byte和4 byte）。 在Java SE平台中，这些类型可以通过接口java.io.DataInput的readUnsignedByte，readUnsignedShort和readInt等方法读取。 1 ClassFile结构一个class文件包含一个单独的ClassFile结构：123456789101112131415161718ClassFile &#123; u4 magic; u2 minor_version; u2 major_version; u2 constant_pool_count; cp_info constant_pool[constant_pool_count-1]; u2 access_flags; u2 this_class; u2 super_class; u2 interfaces_count; u2 interfaces[interfaces_count]; u2 fields_count; field_info fields[fields_count]; u2 methods_count; method_info methods[methods_count]; u2 attributes_count; attribute_info attributes[attributes_count];&#125; ClassFile结构中的各个item含义如下： 1.1 magic魔法数字，用于标识class文件格式，固定为0xCAFEBABE 1.2 minor_version, major_versionminor_version和major_version组合到一起，决定了class文件格式的版本。 假设class文件的major_version为$M$，minor_version为$m$，那么我们将class文件格式的版本记为$M.m$，因此，这个版本号可以按字典序排序，例如$1.5&lt;2.0&lt;2.1$。 一个JVM的实现可以支持的class文件格式版本记为$v$，当且仅当$v$落在连续区间 $M_i.0 \leq v \leq M_j.m$ 时成立。 JVM实现所遵循的Java SE平台的release level决定了其支持范围。 例如 Oracle对JDK release 1.0.2的JVM实现支持的class文件格式版本为$[45.0, 45.3]$ JDK release 1.1.* 支持$[45.0, 45.65535]$ 对于$k \geq 2$，JDK release $1.k$ 支持范围为$[45.0, 44+k.0]$ 1.3 constant_pool_countconstant_pool[]中的条目数量+1 1.4 constant_pool[]constant_pool是一个结构表，表示ClassFile结构及其子结构中引用的各种 字符串常量 类和接口名 字段名 其他常量。 每个constant_pool表项的格式由第一个“标记(tag)”字节表示。constant_pool表下标的取值范围是$[1,\ constant\_pool\_count)$ 1.5 access_flagsaccess_flags 项的值是用于表示对此类或接口的访问权限和属性的标志的掩码。 16bit Flag Name Value 二进制 十进制表示 Interpretation ACC_PUBLIC 0x0001 00000000 00000001 1 声明public ACC_FINAL 0x0010 00000000 00010000 16 声明final ACC_SUPER 0x0020 00000000 00100000 32 当调用invokespecial指令时，特殊对待父类方法 ACC_INTERFACE 0x0200 00000010 00000000 512 指明这是interface而非class ACC_ABSTRACT 0x0400 00000100 00000000 1024 声明abstract ACC_SYNTHETIC 0x1000 00010000 00000000 4096 声明synthetic，在源码中不存在 ACC_ANNOTATION 0x2000 00100000 00000000 8192 指明这是注解 ACC_ENUM 0x4000 01000000 00000000 16384 指明这是enum 1.6 this_classthis_class的值必须是constant_pool[]的有效下标。constant_pool[this_class]必须为CONSTANT_Class_info结构，表示了在此class文件中定义的类或者接口。 1234CONSTANT_Class_info &#123; u1 tag; u2 name_index;&#125; 1.7 super_classsuper_class的值也必须是constant_pool[]的有效下标。 对于类来说， 如果此值不是0， 则constant_pool[super_class]必须为CONSTANT_Class_info结构，表示了此class文件定义的类的直接父类。 如果此值为0，则此class文件必须表示Object类，这是唯一没有直接父类的类或者接口。 对于接口来说，没有规定。 1.8 interfaces_count指明了此类的直接父接口的数量。 1.9 interfaces[]里面的每个值都必须是constant_pool[]的有效下标。所指向的constant_pool中的条目必须为CONSTANT_Class_info结构，指明了其直接父接口。 1.10 fileds_count给出fields[]中field_info结构的数量。field_info结构表示了此类或接口所声明的所有field，包括class variables和instance virables（也就是静态变量和实例变量）。 1.11 fields[]所有值都必须为field_info结构，给出对变量的完整描述。只包含由此类或者接口声明的变量，不包括继承而来的。 1234567field_info &#123; u2 access_flags; u2 name_index; u2 descriptor_index; u2 attributes_count; attribute_info attributes[attributes_count];&#125; 1.12 methods_count给出methods[]中method_info结构的数量 1.13 methods[]所有值都必须为method_info结构。 如果ACC_NATIVE和ACC_ABSTRACT标志都没有在method_info里的access_flags中设置，实现此方法的JVM指令也要提供。 methods[]表达了此class or intreface声明的所有方法，包括实例方法、类方法、实例初始化方法和class or interface 初始化方法，不包括继承而来的方法。 1234567method_info &#123; u2 access_flags; u2 name_index; u2 descriptor_index; u2 attributes_count; attribute_info attributes[attributes_count];&#125; 1.14 attributes_countsize of attributes[] 1.15 attributes[]12345attribute_info &#123; u2 attribute_name_index; u4 attribute_length; u1 info[attribute_length];&#125; 2 The Internal Form of Names 2.1 Binary Class and Interface Namesclass文件结构中，class和interface的名字总是以一种全限定（fully qualified）的形式表达的，被称为binary names（JLS §13.1）。这些名字总是表达为CONSTANT_Utf8_info结构。 类和接口名称是从那些CONSTANT_NameAndType_info结构中引用的, 它们的名称是其描述符的一部分, 并且来自所有CONSTANT_Class_info结构 由于历史原因, 类文件结构中出现的binary names的语法不同于JLS §13.1中记录的二进制名称的语法。标识符（identifiers）构成了binary names，通常用ASCII码（.）分割各个标识符，在这种内部形式中，替换为了ASCII码（/）。标识符本身必须是未限定的名称（unqualified names）。 2.2 Unqualified Names 未限定的名称方法、field、本地变量和形式参数的名称都存储为unqualified names。 一个unqualified names必须包含至少一个Unicode码（Unicode code point），并且不能包含以下ASCII字符：., ;, [, /（即，句号、分号、左方括号和斜线）。 方法名的约束还要加上不能出现ASCII字符&lt;、&gt;（即左尖括号和右尖括号），例外方法为 &lt;init&gt; &lt;clinit&gt; 3 Descriptors 描述符描述符是用于表示field或者方法的字符串。 3.1 Grammer Notaion 语法符号（先定义用于描述描述符构成的语法）：描述符是使用语法指定的。语法是一组生产、 描述字符序列如何形成各种语法正确的描述符。 语法的终端符号以固定宽度字体显示。 非终结名称符号以斜体类型显示。 非终结名称的定义由定义的非终结名称的名称引入，后跟冒号。 非终结名称的一个或多个可选定义随后将跟随后续行。 生产右侧的语法 {x} 表示 x 的零个或多个匹配项。 生产右侧的短语 (one of) 表示以下行或行上的每个终端符号都是可选的定义。 3.2 Field Descriptors field descriptors的说明： FieldType term Type Interpretation B byte signed byte C char Unicode character code point in the Basic Multilingual Plane, encoded with UTF-16 D double double-precision floating-point value F float single-precision floating-point value I int integer J long long integer L ClassName ; reference an instance of class ClassName S short signed short Z boolean true or false [ reference one array dimension 例子： 一个Object实例表示为：Ljava/lang/Object; 一个多维数组double[][][]表示为[[[D 3.3 Method Descriptors 方法描述符 例子： 1 1Object m(int i, double d, Thread t) &#123;...&#125; 描述符为：(IDLjava/lang/Thread;)Ljava/lang/Object; 4 The Constant PoolJava 虚拟机指令不依赖于类、接口、类实例或数组的运行时布局。相反, 指令指的是 constant_pool 表中的符号信息。 constant_pool中的所有条目都具有以下的一般结构： 1234cp_info &#123; u1 tag; u1 info[]; &#125; 其中，tag用于指示cp_info条目的类型，info数组的内容随tag的值而变化。每个标记字节必须后跟两个或多个字节, 以提供有关特定常量的信息。tag的取值范围如下： Constant Type Value CONSTANT_Class 7 CONSTANT_Fieldref 9 CONSTANT_Methodref 10 CONSTANT_InterfaceMethodref 11 CONSTANT_String 8 CONSTANT_Integer 3 CONSTANT_Float 4 CONSTANT_Long 5 CONSTANT_Double 6 CONSTANT_NameAndType 12 CONSTANT_Utf8 1 CONSTANT_MethodHandle 15 CONSTANT_MethodType 16 CONSTANT_InvokeDynamic 18 4.1 CONSTANT_Class_info1234CONSTANT_Class_info &#123; u1 tag; u2 name_index;&#125; tag的值为7，表示这是一个CONSTANT_Class，name_index必须是constant_pool的有效下标，指向的必须是一个CONSTANT_Utf8_info结构。也就是说指向了常量池中表示类名的字符串常量。类名表达为internal form，例如： int[][]表达为[[I Thread[]表达为[Ljava/lang/Thread; 4.2 CONSTANT_Fieldref_info, CONSTANT_Methodref_info和CONSTANT_InterfaceMethodref_info123456789101112131415CONSTANT_Fieldref_info &#123; u1 tag; u2 class_index; u2 name_and_type_index;&#125;CONSTANT_Methodref_info &#123; u1 tag; u2 class_index; u2 name_and_type_index;&#125;CONSTANT_InterfaceMethodref_info &#123; u1 tag; u2 class_index; u2 name_and_type_index;&#125; 4.3 CONSTANT_String_info1234CONSTANT_String_info &#123; u1 tag; u2 string_index;&#125; string_index必须是constant_pool的有效下标，必须指向CONSTANT_Utf8_info结构。 4.4 CONSTANT_Integer_info和CONSTANT_Float_info结构12345678CONSTANT_Integer_info &#123; u1 tag; u4 bytes; &#125;CONSTANT_Float_info &#123; u1 tag; u4 bytes; &#125; bytes表达实际值 4.5 CONSTANT_Long_info和CONSTANT_Double_info结构12345678910CONSTANT_Long_info &#123; u1 tag; u4 high_bytes; u4 low_bytes;&#125;CONSTANT_Double_info &#123; u1 tag; u4 high_bytes; u4 low_bytes;&#125; 4.6 CONSTANT_NameAndType_info12345CONSTANT_NameAndType_info &#123; u1 tag; u2 name_index; u2 descriptor_index;&#125; 4.7 CONSTANT_Utf8_info12345CONSTANT_Utf8_info &#123; u1 tag; u2 length; u1 bytes[length];&#125; 4.8 CONSTANT_MethodHandle_info 12345CONSTANT_MethodHandle_info &#123; u1 tag; u1 reference_kind; u2 reference_index;&#125; reference_kind The value of the reference_kind item must be in the range 1 to 9. The value denotes the kind of this method handle, which characterizes its bytecode behavior (§5.4.3.5)(Chapter 5: Loading, Linking, and Initializing). reference_index 必须是constant_pool的有效下标，指向的结构根据reference_kind的不同而有不同的要求： 如果reference_kind为1、2、3、4，必须指向CONSTANT_Fieldref_info结构 如果reference_kind为5、8，必须指向CONSTANT_Methodref_info，而此结构表达了类的方法或构造器 如果reference_kind为6、7，然后 如果class文件的版本号小于52.0，必须指向CONSTANT_Methodref_info结构，此结构表示类的方法； 如果class文件的版本号大于或等于52.0，则必须指向CONSTANT_Methodref_info结构或者CONSTANT_InterfaceMethodref_info结构，表达了类或接口的方法； 如果reference_kind为9，必须指向CONSTANT_InterfaceMethodref_info结构，表达了接口方法。 4.9 CONSTANT_MethodType_info1234CONSTANT_MethodType_info &#123; u1 tag; u2 descriptor_index;&#125; 4.10 CONSTANT_InvokeDynamic_info12345CONSTANT_InvokeDynamic_info &#123; u1 tag; u2 bootstrap_method_attr_index; u2 name_and_type_index;&#125; bootstrap_method_attr_index The value of the bootstrap_method_attr_index item must be a valid index into the bootstrap_methods array of the bootstrap method table (§4.7.23) of this class file. 5 Fields1234567field_info &#123; u2 access_flags; u2 name_index; u2 descriptor_index; u2 attributes_count; attribute_info attributes[attributes_count];&#125; attributes[]里面的每个item都必须是attribute_info结构 6 Methods1234567method_info &#123; u2 access_flags; u2 name_index; u2 descriptor_index; u2 attributes_count; attribute_info attributes[attributes_count];&#125; attributes[]里面的每个item都必须是attribute_info结构 7 Attributes12345attribute_info &#123; u2 attribute_name_index; u4 attribute_length; u1 info[attribute_length];&#125; The Code Attribute12345678910111213141516Code_attribute &#123; u2 attribute_name_index; u4 attribute_length; u2 max_stack; u2 max_locals; u4 code_length; u1 code[code_length]; u2 exception_table_length; &#123; u2 start_pc; u2 end_pc; u2 handler_pc; u2 catch_type; &#125; exception_table[exception_table_length]; u2 attributes_count; attribute_info attributes[attributes_count];&#125; max_stack给出在方法执行的任意地方操作数栈的最大深度 max_locals给出方法调用时本地方法表的最大容量 code[]给出了实现方法的JVM代码的实际字节（byte）。 The BootstrapMethods Attribute 123456789BootstrapMethods_attribute &#123; u2 attribute_name_index; u4 attribute_length; u2 num_bootstrap_methods; &#123; u2 bootstrap_method_ref; u2 num_bootstrap_arguments; u2 bootstrap_arguments[num_bootstrap_arguments]; &#125; bootstrap_methods[num_bootstrap_methods];&#125; bootstrap_methods[] bootstrap_method_ref其值必须是constant_pool中的有效下标，指向的必须是CONSTANT_MethodHandle_info结构 8 Format Checking（TBD） 9 Constraints on Java Virtual Machine Code （TBD） 10 Verification of class Files （TBD） 10.1 Verification by Type Checking10.2 Verification by Type Inference11 JVM的限制 ClassFile结构中16-bit的constant_pool_count限制了per-class或者per-interface的常量池最多只有65535个条目（entries）。这对单个类或接口的总体复杂性起到了内部限制作用。 ClassFile结构中的fields_count限制了一个class or interface能声明的field数量不能超过65535。（不包括继承的） 方法数量限制同上。（methods_count） 直接父接口的数量限制同上（interfaces_count） 方法调用时创建的帧里面，本地变量表中的本地变量数量最多为65535，由Code attribute中的max_locals item所限制，以及由JVM指令集的16-bit本地变量索引所限制。其中，long和double类型视为两个本地变量 Code attribute中的max_stack item限制了frame中的操作数栈的大小为65535其中，long和double类型的操作数视为两个单元 method descriptor的定义限制了方法参数的数量最多为255其中，实例方法的this占了一个单元，long和double类型的会占两个单元 field和方法的名称、field和方法的descriptor以及其他string常量值（包括被ConstantValue attribute引用的）最多为65535个byte，由CONSTANT_Utf8_info结构中的16-bit无符号length item所限制 数组的维度最多为255，由multianewarray指令中的opcode dimensions的大小所限制 The number of dimensions in an array is limited to 255 by the size of the dimensions opcode of the multianewarray instruction and by the constraints imposed on the multianewarray, anewarray, and newarray instructions Reference The Java&reg; Virtual Machine Specification - Java SE 8 Edition]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid连接池监控的一次改造]]></title>
    <url>%2F2017%2F08%2F31%2Fdruid-monitor-remould%2F</url>
    <content type="text"><![CDATA[1. 背景druid本身提供了监控功能，具体在我另一篇博文《Druid连接池监控》里有介绍。当时提到有以下缺陷： 无法灵活监控多个目标 切换环境不方便 JMX重连不会成功 因此针对这些问题，对其进行改造。改造后的源码已经放在个人的github上：https://github.com/bungder/druid-aggregated-monitor 对应本文的版本，已经打了tag：https://github.com/bungder/druid-aggregated-monitor/releases/tag/0.0.1 对于在同一个工程里进行监控和展示的情况不进行考虑，具体原因见刚刚提到的博文。 2. 原理分析首先，监控数据的展示是通过com.alibaba.druid.support.http.StatViewServlet实现的，将其源码贴上来分析： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181package com.alibaba.druid.support.http;import java.io.IOException;import java.util.HashMap;import java.util.Map;import javax.management.MBeanServerConnection;import javax.management.ObjectName;import javax.management.remote.JMXConnector;import javax.management.remote.JMXConnectorFactory;import javax.management.remote.JMXServiceURL;import javax.servlet.ServletException;import com.alibaba.druid.stat.DruidStatService;import com.alibaba.druid.support.logging.Log;import com.alibaba.druid.support.logging.LogFactory;/** * 注意：避免直接调用Druid相关对象例如DruidDataSource等，相关调用要到DruidStatManagerFacade里用反射实现 * * @author sandzhang&lt;sandzhangtoo@gmail.com&gt; */public class StatViewServlet extends ResourceServlet &#123; private final static Log LOG = LogFactory.getLog(StatViewServlet.class); private static final long serialVersionUID = 1L; public static final String PARAM_NAME_RESET_ENABLE = "resetEnable"; public static final String PARAM_NAME_JMX_URL = "jmxUrl"; public static final String PARAM_NAME_JMX_USERNAME = "jmxUsername"; public static final String PARAM_NAME_JMX_PASSWORD = "jmxPassword"; private DruidStatService statService = DruidStatService.getInstance(); /** web.xml中配置的jmx的连接地址 */ private String jmxUrl = null; /** web.xml中配置的jmx的用户名 */ private String jmxUsername = null; /** web.xml中配置的jmx的密码 */ private String jmxPassword = null; private MBeanServerConnection conn = null; public StatViewServlet()&#123; super("support/http/resources"); &#125; public void init() throws ServletException &#123; super.init(); try &#123; String param = getInitParameter(PARAM_NAME_RESET_ENABLE); if (param != null &amp;&amp; param.trim().length() != 0) &#123; param = param.trim(); boolean resetEnable = Boolean.parseBoolean(param); statService.setResetEnable(resetEnable); &#125; &#125; catch (Exception e) &#123; String msg = "initParameter config error, resetEnable : " + getInitParameter(PARAM_NAME_RESET_ENABLE); LOG.error(msg, e); &#125; // 获取jmx的连接配置信息 String param = readInitParam(PARAM_NAME_JMX_URL); if (param != null) &#123; jmxUrl = param; jmxUsername = readInitParam(PARAM_NAME_JMX_USERNAME); jmxPassword = readInitParam(PARAM_NAME_JMX_PASSWORD); try &#123; initJmxConn(); &#125; catch (IOException e) &#123; LOG.error("init jmx connection error", e); &#125; &#125; &#125; /** * 读取servlet中的配置参数. * * @param key 配置参数名 * @return 配置参数值，如果不存在当前配置参数，或者为配置参数长度为0，将返回null */ private String readInitParam(String key) &#123; String value = null; try &#123; String param = getInitParameter(key); if (param != null) &#123; param = param.trim(); if (param.length() &gt; 0) &#123; value = param; &#125; &#125; &#125; catch (Exception e) &#123; String msg = "initParameter config [" + key + "] error"; LOG.warn(msg, e); &#125; return value; &#125; /** * 初始化jmx连接 * * @throws IOException */ private void initJmxConn() throws IOException &#123; if (jmxUrl != null) &#123; JMXServiceURL url = new JMXServiceURL(jmxUrl); Map&lt;String, String[]&gt; env = null; if (jmxUsername != null) &#123; env = new HashMap&lt;String, String[]&gt;(); String[] credentials = new String[] &#123; jmxUsername, jmxPassword &#125;; env.put(JMXConnector.CREDENTIALS, credentials); &#125; JMXConnector jmxc = JMXConnectorFactory.connect(url, env); conn = jmxc.getMBeanServerConnection(); &#125; &#125; /** * 根据指定的url来获取jmx服务返回的内容. * * @param connetion jmx连接 * @param url url内容 * @return the jmx返回的内容 * @throws Exception the exception */ private String getJmxResult(MBeanServerConnection connetion, String url) throws Exception &#123; ObjectName name = new ObjectName(DruidStatService.MBEAN_NAME); String result = (String) conn.invoke(name, "service", new String[] &#123; url &#125;, new String[] &#123; String.class.getName() &#125;); return result; &#125; /** * 程序首先判断是否存在jmx连接地址，如果不存在，则直接调用本地的duird服务； 如果存在，则调用远程jmx服务。在进行jmx通信，首先判断一下jmx连接是否已经建立成功，如果已经 * 建立成功，则直接进行通信，如果之前没有成功建立，则会尝试重新建立一遍。. * * @param url 要连接的服务地址 * @return 调用服务后返回的json字符串 */ protected String process(String url) &#123; String resp = null; if (jmxUrl == null) &#123; resp = statService.service(url); &#125; else &#123; if (conn == null) &#123;// 连接在初始化时创建失败 try &#123;// 尝试重新连接 initJmxConn(); &#125; catch (IOException e) &#123; LOG.error("init jmx connection error", e); resp = DruidStatService.returnJSONResult(DruidStatService.RESULT_CODE_ERROR, "init jmx connection error" + e.getMessage()); &#125; if (conn != null) &#123;// 连接成功 try &#123; resp = getJmxResult(conn, url); &#125; catch (Exception e) &#123; LOG.error("get jmx data error", e); resp = DruidStatService.returnJSONResult(DruidStatService.RESULT_CODE_ERROR, "get data error:" + e.getMessage()); &#125; &#125; &#125; else &#123;// 连接成功 try &#123; resp = getJmxResult(conn, url); &#125; catch (Exception e) &#123; LOG.error("get jmx data error", e); resp = DruidStatService.returnJSONResult(DruidStatService.RESULT_CODE_ERROR, "get data error" + e.getMessage()); &#125; &#125; &#125; return resp; &#125;&#125; 首先，现在需要搞清楚的问题有： 配置信息是如何生效的 监控数据是怎么流动的 权限控制是怎样实现的 为什么重连会失败 逐个方法去看，init方法是初始化的，应该能找到『配置信息是如何生效的』的答案。里面调用了readInitparam方法来读取，而这个方法又调用了getInitParameter方法，进入方法后发现此方法是javax.servlet.GenericServlet里的，已经不是druid的代码，意味着读取参数是通过调用容器的api实现的，这个过程无法进行篡改。 这里只是读取参数值，还没使用，让我们一步步回退回init方法，在读取了参数值之后就调用initJmxConn方法，该方法初始化了与监控目标之间的JMX连接，是关键的地方。但是里面也没多少东西，主要就是根据url去获取连接，对于『为什么重连会失败』，应该也是一个切入点。但是一路点进去看都没发现有重试的机制。 接着往下看，剩下getJmxResult和process两个方法，其注释里已经讲得很明白了。可以发现，在process方法里有重连的机制，那么还是没搞清楚为什么无法重连成功。 在看完这个类之后，可以发现请求的调用链并没有体现出来，所以看它的父类ResourceServlet。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286/* * Copyright 1999-2011 Alibaba Group Holding Ltd. * * Licensed under the Apache License, Version 2.0 (the "License"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package com.alibaba.druid.support.http;import com.alibaba.druid.support.http.util.IPAddress;import com.alibaba.druid.support.http.util.IPRange;import com.alibaba.druid.support.logging.Log;import com.alibaba.druid.support.logging.LogFactory;import com.alibaba.druid.util.StringUtils;import com.alibaba.druid.util.Utils;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;import java.io.IOException;import java.util.ArrayList;import java.util.List;@SuppressWarnings("serial")public abstract class ResourceServlet extends HttpServlet &#123; private final static Log LOG = LogFactory.getLog(ResourceServlet.class); public static final String SESSION_USER_KEY = "druid-user"; public static final String PARAM_NAME_USERNAME = "loginUsername"; public static final String PARAM_NAME_PASSWORD = "loginPassword"; public static final String PARAM_NAME_ALLOW = "allow"; public static final String PARAM_NAME_DENY = "deny"; public static final String PARAM_REMOTE_ADDR = "remoteAddress"; protected String username = null; protected String password = null; protected List&lt;IPRange&gt; allowList = new ArrayList&lt;IPRange&gt;(); protected List&lt;IPRange&gt; denyList = new ArrayList&lt;IPRange&gt;(); protected final String resourcePath; protected String remoteAddressHeader = null; public ResourceServlet(String resourcePath)&#123; this.resourcePath = resourcePath; &#125; public void init() throws ServletException &#123; initAuthEnv(); &#125; private void initAuthEnv() &#123; String paramUserName = getInitParameter(PARAM_NAME_USERNAME); if (!StringUtils.isEmpty(paramUserName)) &#123; this.username = paramUserName; &#125; String paramPassword = getInitParameter(PARAM_NAME_PASSWORD); if (!StringUtils.isEmpty(paramPassword)) &#123; this.password = paramPassword; &#125; String paramRemoteAddressHeader = getInitParameter(PARAM_REMOTE_ADDR); if (!StringUtils.isEmpty(paramRemoteAddressHeader)) &#123; this.remoteAddressHeader = paramRemoteAddressHeader; &#125; try &#123; String param = getInitParameter(PARAM_NAME_ALLOW); if (param != null &amp;&amp; param.trim().length() != 0) &#123; param = param.trim(); String[] items = param.split(","); for (String item : items) &#123; if (item == null || item.length() == 0) &#123; continue; &#125; IPRange ipRange = new IPRange(item); allowList.add(ipRange); &#125; &#125; &#125; catch (Exception e) &#123; String msg = "initParameter config error, allow : " + getInitParameter(PARAM_NAME_ALLOW); LOG.error(msg, e); &#125; try &#123; String param = getInitParameter(PARAM_NAME_DENY); if (param != null &amp;&amp; param.trim().length() != 0) &#123; param = param.trim(); String[] items = param.split(","); for (String item : items) &#123; if (item == null || item.length() == 0) &#123; continue; &#125; IPRange ipRange = new IPRange(item); denyList.add(ipRange); &#125; &#125; &#125; catch (Exception e) &#123; String msg = "initParameter config error, deny : " + getInitParameter(PARAM_NAME_DENY); LOG.error(msg, e); &#125; &#125; public boolean isPermittedRequest(String remoteAddress) &#123; boolean ipV6 = remoteAddress != null &amp;&amp; remoteAddress.indexOf(':') != -1; if (ipV6) &#123; return "0:0:0:0:0:0:0:1".equals(remoteAddress) || (denyList.size() == 0 &amp;&amp; allowList.size() == 0); &#125; IPAddress ipAddress = new IPAddress(remoteAddress); for (IPRange range : denyList) &#123; if (range.isIPAddressInRange(ipAddress)) &#123; return false; &#125; &#125; if (allowList.size() &gt; 0) &#123; for (IPRange range : allowList) &#123; if (range.isIPAddressInRange(ipAddress)) &#123; return true; &#125; &#125; return false; &#125; return true; &#125; protected String getFilePath(String fileName) &#123; return resourcePath + fileName; &#125; protected void returnResourceFile(String fileName, String uri, HttpServletResponse response) throws ServletException, IOException &#123; String filePath = getFilePath(fileName); if (fileName.endsWith(".jpg")) &#123; byte[] bytes = Utils.readByteArrayFromResource(filePath); if (bytes != null) &#123; response.getOutputStream().write(bytes); &#125; return; &#125; String text = Utils.readFromResource(filePath); if (text == null) &#123; response.sendRedirect(uri + "/index.html"); return; &#125; if (fileName.endsWith(".css")) &#123; response.setContentType("text/css;charset=utf-8"); &#125; else if (fileName.endsWith(".js")) &#123; response.setContentType("text/javascript;charset=utf-8"); &#125; response.getWriter().write(text); &#125; public void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; String contextPath = request.getContextPath(); String servletPath = request.getServletPath(); String requestURI = request.getRequestURI(); response.setCharacterEncoding("utf-8"); if (contextPath == null) &#123; // root context contextPath = ""; &#125; String uri = contextPath + servletPath; String path = requestURI.substring(contextPath.length() + servletPath.length()); if (!isPermittedRequest(request)) &#123; path = "/nopermit.html"; returnResourceFile(path, uri, response); return; &#125; if ("/submitLogin".equals(path)) &#123; String usernameParam = request.getParameter(PARAM_NAME_USERNAME); String passwordParam = request.getParameter(PARAM_NAME_PASSWORD); if (username.equals(usernameParam) &amp;&amp; password.equals(passwordParam)) &#123; request.getSession().setAttribute(SESSION_USER_KEY, username); response.getWriter().print("success"); &#125; else &#123; response.getWriter().print("error"); &#125; return; &#125; if (isRequireAuth() // &amp;&amp; !ContainsUser(request)// &amp;&amp; !("/login.html".equals(path) // || path.startsWith("/css")// || path.startsWith("/js") // || path.startsWith("/img"))) &#123; if (contextPath.equals("") || contextPath.equals("/")) &#123; response.sendRedirect("/druid/login.html"); &#125; else &#123; if ("".equals(path)) &#123; response.sendRedirect("druid/login.html"); &#125; else &#123; response.sendRedirect("login.html"); &#125; &#125; return; &#125; if ("".equals(path)) &#123; if (contextPath.equals("") || contextPath.equals("/")) &#123; response.sendRedirect("/druid/index.html"); &#125; else &#123; response.sendRedirect("druid/index.html"); &#125; return; &#125; if ("/".equals(path)) &#123; response.sendRedirect("index.html"); return; &#125; if (path.contains(".json")) &#123; String fullUrl = path; if (request.getQueryString() != null &amp;&amp; request.getQueryString().length() &gt; 0) &#123; fullUrl += "?" + request.getQueryString(); &#125; response.getWriter().print(process(fullUrl)); return; &#125; // find file in resources path returnResourceFile(path, uri, response); &#125; public boolean ContainsUser(HttpServletRequest request) &#123; HttpSession session = request.getSession(false); return session != null &amp;&amp; session.getAttribute(SESSION_USER_KEY) != null; &#125; public boolean isRequireAuth() &#123; return this.username != null; &#125; public boolean isPermittedRequest(HttpServletRequest request) &#123; String remoteAddress = getRemoteAddress(request); return isPermittedRequest(remoteAddress); &#125; protected String getRemoteAddress(HttpServletRequest request) &#123; String remoteAddress = null; if (remoteAddressHeader != null) &#123; remoteAddress = request.getHeader(remoteAddressHeader); &#125; if (remoteAddress == null) &#123; remoteAddress = request.getRemoteAddr(); &#125; return remoteAddress; &#125; protected abstract String process(String url);&#125; 其中，service方法里有URL的判断，方法里有request和response，看上去就是流程的起点，但是一般我们写servlet都是从doGet和doPost入手的，这里面不知道做了什么封装，于是继续往父类去看，发现其父类是javax.servlet.http.HttpServlet，已经是J2EE定义的类了，我用的是tomcat容器，所以这个类由tomcat提供。里面有常见的doPost和doGet方法 可以看到，doGet和doPost方法默认都是不通的： 12345678910111213141516171819202122232425protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException&#123; String protocol = req.getProtocol(); String msg = lStrings.getString("http.method_get_not_supported"); if (protocol.endsWith("1.1")) &#123; resp.sendError(HttpServletResponse.SC_METHOD_NOT_ALLOWED, msg); &#125; else &#123; resp.sendError(HttpServletResponse.SC_BAD_REQUEST, msg); &#125;&#125;protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String protocol = req.getProtocol(); String msg = lStrings.getString("http.method_post_not_supported"); if (protocol.endsWith("1.1")) &#123; resp.sendError(HttpServletResponse.SC_METHOD_NOT_ALLOWED, msg); &#125; else &#123; resp.sendError(HttpServletResponse.SC_BAD_REQUEST, msg); &#125;&#125; 赶紧去看注释： 1234567891011121314151617181920212223242526272829303132333435/** * Called by the server (via the &lt;code&gt;service&lt;/code&gt; method) to * allow a servlet to handle a GET request. * * &lt;p&gt;Overriding this method to support a GET request also * automatically supports an HTTP HEAD request. A HEAD * request is a GET request that returns no body in the * response, only the request header fields. * * &lt;p&gt;When overriding this method, read the request data, * write the response headers, get the response's writer or... * /protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException&#123; //...&#125;/** * Called by the server (via the &lt;code&gt;service&lt;/code&gt; method) * to allow a servlet to handle a POST request. * * ... * * &lt;p&gt;When overriding this method, read the request data, * write the response headers... * */ protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //...&#125; 原来这些方法是由service方法调用的，并且需要自己覆盖，很符合我们一贯的经验。再看看service方法是怎么回事： 1234567891011121314151617181920212223242526272829303132/** * Receives standard HTTP requests from the public * &lt;code&gt;service&lt;/code&gt; method and dispatches * them to the &lt;code&gt;do&lt;/code&gt;&lt;i&gt;Method&lt;/i&gt; methods defined in * this class. This method is an HTTP-specific version of the * &#123;@link javax.servlet.Servlet#service&#125; method. There's no * need to override this method. * * @param req the &#123;@link HttpServletRequest&#125; object that * contains the request the client made of * the servlet * * @param resp the &#123;@link HttpServletResponse&#125; object that * contains the response the servlet returns * to the client * * @exception IOException if an input or output error occurs * while the servlet is handling the * HTTP request * * @exception ServletException if the HTTP request * cannot be handled * * @see javax.servlet.Servlet#service */protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; //...&#125; 这个方法是请求的统一接收入口，然后将请求分发到doGet、doPost、doHead等对应标准HTTP请求方法的方法去。注释里特地说明了没有必要覆盖这个方法，druid的开发者很直接粗暴，不管你请求方法是什么，全部一刀切，反正这玩意儿要求不高。总之，现在我们知道了，service方法就是请求的入口，这样我们再回去看看com.alibaba.druid.support.http.ResourceServlet的service方法，通过这个方法应该就能理顺整个流程。 将其方法代码加上我自己的注释贴出来： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** * 整个方法其实做的还是路由分发的工作，根据请求的地址，分别返回不同的资源，并且进行访问控制。 */public void service(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; String contextPath = request.getContextPath(); String servletPath = request.getServletPath(); String requestURI = request.getRequestURI(); response.setCharacterEncoding("utf-8"); if (contextPath == null) &#123; // root context contextPath = ""; &#125; String uri = contextPath + servletPath; String path = requestURI.substring(contextPath.length() + servletPath.length()); // 禁止访问的时候返回nopermit.html，returnResourceFile这个方法很关键，下文说说 if (!isPermittedRequest(request)) &#123; path = "/nopermit.html"; returnResourceFile(path, uri, response); return; &#125; /* 从名字看来，这是接收登录请求的 * 很明显，校验就是匹配username和password是否都与配置的匹配，如果匹配就在session里塞点标识 * 很原始的做法，但是对于一个内部使用的线程池监控来说也不用做得太复杂 */ if ("/submitLogin".equals(path)) &#123; String usernameParam = request.getParameter(PARAM_NAME_USERNAME); String passwordParam = request.getParameter(PARAM_NAME_PASSWORD); if (username.equals(usernameParam) &amp;&amp; password.equals(passwordParam)) &#123; request.getSession().setAttribute(SESSION_USER_KEY, username); response.getWriter().print("success"); &#125; else &#123; response.getWriter().print("error"); &#125; return; &#125; /* 拦截登录 */ if (isRequireAuth() // &amp;&amp; !ContainsUser(request)// &amp;&amp; !("/login.html".equals(path) // || path.startsWith("/css")// || path.startsWith("/js") // || path.startsWith("/img"))) &#123; if (contextPath.equals("") || contextPath.equals("/")) &#123; response.sendRedirect("/druid/login.html"); &#125; else &#123; if ("".equals(path)) &#123; response.sendRedirect("druid/login.html"); &#125; else &#123; response.sendRedirect("login.html"); &#125; &#125; return; &#125; // 缺省首页的跳转 if ("".equals(path)) &#123; if (contextPath.equals("") || contextPath.equals("/")) &#123; response.sendRedirect("/druid/index.html"); &#125; else &#123; response.sendRedirect("druid/index.html"); &#125; return; &#125; if ("/".equals(path)) &#123; response.sendRedirect("index.html"); return; &#125; /* * 在不改造的时候，正常监控一个druid实例，会发现页面的数据都是异步刷新的， * 通过浏览器的开发者工具能发现取数据的请求都是json后缀的，所以这里就是监控数据流动的节点 * process方法是关键，而这个方法是一个抽象方法，由具体的实现类来实现，下文将回到&#123;@link com.alibaba.druid.support.http.StatViewServlet#process(String)&#125;方法里看 * / if (path.contains(".json")) &#123; String fullUrl = path; if (request.getQueryString() != null &amp;&amp; request.getQueryString().length() &gt; 0) &#123; fullUrl += "?" + request.getQueryString(); &#125; response.getWriter().print(process(fullUrl)); return; &#125; // 在以上情况都不匹配的时候，返回资源文件 // find file in resources path returnResourceFile(path, uri, response);&#125; 有两个方法需要看： returnResourceFile process returnResourceFile方法在ResourceServlet里面实现了（注释是我加的）： 123456789101112131415161718192021222324252627282930313233343536373839protected void returnResourceFile(String fileName, String uri, HttpServletResponse response) throws ServletException, IOException &#123; String filePath = getFilePath(fileName); // 如果是jpg，则返回流 if (fileName.endsWith(".jpg")) &#123; byte[] bytes = Utils.readByteArrayFromResource(filePath); if (bytes != null) &#123; response.getOutputStream().write(bytes); &#125; return; &#125; /* * 否则读取文件，返回文件内的文本 * 其中，Utils.readFromResource有这么关键的一行 * Thread.currentThread().getContextClassLoader().getResourceAsStream(resource); * 这和servlet初始化的时候是有关的 * ResourceServlet本身也是一个抽象类，其子类StatViewServlet初始化的时候指定了资源目录的路径： * public StatViewServlet()&#123; * super("support/http/resources"); * &#125; */ String text = Utils.readFromResource(filePath); if (text == null) &#123; // 如果请求的路径映射不到资源文件，则调到默认首页（其实就是将404指向了index.html） response.sendRedirect(uri + "/index.html"); return; &#125; // 如果是css或者是js文件，则还需要设置相应的响应头部 if (fileName.endsWith(".css")) &#123; response.setContentType("text/css;charset=utf-8"); &#125; else if (fileName.endsWith(".js")) &#123; response.setContentType("text/javascript;charset=utf-8"); &#125; response.getWriter().write(text);&#125; 接着看process方法（注释是原有的）： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 程序首先判断是否存在jmx连接地址，如果不存在，则直接调用本地的duird服务； 如果存在，则调用远程jmx服务。在进行jmx通信，首先判断一下jmx连接是否已经建立成功，如果已经 * 建立成功，则直接进行通信，如果之前没有成功建立，则会尝试重新建立一遍。. * * @param url 要连接的服务地址 * @return 调用服务后返回的json字符串 */protected String process(String url) &#123; String resp = null; if (jmxUrl == null) &#123; resp = statService.service(url); &#125; else &#123; if (conn == null) &#123;// 连接在初始化时创建失败 try &#123;// 尝试重新连接 initJmxConn(); &#125; catch (IOException e) &#123; LOG.error("init jmx connection error", e); resp = DruidStatService.returnJSONResult(DruidStatService.RESULT_CODE_ERROR, "init jmx connection error" + e.getMessage()); &#125; if (conn != null) &#123;// 连接成功 try &#123; resp = getJmxResult(conn, url); &#125; catch (Exception e) &#123; LOG.error("get jmx data error", e); resp = DruidStatService.returnJSONResult(DruidStatService.RESULT_CODE_ERROR, "get data error:" + e.getMessage()); &#125; &#125; &#125; else &#123;// 连接成功 try &#123; resp = getJmxResult(conn, url); &#125; catch (Exception e) &#123; LOG.error("get jmx data error", e); resp = DruidStatService.returnJSONResult(DruidStatService.RESULT_CODE_ERROR, "get data error" + e.getMessage()); &#125; &#125; &#125; return resp;&#125; 可见它是有重连的，而保证了连接成功之后，获取数据的方法是getJmxResult，这个是在StatViewServlet里面实现的： 1234567private String getJmxResult(MBeanServerConnection connetion, String url) throws Exception &#123; ObjectName name = new ObjectName(DruidStatService.MBEAN_NAME); String result = (String) conn.invoke(name, "service", new String[] &#123; url &#125;, new String[] &#123; String.class.getName() &#125;); return result;&#125; 所以实际上就是用MBeanServer的连接去直接取数据然后原样返回，所有的监控数据其实是缓存在被监控的目标处的，web的监控只是一个请求转发与展示的作用。 所以现在总结StatViewServlet整个工作的主要过程： 记录用户名和密码 根据配置的jmxUrl初始化jmx连接 接收请求，分发请求 如果请求是json数据请求，则通过jmx连接到被监控对象处取数据，然后返回 3. 改进思路3.1 思路一：动态创建并注册StatViewServlet这是首先想到的思路，因为使用这种办法不需要对druid的web监控细节了解多少。要实现这个目标，需要做到以下两点之一： 对于Java Web容器的启动过程很了解，并且深入细节 Google能找到相似的例子 第一点我还做不到，短时间内也做不到，所以只能往第二点去努力。找到了一个最贴切的办法是：Dynamic Servlet Registration Example 12345678910111213141516171819202122232425262728import java.util.Map;import javax.servlet.ServletContext;import javax.servlet.ServletContextEvent;import javax.servlet.ServletContextListener;import javax.servlet.ServletRegistration;import javax.servlet.annotation.WebListener;@WebListenerpublic class ServletContextListenerImpl implements ServletContextListener &#123; @Override public void contextInitialized(final ServletContextEvent sce) &#123; final ServletContext servletContext = sce.getServletContext(); final ServletRegistration.Dynamic dynamic = servletContext.addServlet("Example Servlet", ExampleServlet.class); dynamic.addMapping("/"); final Map&lt;String, ? extends ServletRegistration&gt; map = servletContext.getServletRegistrations(); for (String key : map.keySet()) &#123; servletContext.log("Registered Servlet: " + map.get(key).getName()); &#125; &#125; @Override public void contextDestroyed(final ServletContextEvent sce) &#123; //NO-OP &#125;&#125; 主要就是实现javax.servlet.ServletContextListener，通过javax.servlet.ServletContextEvent实例获取javax.servlet.ServletContext实例，然后调用它的方法去注册新的servlet。 看上去好像可行的样子，但是在实际运行起来之后，在这一行报错了： 1final ServletRegistration.Dynamic dynamic = servletContext.addServlet("Example Servlet", ExampleServlet.class); 错误信息没有记录下来，但是意思就是说这个操作是不支持的，反正就是没戏。具体为什么，还需要进一步了解。 3.2 思路二：修改StatViewServlet的机制在第一个思路走不通之后，只能从其工作机制上入手。其原理分析已经在上文给出。 因为只要有JMX的连接就可以获取数据了，所以关键在于以下几点： 持有多个jmx连接并且与不同的请求关联起来 根据配置去动态创建连接 将原本固定的几个页面与配置的多个监控对象动态地对应起来 配置能根据部署环境的不同而改变，并且发生变更的时候能轻易修改 列出所有被监控对象 对于第一点，创建jmx连接只要有jmxUrl就够了，所以很容易做到，至于与请求关联起来，其实就是从请求的url里提取特征，用于表示不同的监控对象，然后将此特征映射到对应的jmx连接即可。 对于第二点，这其实就是普通的读取、解析配置，然后用配置信息去初始化jmx连接（当然还有登录名、密码和黑白名单等）。 第三点，转下弯，原有的逻辑是将url直接映射为资源文件，只要在这中间加一层解析即可。 第四点，简单的方案是配置多个配置文件，根据不同的环境打不同的包。但是这种做法不灵活，最好还是做成注册中心的形式，被监控对象启动的时候网注册中心写入信息，这边从注册中心读，还有下线机制。但是这种做法工作量大，而且要改被监控的一方，容易引入bug。要不就与配置中心集成，这样就只需改动web监控一端即可。这里的配置方案有多种，很适合采用SPI。 第五点，根据配置信息做个汇总，然后给个页面列出来就可以。 4 实现上文提及的问题在这里基本上都解决了，思路都讲清楚了，实现就不再重复讲。请移步我github的仓库：https://github.com/bungder/druid-aggregated-monitor 在这里提提失败重连的问题 4.1重连失败的问题其重连失败的问题，在debug的时候发现其实并没有重连，它重连的条件是conn为null，但是实际上conn初始化之后就不会为null了，但是当连接失效之后，里面的terminated属性为true，而MBeanServerConnection是一个interface，本身没定义操作这个属性的方法，并且至少有两个类实现了这个接口，运行时的实际类型并不确定是否总是某个实现类型，所以也不好去强转类型进行操作。但是可以利用它本身的逻辑：既然它触发重连的条件是conn为null，我就将它设成null好了。当conn不为null但是获取数据又出错的时候，就可以判断连接有问题，不妨触发重试，即使这种情况下不一定是连接失效了，但是正常情况下不会出现这种现象，就将其当成是连接失效也无妨。]]></content>
      <tags>
        <tag>druid</tag>
        <tag>连接池</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid连接池监控]]></title>
    <url>%2F2017%2F08%2F27%2Fdruid-monitor%2F</url>
    <content type="text"><![CDATA[1. 普通的web监控阿里开源的连接池Druid自带了web监控的功能，具体的操作在其github的wiki上可以找到：配置_StatViewServlet配置 · alibaba/driud wiki 其实主要就是配置一个Servlet，但是github wiki上的做法只能在druid所在的项目中启用web服务的做法，在如今盛行的微服务架构中，很多时候druid所在的服务里并不提供web服务，这就需要额外的配置。其实druid本身也提供了这种功能，只是wiki里没有说。 druid本身可以通过启用JMX端口来将监控数据传输到远端进行处理，具体做法是： 在启动服务的时候加上JVM启动参数（下文说） 在远端启用web服务，配置StatViewServlet，在initParam中指定JMX地址 1.1 JVM启动参数： -Djava.net.preferIPv4Stack=true-Dcom.sun.management.jmxremote-Djava.rmi.server.hostname=192.168.199.123-Dcom.sun.management.jmxremote.port=9876-Dcom.sun.management.jmxremote.authenticate=false-Dcom.sun.management.jmxremote.ssl=false 其中，-Djava.rmi.server.hostname一项指定了服务所在的IP地址，-Dcom.sun.management.jmxremote.port一项指定了JMX端口。 1.2 远程web工程Servlet配置在web.xml里添加如下配置： 123456789101112131415161718192021&lt;servlet&gt; &lt;servlet-name&gt;DruidStatView&lt;/servlet-name&gt; &lt;servlet-class&gt;com.alibaba.druid.support.http.StatViewServlet&lt;/servlet-class&gt; &lt;!-- 远程访问JavaSE项目使用jmx连接 --&gt; &lt;init-param&gt; &lt;param-name&gt;jmxUrl&lt;/param-name&gt; &lt;param-value&gt;service:jmx:rmi:///jndi/rmi://192.168.199.123:9876/jmxrmi&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;loginUsername&lt;/param-name&gt; &lt;param-value&gt;admin&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;loginPassword&lt;/param-name&gt; &lt;param-value&gt;admin&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DruidStatView&lt;/servlet-name&gt; &lt;url-pattern&gt;/druid/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 如果想快速打包启动，不依赖外部web容器，可以采用SpringBoot，用嵌入式的web容器启动，原理也是一样。具体做法是： 创建Servlet： DruidStatViewServlet.java123456789101112131415161718192021package com.tansc.test.springboot.config.druid; import com.alibaba.druid.support.http.StatViewServlet; import javax.servlet.annotation.WebInitParam;import javax.servlet.annotation.WebServlet; /** * StatViewServlet */@SuppressWarnings("serial")@WebServlet(urlPatterns = "/druid/*", initParams=&#123; @WebInitParam(name="loginUsername",value="admin"),// 用户名 @WebInitParam(name="loginPassword",value="admin"),// 密码 @WebInitParam(name="jmxUrl",value="service:jmx:rmi:///jndi/rmi://192.168.199.123:9876/jmxrmi"), @WebInitParam(name="resetEnable",value="false")// 禁用HTML页面上的“Reset All”功能 &#125;)public class DruidStatViewServlet extends StatViewServlet &#123;&#125; 在启动类上加上注解@ServletComponentScan(&quot;com.tansc.test.springboot&quot;)以扫描Servlet。 1.3 查看监控数据上面的都配置好之后，启动服务，等服务启动成功之后，启动远端的web服务，然后访问web服务的/druid目录（与上面配置的匹配即可），例如在我本地起的：http://127.0.0.1:8080/druid 1.4 缺陷 这种做法是一对一的，也就是一个druid连接池实例必须对应一个StatViewServlet，一个StatViewServlet也只能对应一个druid实例。 切换环境不方便 JMX重连不会成功 虽然远端会有重连机制，但是在服务重启之后，重连总是报Connection refused，只能将远端的web服务重启才能成功连上（后来发现这其实没有重连，报错只是使用一个已经terminated的连接获取数据报错，具体见下一篇博文） 1.5 改进对于上面提到的两点缺陷，可以用以下两个思路去改进： 动态创建servlet，模仿分布式服务的注册中心的形式来改造 Servlet本身不提供这样的API，但是应该是可以做到的，具体要继续探索。 使用配置文件 web.xml里面本身不能读取配置文件的值，但是可以通过继承StatViewServlet来实现。 需要分析源码 （2017-08-31 目前已经改造了，见我下一篇博文：Druid连接池监控的一次改造） 1.6 性能损耗当远程的web服务启动并且在浏览器里访问统计页面之后，该服务的内存变化如下： 2. 持久化/自定义传输监控记录可以通过定制StatLogger实现，具体见其github的wiki：怎么保存Druid的监控记录 · alibaba/driud wiki 参考资料 配置StatFilter · alibaba/driud wikihttps://github.com/alibaba/druid/wiki/%E9%85%8D%E7%BD%AE_StatFilter 非web项目如何配置Druid监控 - 若鱼的专栏 - CSDN博客http://blog.csdn.net/goldenfish1919/article/details/68941237 Spring Boot 使用 Druid 和监控配置 - 小单的博客专栏 - CSDN博客http://blog.csdn.net/catoop/article/details/50925337 怎么保存Druid的监控记录 · alibaba/driud wikihttps://github.com/alibaba/druid/wiki/%E6%80%8E%E4%B9%88%E4%BF%9D%E5%AD%98Druid%E7%9A%84%E7%9B%91%E6%8E%A7%E8%AE%B0%E5%BD%95]]></content>
      <tags>
        <tag>druid</tag>
        <tag>连接池</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[持续集成简述]]></title>
    <url>%2F2017%2F07%2F24%2FCI%2F</url>
    <content type="text"><![CDATA[1 概述互联网软件的开发和发布，已经形成了一套标准流程，最重要的组成部分就是持续集成（Continuous integration, CI）。持续集成也被认为是敏捷开发的最重要实践之一。 本文主要介绍持续集成的概念以及一些工具与实践。 2 概念图2.1 持续集成概念示意图之一 上图来源于谈谈持续集成，持续交付，持续部署之间的区别，这图很直观地描述了持续集成相关的概念之间的关系，与持续集成密切相关的概念还有持续交付、持续部署，因此在下文将会就这三个概念进行阐述。 2.1 持续集成维基百科的定义（Continuous integration - Wikipedia）： 在软件工程里，持续集成（Continuous Integration, CI）是指这样的一种实践：在一天里多次将所有开发人员的代码合并到一个共享的主干里，每次合并都会触发持续集成服务器进行自动构建，这个过程包括了编译、单元测试、集成测试、质量分析等步骤，结果只有两个：成功或者失败。如果得到失败的结果，说明有人提交了不合格的代码，这就能及时发现问题。 在瀑布模型中，软件的开发过程被分为以下几个阶段： 需求分析 系统设计 编码实现 测试 集成 部署 维护 而一般在实际的开发中，基本也就遵循这些步骤进行开发，大部分情况下，不会完全按照瀑布流的方式执行，但是这些步骤都是有的，无非就是在文档的落实、步骤的次序与侧重有所变化。其中，所谓的集成，在平时的开发过程中很少直接提到这个名词，其含义是（见：System integration - Wikipedia）： 将子系统的组件整合到一个系统中并且确保子系统的功能可以组合为一个系统的过程。 所以，所谓的『集成』就是在存在多个系统协作的情况下，将子系统集成为一个大系统的过程，这个概念与『集成测试』中的『集成』是同一个概念。但是有些文章在解释『持续集成』这个概念的时候，将其解释为『将代码集成到主干分支』，目前也没见到有人说这是错的。 如果只有一个系统，那么就不存在『系统集成』这一个步骤，但是『持续集成』这个做法还是可以应用其中的，因为『持续集成』的精髓不在于『集成』，而在于『持续』，而我所理解的『持续』，就是将一些重复的操作自动化，然后频繁地触发这个自动化过程。 所以，结合上面提到的瀑布模型的几个阶段，以及『持续集成』这个名字，图2.1应该改为这样： 图2.2 持续集成概念示意图之二 从上图中看来，持续集成应该至少包括以下几部分: 自动化构建 自动化测试 自动发布 图2.3 持续集成阶段示意图 每一次的构建与测试，都应该得到一个结果：通过或者不通过，开发人员应该都能看到每一次构建与测试的结果，得到不通过的结果时应该能马上修复相关的缺陷，这就需要有一种合适的反馈渠道。 2.1.1 自动化构建自动化构建包括以下过程： 将源码编译成为二进制码 打包二进制码 运行自动化测试 生成文档 生成分发媒体（例如：Debian DEB、Red Hat RPM或者Windows MSI文件） 自动化构建可以通过两类工具实现： 构建自动化软件（Build automation utility） 例如Make、Ant、Maven、Gradle，目的是通过编译等活动来生成构建产物（build artifact）。 构建自动化服务器（Build automation servers） 一般是基于web的工具，通过计划任务或者是事件触发的方式调用构建自动化软件。一个CI服务器就是一类构建自动化服务器。 2.1.2 自动化测试自动化测试是持续集成必不可少的一部分，基本上，没有自动化测试的持续集成，都很难称之为真正的持续集成。我们希望持续集成能够尽早的暴露问题，但这远非配置一个 Hudson/Jenkins服务器那么简单，只有真正用心编写了较为完整的测试用例，并一直维护它们，持续集成才能孜孜不倦地运行测试并第一时间报告问题。 测试自动化是使用特定的软件（独立于被测试的软件）来控制测试的执行以及比较实际输出与预期输出。测试自动化可以将某些重复但必要的任务自动化，或者执行某些难以手动执行的额外测试。 自动化测试还包括单元测试、集成测试、系统测试、验收测试、性能测试等，在不同的场景下，它们都能为软件开发带来极大的价值。 2.1.2.1 单元测试单元测试（Unit Testing）又称为模块测试, 是针对程序模块（软件设计的最小单位）来进行正确性检验的测试工作。程序单元是应用的最小可测试部件。在过程化编程中，一个单元就是单个程序、函数、过程等；对于面向对象编程，最小单元就是方法，包括基类（超类）、抽象类、或者派生类（子类）中的方法。 通常来说，程序员每修改一次程序就会进行最少一次单元测试，在编写程序的过程中前后很可能要进行多次单元测试，以证实程序达到预期的工作目标，没有程序错误。 2.1.2.2 集成测试集成测试（Integration Testing，有时也叫Integration and Testing, I&amp;T）即对独立的软件模块组装起来看成是一个整体进行测试。集成测试一般在单元测试之后、系统测试之前进行。实践表明，有时模块虽然可以单独工作，但是并不能保证组装起来也可以同时工作。 集成与单元测试最大的区别是它需要尽可能的测试整个功能及相关环境，对于测试Web应用而言，通常有这么几步： 启动Web容器 部署待测试Web应用 以Web客户端的角色运行测试用例 停止Web容器 通常有三种手段实现集成测试： 大爆炸（Big Bang）将所有单元组合到一起一次性测试一遍。 自上而下（Top Down）先测试高层次的单元，然后逐渐测试低层次的单元。 自下而上（Bottom Up）先测试低层次的单元，然后逐渐测试高层次的单元。 2.2 持续交付图2.4 持续交付示意图 持续交付（Continuous Delivery, CD）是一种软件工程的手段，让软件在短周期内产出，确保软件随时可以被可靠地发布。其目的在于更快、更频繁地构建、测试以及发布软件。通过加强对生产环境的应用进行渐进式更新，这种手段可以降低交付变更的成本与风险。一个简单直观的与可重复的部署过程对于持续交付来说是很重要的。 2.2.1 与DevOps的关系持续交付与DevOps的含义很相似，所以经常被混淆。但是它们是不同的两个概念。DevOps的范围更广，它以文化变迁为中心，特别是软件交付过程所涉及的多个团队之间的合作（开发、运维、QA、管理部门等），并且将软件交付的过程自动化。另一方面，持续交付是一种自动化交付的手段，关注点在于将不同的过程集中起来，并且更快、更频繁地执行这些过程。因此，DevOps可以是持续交付的一个产物，持续交付直接汇入DevOps。 2.2.2 与持续部署的关系有时候，持续交付也与持续部署混淆。持续部署意味着所有的变更都会被自动部署到生产环境中。持续交付意味着所有的变更都可以被部署到生产环境中，但是出于业务考虑，可以选择不部署。如果要实施持续部署，必须先实施持续交付。 2.2.3 原则图2.5 持续交付流水线示意图 持续交付将部署流水线（deployment pipeline）这个老生常谈的概念看作一种简单的防错方法：一系列的确认。通过这些确认，一款软件必须遵循特定的路径来发布。每当有变更被提交到源码控制仓库中时，代码要首先进行编译（有必要的话），然后由构建服务器进行打包，接着由一些不同的技术来进行测试，最后才可以将代码标记为可发布的（releasable）。 在一个持续交付环境中工作时，习惯了漫长开发周期的开发人员或许需要改变他们心态。任何代码提交在任何时间点都可能会向客户发布，明白这一点是很重要的。对于在早期提交尚未准备好让用户使用的代码，类似功能开关这样的模式是很有用的。使用NoSQL可以消除持续交付工作流中的数据迁移和模式变更的步骤、手工操作步骤以及异常情况。对于代码隔离的其他有用的技术——例如代码分支——在持续交付工作流中并未过时，但是必须加以修改以适应持续交付的原则。例如，运行多个长生命周期的代码分支是不实际的，因为一个可发布的产物如果要经历流水线的所有阶段，它必须从一个单独的分支中构建而来。 2.3 持续部署图2.6 持续部署示意图 如图所示，持续部署与持续交付之间的差异就是前者将部署自动化了。 在持续交付的实践中，交付的目标是QA，但是实际上，软件最终是要交付到客户手上的。在SaaS领域里，持续部署采用得比较广泛，因为服务比较容易做到静默升级。 采用持续部署的前提是自动化测试的覆盖率足够高。 采用持续部署的好处是能减少运维的工作量，缩短新特性从开发到实际交付的周期。 3 代价与好处3.1 代价 构造自动测试用例会耗费大量的工作，而且要去覆盖不断增加的新功能点，也要随着代码的改动去修改测试用例。 测试被视为软件开发的一种最佳实践。无论是否采用持续集成，在一些方法论里——例如测试驱动开发——自动化测试都是必不可少的一部分。 采用持续集成的时候也可以不使用任何测试用例，但是在发布一个产品之前的质量保证（Quality Assurance）成本就会变得很高，因为一切都要频繁地采用人手操作。 要花费精力去部署一个构建系统，这有时候会比较复杂，难以灵活地修改 然而，有大量的开源的持续集成软件项目，选择很多 如果项目比较小，或者包含了未经测试的遗留代码，持续集成的价值就并不是那么大 持续集成所增加的价值依赖于测试的质量以及代码实际上的可测试程度。 团队规模大就意味着代码会非常频繁地提交到集成队列里，跟踪提交就会比较困难，并且排队进行构建会减慢每个人的工作，如果构建失败，整个开发团队必须停下手里的工作，立刻修复他们的错误。 但是持续集成是可以灵活变通的，不一定要在一天里多次构建，可以配置每天下班前半小时自动构建，有错误的话开发人员可以在下班前进行处理；或者每天凌晨自动构建，第二天早上开发人员一上班就能看到构建的结果报告，然后进行处理。 一天里有多个提交和合并，一个新功能的部分代码能轻易地推送上去，在新功能完成之前，集成测试都是不通过的。 3.2 好处 能快速发现错误和定位错误 避免在发布日期大家都在检查自己有冲突的版本，造成混乱 当单元测试不通过或者出现bug的时候，如果开发人员需要回滚代码，只会造成少量的改动丢失，因为集成是频繁的。 防止分支大幅偏离主干。如果不是经常集成，主干又在不断更新，会导致以后集成的难度变大，甚至难以集成。 无论对于测试、demo还是发布的目的或需求，总是有一份当前的构建可用。 频繁的代码检查驱使开发人员编写模块化的、低复杂度的代码。 4 具体实践4.1 工具 名称 授权 价格 Git支持 Docker支持 自动测试 备注 Jenkins MIT 免费 不支持 不支持 需插件支持 GitLab CI MIT 免费 支持 支持 需自配测试服务 Phabricator Apache 2.0 免费 支持 Facebook出品 Travis CI 免费 支持 支持 不支持私有部署 Bamboo 收费 支持 支持 支持 Codeship 免费/收费 关联Github, GitLab 支持 不支持私有部署 CircleCI 免费/收费 关联Github 支持 支持大部分测试框架 不支持私有部署 Hudson Eclipse Public License 1.0 免费 需插件 需插件 其实这里的支持，意思应该是直接的支持，例如Jenkins，其实和git结合也很简单，通过脚本就可以实现。 5 参考资料 Maven实战（五）——自动化Web应用集成测试http://www.infoq.com/cn/news/2011/03/xxb-maven-5-integration-test 单元测试 - 维基百科https://zh.wikipedia.org/wiki/%E9%9B%86%E6%88%90%E6%B5%8B%E8%AF%95 Integration testing – Wikipediahttps://en.wikipedia.org/wiki/Integration_testing Continuous integration – Wikipediahttps://en.wikipedia.org/wiki/Continuous_integration Continuous integration | ThoughtWorkshttps://www.thoughtworks.com/continuous-integration 另一种声音：持续集成已死http://www.infoq.com/cn/news/2014/10/continuous-integration 持续集成是什么？http://www.ruanyifeng.com/blog/2015/09/continuous-integration.html 25 best continuous integration tools as of 2017 - Slanthttps://www.slant.co/topics/799/~best-continuous-integration-tools Bamboo vs Jenkins Comparison | Atlassianhttps://www.atlassian.com/software/bamboo/comparison/bamboo-vs-jenkins 通过Docker容器运行持续集成/持续部署http://dockone.io/article/468 Continuous Integration, Deployment &amp; Delivery with Codeshiphttps://codeship.com 谈谈持续集成，持续交付，持续部署之间的区别http://blog.flow.ci/cicd_difference/ Continuous delivery – Wikipediahttps://en.wikipedia.org/wiki/Continuous_delivery Build automation – Wikipediahttps://en.wikipedia.org/wiki/Build_automation List of build automation software – Wikipediahttps://en.wikipedia.org/wiki/List_of_build_automation_software Software development process - Wikipediahttps://en.wikipedia.org/wiki/Software_development_process System integration - Wikipediahttps://en.wikipedia.org/wiki/System_integration Continuous Integration - Martin Fowlerhttps://martinfowler.com/articles/continuousIntegration.html#PracticesOfContinuousIntegration Integration Testing - Software Testing Fundamentalshttp://softwaretestingfundamentals.com/integration-testing/ Practical continuous deployment: a guide to automated software deliveryhttps://www.atlassian.com/blog/continuous-delivery/practical-continuous-deployment]]></content>
      <tags>
        <tag>持续集成</tag>
        <tag>软件工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一种RESTful接口的约定]]></title>
    <url>%2F2017%2F07%2F24%2FREST%2F</url>
    <content type="text"><![CDATA[1 概述1.1 撰写目的本文用于定义一种统一的RESTful接口设计方案，希望具有参考价值。本文所描述的方案比较学院派（死板），在上一家公司提出没有被采纳，在所了解到的有限的若干家声称采用了RESTful风格的公司里，发现他们也偏离甚远，而在书本以及网上大部分介绍RESTful的资料里，却都是这样的方案（URL命名风格、请求与响应的设计）。当然，他们这么做是有理由的，我也理解，这只是取舍问题。这篇文章其实是旧文了，2016年年底就已经写好，但是一直躺在电脑的硬盘里，不想白费了当时的功夫，因此在此公开。 1.2 为什么采用REST目的是为了服务端与客户端的解耦。SOA仅仅是从结构上将前后端分离，但是实际上数据逻辑还是没有实现解耦，服务端接口升级往往会影响客户端，两者的行为需要严格约定。而REST采用HTTP协议进行约定，客户端仅仅需要按照HTTP协议来理解服务端返回的数据，虽然与业务相关的数据结构还是需要约定，但是这确实进一步解耦了服务端与客户端。 另外，由于严格遵照HTTP协议进行数据返回，对于安全的接口，可以在返回的Header里设置缓存策略（接口安全性的概念在下文会解释）。 1.3 文档结构第二部分将阐述关于RESTful的若干个关键的概念，明确第二部分阐述的几个概念有利于设计、实现优雅规范的接口。 第三部分就URL命名的问题进行约定。 第四部分对消息实体进行约定。 第五部分对『向RESTful接口发起请求』进行阐述，约定要实现的方法，约定请求的头部和body的格式。 第六部分对接口的响应格式进行约定，包括响应消息的头部、状态码、JSON实体。 第七部分对版本控制的问题进行约定。 第八部分对RESTful接口的实现提出了实现工具的建议。 2 关键概念明确一些关键的概念是很重要的，虽然RESTful风格的API设计方案并没有统一的标准，但是还是需要符合一定的原则进行设计，否则就不能称为RESTful风格的API。因为许多人并没有对REST进行充分的了解就宣称自己的API是RESTful风格的API，以至于RESTful的提出者Fielding博士本人无法忍受，在2008年为此专门写了一篇博客『REST APIs must be hypertext-driven』，hypertext-driven与HATEOAS是同一个概念的不同表述，在下文会进行阐述。 2.1 RESTfulREST不是一种协议，也不是一种文件格式，更不是一种开发框架。它是一系列的设计约束的集合：无状态性、将超媒体作为应用状态的引擎等。REST是Representation State Transfer的缩写，中文是『表述性状态转移』，这里就涉及到资源的表述与状态两个概念。 简单地说，资源可以看作是服务器上存储的所有数据，资源的表述则是服务器对外提供的指向这些资源的方式，使用JSON、XML等均可，一个资源可以有多种表述；资源的状态则是服务器的数据存储状态，例如在t时刻，服务器中存储了m条数据，这时候客户端向服务端提交了一个创建数据的请求，服务器处理了此请求并创建了一条数据，那么在t+1时刻，服务器中就存储了m+1条数据，这两个时刻的资源状态就是不一样的，t时刻发生的请求导致了资源状态的改变。 2.2 HATEOASHypermedia As The Engine Of Application State，超媒体作为应用程序状态的引擎。这是REST区别于其他SOA风格的主要特点。客户端与服务端进行互动的时候，完全是通过服务端动态提供的超媒体进行的。除了对超媒体的一般理解，客户端不需要知道其他额外的知识。相反，在一些SOA接口的设计中，客户端与服务端的通信是要事先进行约定的，例如通过文档或者接口描述语言（Interface Description Language, IDL）。而基于HTTP协议的REST设计里，一般采用的就是请求与响应的Header来体现HATEOAS原则（具体请参考：https://en.wikipedia.org/wiki/HATEOAS）。这里也隐含这样一层含义：REST应尽可能地利用HTTP标准中现有的东西，例如Header、标准方法与状态码。 从标准的角度看，HTTP标准是一项RFC标准，世界认可；而其他自定义的SOA标准则可能是一项个人标准或者公司标准，最多是一项互联网草案（这对大部分公司来说都不可能），而一项标准越是被广为认可接受，其实现的通用性就越强。个人标准和公司标准都五花八门，这样对每一个标准都要参照其相关文档实现相应的行为逻辑是很麻烦的。 2.3 安全性一个方法被调用1次与被调用0次是一样的，此方法就是安全的，否则就是不安全的。例如，一个方法A仅仅是读取数据，并不创建或者修改数据，不论A方法被调用多少次，都不对数据记录产生任何影响，A方法是安全的。而假如有另一个方法B对数据进行删除，B方法被调用1次后，数据会被删除（或者标识位被修改），系统里的数据发生了变化，那么B方法是不安全的。 2.4 幂等性一个方法被同样地调用1次与被调用多次是一样的，即同样的输入会得到同样的输出，此方法就是幂等的，否则就不是幂等的。 2.3节中A方法与B方法都是幂等的，一个安全的方法一定是幂等的，一个幂等的方法不一定是安全的。 假设一个方法C对某个全局计数器执行自增操作并写入数据库，每次调用C方法都会对系统数据产生影响，那么C方法就不是幂等的。 3 URL命名URL用于标识资源，因此URL应该以名词进行命名，例如/users, /users/children等。 一般URL会内嵌参数，例如要获取id为313的user的信息，那么URL应该为/users/313，前面的user采用复数，如果要列出其所有后代，则URL应为/users/313/children，children为复数形式，如果要获取其id为499的后代，则URL应为/users/313/children/499 4 消息实体消息实体，就是请求和响应消息中的entity-body（也称为body），消息实体采用JSON字符串格式。 5 请求5.1 方法使用HTTP标准定义的请求方法。 5.1.1 get获取资源，单个参数一般写在URL上，多个参数则作为query parameter附在URL后面，例如： 单个参数：/user/123, 表示id为123的user 多个参数：/user?name=tom&amp;phone=13787890987&amp;gender=male get方法应为幂等的，并且不对数据记录产生影响。对于汉字与特殊字符，应该进行urlencode。 5.1.2 post创建资源，请求的headers里设置Content-type为application/json，参数为json类型。 根据约定，在创建成功之后，返回的状态码应该是201（Created），并且在response的Header里设置Location为新创建的资源的URL，例如，创建了一个新的user，该user创建后id为888，那么Header里应该设置Location为/users/888，当然，这应该是一个完整的URL，这里只是给出了一个相对路径的URI以作为说明。返回了这些数据后，客户端可以自定义后续行为，或者查看创建后的user，或者刷新当前的user列表，这些行为服务端并不关心。 如果重复提交了相同的数据，第一次应该返回201，以后则应返回409（Conflict），并且在response的Header里设置Location指向已经存在的资源，说明冲突的来源。 5.1.3 put更新资源，对现有资源进行修改，请求的headers与post一样，参数也是。此方法应该是幂等的。 5.1.4 delete删除资源。此方法应是幂等的。 5.2 HeaderContent-type应设为application/json。 另外应设置一个version，指明所使用的接口版本。这不属于HTTP协议中的一部分，是自定义的，出于版本控制的考量，具体见第七章。 5.3 body采用JSON字符串，具体的结构有待商定，这不属于HTTP协议的一部分，是自定义的。 这里主要放置业务相关的数据。 借用一篇10年前的文章的一张图： 6 响应6.1 Header根据响应的状态码不同，相应地设置头部，具体见下一节。 但是在我所了解的公司里，做法都是统一返回200，然后在返回的JSON字符串里设置消息码。我是不能理解的。据一位前端同学说，前端代码接收到了请求以后，不方便获取Http状态码。其实我也写过前端，不深入，但是一些基本的知识还是有的，我觉得这并不难做到，估计是他的代码封装的时候没有考虑到这一点，现在要改比较麻烦，所以不想大动干戈、伤筋动骨。 6.2 状态码 状态码 语义 使用场景 200 OK 正常返回消息，什么问题也没有 201 Created 创建资源成功，Header里应设置Location指向新创建的资源 202 Accepted 请求已被接收，但是处理过程较长，不能马上返回结果 304 Not Modified 没有任何修改发生 401 Unauthorized 缺乏权限，指已经登录但是缺乏请求这个资源的权限 403 Forbidden 拒绝访问，可用于未登录时拦截返回的状态码，此时Header里应设置Location为登录页面的URL 404 Not Found 不存在所请求的资源 406 Not Acceptable 请求没有被接收，参数约束校验不通过，或者其他业务类型的错误都可以返回这个状态码，response的body里应有表示错误信息的JSON实体。 409 Conflict 请求的资源有冲突，例如多次提交一样的创建请求，response的Header里应设置Location为产生冲突的资源的URL 500 Internal Server Error 服务器的非业务类错误，response的body里应有表示错误信息的JSON实体 6.3 body采用JSON字符串。JSON的结构分为两种：成功、失败。 一般而言，只有返回200的时候才需要读取成功的JSON，只有返回406和500的时候才需要读取失败的JSON，对于其他的状态码，客户端不需要服务器提供额外的消息。 对于成功的JSON，里面应该只包含一个result对象，而失败的JSON应该使用这样的结构： 1234567&#123; error: &#123; code: xxx, message: "xxx", data: &#123;...&#125; &#125;&#125; 失败的JSON只有一个error对象，包含错误码、消息及相关数据，message应该是直接可读的消息，客户端毋需理解发生了什么错误，客户端只需将消息展示出来即可。在收到406的时候，客户端只需知道发生的错误是由客户端造成的即可，具体是什么类型并不需要知道，将消息直接展示出来，让使用的人知道是什么即可，所以message应该是人类可以理解的文本。同理，收到500的时候，只需知道这个错误是服务端的问题即可，客户端也毋需知道具体的错误类型，最多就将错误码和消息展示出来，让使用者有反馈的依据即可。 7 版本控制考虑到接口有可能升级，升级的类型有几种： 新增功能接口 原有接口返回数据增加字段 现有接口返回数据变更现有字段格式或删除现有字段 现有接口变更业务逻辑 删除接口 其中，前两种升级并不会影响客户端，因此毋需处理。而后面三种会导致使用旧接口的客户端不能正常工作。 一般服务端升级与客户端升级都不是同步的，客户端升级往往会滞后，因此在服务端升级后应该保留旧版本的接口继续运行一段时间，让未升级的客户端可以继续工作一段时间，同时可以上线新版本的客户端。过一段时间后再将旧版本的接口下线。 而版本控制应该是向下兼容的，即假设当前版本是1.2，如果客户端请求1.3版本的服务，应当用当前版本提供服务。如果没有注明请求的版本号，应当提供当前版本的服务。 一般情况下，客户端请求需要带版本号，但是服务端并不需要对此进行处理，除非是同时运行新旧版本的同一个接口，才需要做差异处理。 8 实现工具8.1 Spring HATEOASSpring HATEOAS可以很方便地与Spring MVC结合来开发RESTful接口。具体参照其文档：http://docs.spring.io/spring-hateoas/docs/0.20.0.RELEASE/reference/html/#fundamentals.jaxb-json 9 缺陷其实这个方案基本就是网上一些被大部分人认可的做法的汇总，但是缺乏细节，例如分页，但是其实这些可以灵活变通，例如在查询字符串里加上分页参数。《一次无后端的供应链系统开发实践 (上篇)： 前后端分离的 Restful 接口设计》这篇文章的设计就比较全面，但是两者考量的问题范畴不同，他这个是serverless的设计，将业务逻辑都压在前端，后端仅仅作为前端与数据源之间的代理（数据源、客户端都在自己控制范围内的话其实没必要这样搞），这样的话，接口要表达的逻辑就比较复杂，而本文还是从传统的抽象思想去考虑，逻辑都在后端封装，因此接口并不需要表达多么复杂的逻辑。 10 参考文献当时写的时候没记下来，所以就不列出来了，在此不保证来源的权威性，请读者自行鉴别。^_^]]></content>
      <tags>
        <tag>REST</tag>
      </tags>
  </entry>
</search>
